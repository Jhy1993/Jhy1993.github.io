<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/hexo_blog/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/hexo_blog/css/main.css?v=6.4.2" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/hexo_blog/images/apple-touch-icon-next.png?v=6.4.2">


  <link rel="icon" type="image/png" sizes="32x32" href="/hexo_blog/images/favicon-32x32-next.png?v=6.4.2">


  <link rel="icon" type="image/png" sizes="16x16" href="/hexo_blog/images/favicon-16x16-next.png?v=6.4.2">


  <link rel="mask-icon" href="/hexo_blog/images/logo.svg?v=6.4.2" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/hexo_blog/',
    scheme: 'Pisces',
    version: '6.4.2',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="One of the holy grails of machine learning is to compute meaningful low-dimensional embeddings for high-dimensional complex objects. That ability is crucial to tackle advanced tasks, such as inferenc">
<meta property="og:type" content="article">
<meta property="og:title" content="Sentences 1-100">
<meta property="og:url" content="https://Jhy1993.github.io/2018/06/11/english_note/1-100/index.html">
<meta property="og:site_name" content="Jimmy">
<meta property="og:description" content="One of the holy grails of machine learning is to compute meaningful low-dimensional embeddings for high-dimensional complex objects. That ability is crucial to tackle advanced tasks, such as inferenc">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2018-11-06T10:04:03.578Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Sentences 1-100">
<meta name="twitter:description" content="One of the holy grails of machine learning is to compute meaningful low-dimensional embeddings for high-dimensional complex objects. That ability is crucial to tackle advanced tasks, such as inferenc">






  <link rel="canonical" href="https://Jhy1993.github.io/2018/06/11/english_note/1-100/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Sentences 1-100 | Jimmy</title>
  











  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/hexo_blog/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jimmy</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">
    <a href="/hexo_blog/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br />Home</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-about">
    <a href="/hexo_blog/about/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-user"></i> <br />About</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
    <a href="/hexo_blog/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />Tags</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">
    <a href="/hexo_blog/categories/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-th"></i> <br />Categories</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/hexo_blog/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />Archives</a>
  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://Jhy1993.github.io/hexo_blog/2018/06/11/english_note/1-100/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jimmy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/hexo_blog/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jimmy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Sentences 1-100
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-06-11 21:33:44" itemprop="dateCreated datePublished" datetime="2018-06-11T21:33:44+08:00">2018-06-11</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2018-11-06 18:04:03" itemprop="dateModified" datetime="2018-11-06T18:04:03+08:00">2018-11-06</time>
              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/hexo_blog/categories/English/" itemprop="url" rel="index"><span itemprop="name">English</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <ol>
<li>One of <strong>the holy grails</strong> of machine learning is to compute meaningful low-dimensional embeddings for high-dimensional complex objects. That ability is crucial to tackle advanced tasks, such as inference on texts using word embeddings [Pennington et al., 2014, Bojanowski et al., 2016], image understanding [Norouzi et al., 2014] or concise representations for nodes in a huge graph [Grover and Leskovec, 2016].<a id="more"></a></li>
<li>While learning and performing inference on homogeneous networks <strong>have motivated a large amount of research, few work exists on</strong> heterogeneous networks and there are open and challenging issues for existing methods that were previously developed for homogeneous networks. </li>
<li>We demonstrate empirically that Walklets’s latent representations encode various scales of social hierarchy, and improves multi-label classification results over previous methods on a variety of real world graphs </li>
<li>Embedding complex objects as vectors in low dimensional spaces is <strong>a longstanding problem</strong> in machine learning.</li>
<li>To learn the community embedding, we <strong>hinge upon the insight</strong> that </li>
<li>We propose in this work a comprehensive framework for probabilistic embeddings, <strong>in which point embeddings are seamlessly handled as a particular case.</strong></li>
<li><strong>The cornerstone of our approach lies in the use of</strong> the 2-Wasserstein distance</li>
<li>Classification in heterogeneous networks, representative of real world media, is much more recent with only a few attempts for now. Many of them rely on the idea of mapping an heterogeneous network onto an homogeneous network so that classical relational techniques are then used [8, 10, 14, 2]. </li>
<li>Motivated by the ideology of distributed representation learning in recent years, network representation learning (NRL) is proposed to address these issues. NRL aims to learn a real-valued vector for each vertex to reflect its network in-formation </li>
<li>Both methods leverage random walks to achieve this and utilize the skip-gram model to learn the repre-sentation of a node that facilitates the prediction of its structural context—local neighborhoods—in a homogeneous network </li>
<li>These low-dimensional representations are distributed; meaning each social phenomena is expressed by a subset of the dimensions and each dimension contributes to a subset of the social concepts expressed by the space. </li>
<li>This ubiquitous form of representing information has been studied in many disciplines. For instance, in connection with statistical learning, it draws much input from Relational Learning, which aims at capturing the correlation between connected objects, especially in the presence of uncertainty </li>
<li><strong>To lift this limitation</strong>, several reformulations of elliptical distributions have been proposed to handle degenerate scale matrices</li>
<li>Graph embedding is a main stream graph representation framework , which aims to project a graph into a low-dimensional space for further analytic tasks, such as classification, clustering and so on. </li>
<li>Graph data is becoming increasingly popular, thanks to the proliferation of various social media (e.g., blogs, Flickr, Twitter) and many other kinds of information networks (e.g., DBLP, knowledge graphs) </li>
<li>For example, taking a user-selected meta-path &lt; author − paper − venue − paper − author &gt; (i.e., shared-venues) to find similar authors, the projection will generate a homogeneous network consisting of only authors, but it loses important network structure information (e.g., venues and papers) </li>
<li>PathSim oly capture desired semantic similarity between peer objects </li>
<li>They do not fully exploit the correlations between the different node labels or characteristics. </li>
<li>Objects of different types and links carry different semantic meanings,can identify objects that not only are strongly connected but also share similar type </li>
<li>The assumption we make in the paper is that nodes of different types do influence each other, so that labels of the different node types will be inter-dependent; modeling this dependence between different node types is thus important for accurate node classification or labeling and cannot be achieved via classical homogeneous network formulations </li>
<li>Without loss of generality, we define the relations between vertices as a set of labels, instead of a single label </li>
<li>It is important to mention that the proposed algorithm learns a latent representation of the network nodes so that all nodes, irrespectively of their type, will share a common latent space. Such representation will then be effectively used to infer the categories </li>
<li>The main contributions are :1) an novel method for mapping nodes and edges onto a common latent sapce:This mapping exploits…… </li>
<li>we proposed a general framework to lean etwork represention in heterogeneous networks </li>
<li>The paper is organized as follows: Section 2 presents the notations, and introduces some background concerning  Section 3 describes the proposed algorithm and…Section 4 presents experimental results on ?? datasets, and a qualitative analysis of the latent representions learned by the model </li>
<li>a regularized term is adopted to avoid the over-fitting problem. That is due to the fact that we seek to preserve the inference ability that can match the vertices containing the similar contexts, rather than match those vertices containing exactly the same context </li>
<li>A heterogeneous network will be modeled as an undirected weighted graph with a node type associated to each node. </li>
<li>algorithm based on mete-path has teo drawbacks：1）Dependencies between the different the two types of nodes are lost,since the latent represention of nodes are only dependent of the nodes of its neighbors of the same type. </li>
<li>However, these homogeneous models cannot capture the information about entity types nor about relations across different typed entities in HINs. </li>
<li>Various similarity metrics have been designed for vector spaces in different tasks, such as cosine similarity [24], Jaccard coefficient [14], and the p -norm distance </li>
<li>Unlike the reconstruction task, this task predicts the future links instead of reconstructing the existing links. </li>
<li>In addition, we add Common Neighbor in this task because it has been proved as an effective method to do link prediction . </li>
<li>XXX alse extends the DeepWalk in generating random walks fed to SkipGram by only selecting unvisited neighbors of the predecessor node in the early stage </li>
<li>it’s can consider more information in diferent edges2 choosing neighbors of the predecessors node with bias probability w.r.t. different connection weights, meaning the co-occurrence frequency of two nodes in the corpus reflects their correlation </li>
<li>The key difference from existing embedding methods for a single net- work is that there are three kind of edges and two type of latent spaces corresponding to two networks </li>
<li>The experiment is to evaluate the effectiveness of the latent features learned by the proposed EOE model on com-mon data mining tasks including visualization, link predic- tion, multi-class classification, and multi-label classification. </li>
<li>The basic idea of our approach is to preserve the HIN structure information into the learned embeddings, such that vertices which co-occur in many path instances turn to have similar embeddings. </li>
<li>Since learning rich vertex embeddings does not require the accurate probability of each path instance, we adopt the NCE for optimization, which has been proved to significantly accelerate the training without cutting down the embedding  qualities </li>
<li>However, the discriminant power of the node embeddings maybe improved by considering the node label  information and the node attribute information. </li>
<li>Recently, motivated by the success of the unsupervised distributed representation learning techniques in the natural language processing area, several novel network embedding  methods have been proposed to learn distributed dense representations for networks </li>
<li>Instead of handcrafed network feature design, these  representation learning methods enable the automatic discovery of  useful and meaningful (latent) features from the “raw networks.” </li>
<li>Yet a large number of social and  information networks are heterogeneous in nature, involving diversity of node types and/or relationships between nodes </li>
<li>How do we efectively preserve  the concept of “word-context” among multiple types of nodes, e.g.,  authors, papers, venues, organizations, etc </li>
<li>Recently, network embedding has been utilized to fill the  gap of applying tuple-based data mining models to networked datasets by learning embeddings which preserve the  network structure </li>
<li>For the  implementation of the algorithm to solve the EII model, the  dimension of embeddings is set as… </li>
<li>Different from existing  approaches that rely heavily on feature engineering, we propose to  use network embedding approach to address the problem, which  can automatically represent nodes into lower dimensional feature  vectors </li>
<li>Although feature engineering can incorporate prior knowledge  of the problem and network structure, usually it is time-consuming,  problem specific (thus not transferable), and the extracted features  may be too simple for complicated data sets </li>
<li>A key idea  behind network embedding is learning to map nodes into vector  space, such that the proximities among nodes can be preserved </li>
<li>From these two examples, it is easy to see that in a heterogeneous  network, even compare two nodes of the same type (e.g. paper), going from different paths can lead to different semantic meanings. </li>
<li>The networked data is usually high-dimensional and sparse, as  there can be many nodes but the links are usually sparse [1]. This  brings challenges to represent nodes in the network </li>
<li>Most of existing network embedding techniques [17, 26, 25] are  based on the idea that, embeddings of nodes can be learned by  neighbor prediction, which is to predict the neighborhood given  a node, i.e. the linking probability P(j|i) from node i to node j. </li>
<li>The former focuses more on the direct information related  to the specific task, while the latter can better explore more global  and diverse information in the heterogeneous information network. </li>
<li>We study the hyper-parameters ω, which is the trade-off term for  combing A and B </li>
<li>Machine learning applications in net-works (such as network classification [16,37], content rec- ommendation [12], anomaly detection [6], and missing link prediction [23]) must be able to deal with this sparsity in  order to survive. </li>
<li>These low-dimensional representations  are distributed; meaning each social phenomena is expressed  by a subset of the dimensions and each dimension contributes  to a subset of the social concepts expressed by the space. </li>
<li>It is this connection to local structure that motivates us  to use a stream of short random walks as our basic tool  for extracting information from a network </li>
<li>The  objective of a good embedding is to preserve the proximity (i.e., similarity) between vertices in the original graph </li>
<li>The availability and growth of large networks, such as social net- </li>
<li>works, co-author networks, and knowledge base graphs, has given  rise to numerous applications that search and analyze information  in them </li>
<li>The proximity among  objects in a HIN is not just a measure of closeness or distance, but  it is also based on semantics </li>
<li>The heterogeneity of nodes and edges in HINs bring challenges, but also opportunities to support important applications </li>
<li>Deep Learning’s recent successes have mostly relied on Convolutional Networks,which exploit fundamental statistical properties of images, sounds and video data:the local stationarity and multi-scale compositional structure, that allows expressing long range interactions in terms of shorter, localized interactions </li>
<li>In this paper we consider the general question of how to….. In particular, we deveplop… </li>
<li>In this work, we are interested in generalizing convolutional neural networks(CNNs) from low-dimensional regular grids, where image, video and speech arerepresented, to high-dimensional irregular domains, such as social networks, brainconnectomes or words’ embedding, represented by graphs </li>
<li>User data on social networks, gene data on biological regulatory networks, log data on telecom-munication networks, or text documents on word embeddings are important examples of data lyingon irregular or non-Euclidean domains which can be structured with graphs, which are universalrepresentations of heterogeneous pairwise relationships </li>
<li>To address these issues, Network Representation Learning (NRL) aims at the possibility of encoding node information in a unified continuous space </li>
<li>Despite the success of embedding individual nodes for graph analytics, we notice that an important concept of embedding communities (i.e., groups of nodes) is missing in the literature </li>
<li>Our major insight to learn the community embedding is hinged upon the mutual reinforcement between node embedding and community embeddin </li>
<li>With the success of deep learning in recent years, there is a marked switch from hand-crafted features to those that are learned from raw data in recommendation research </li>
<li>Besides, this research sheds new light on the usage of heterogeneous information in the knowledge base, which can be consumed in more application scenarios </li>
<li>However, for relational network classification, DeepWalk can be suboptimal as it lacks a mechanism to optimize the objective of the target task </li>
<li>The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data </li>
<li>Good representations are expressive, meaning that areasonably-sized learned representation can capture a hugenumber of possible input configurations </li>
<li>One of the challenges of representation learning that distinguishes it from other machine learning tasks such as classification is the difficulty in establishing a clear objective, or target for training </li>
<li>As the core of semantic proximity search, we have to measure the proximity on a heterogeneous graph, whose nodes are various types of objects. </li>
<li>With recent development on graph embedding, we see a good chance to avoid feature engineering for semantic proximity search </li>
<li>The core problem of semantic proximity search is how to measure the proximity on a heterogeneous graph with various types of objects </li>
<li>Embedding nodes is an indirect approach to learn proximity, because it lacks an explicit representation for the network structure between two possibly distant nodes </li>
<li>To enable knowledge discovery from such networked data, network representation learning (NRL) aims to learn vector representations for network nodes, such that off-the-shelf machine learning algorithms can be directly applied </li>
<li>Existing studies on network representation learning (NRL)  have shown that NRL can effectively capture network structure to facilitate subsequent analytic tasks, such as node classification [3], anomaly detection [4], and link prediction </li>
<li>Despite its great potential, learning useful network representations faces several challenges. (1) Sparsity (2) Structure preserving (3) Rich-Content </li>
<li>How to leverage rich content information and its interplay with network structure for representation learning remains an open problem. </li>
<li>By simultaneously integrating homophily, structural context, and node content, HSCA effectively embeds a network into a single latent representation space that captures the interplay between the three information sources. </li>
<li>Graph embedding aims to reduce the dimensionality of the original data in the vector-based form, while NRL seeks to learn effective vector representations of network nodes to facilitate downstream network analytic tasks. </li>
<li>This approach turns the discrete topology of the relations into a continuous one, enabling the design of efficient algorithms and potentially benefitting many applications. </li>
<li>This source of information has been ignored by most of the current recommender systems while it can potentially alleviate the sparsity problem and improve the quality of recommendations </li>
<li>With the large volume of new information created every day, determining the validity of information in a knowledge graph and filling in its missing parts are crucial tasks for many researchers and practitioners </li>
<li>A number of KGC algorithms have been developed in recent years, and the most successful models all have one thing in common: they use low-dimensional embedding vectors to represent entities and relationships. </li>
<li>With these considerations in mind, in the present work we rethink some of the basic decisions made by previous models to create a projection embedding model (ProjE) for KGC. </li>
<li>The newest contributions in this line of work focus primarily on the changes in how the embedding planes are computed and/or how the embeddings are combined </li>
<li>To recap, the contributions of the present work are as follows: </li>
<li>In recent years, a series of approaches have been proposed to embed a knowledge graph into a continuous vector space while preserving the properties of the knowledge graph </li>
<li>Generally speaking, precise link prediction would improve the feasibility of knowledge completion, the effectiveness of knowledge reasoning, and the performance of many knowledge-related tasks </li>
<li>What is the best way to describe a user in a social network with just a few numbers? Mathematically, this is equivalent to assigning a vector representation to each node in a graph, a process called graph embedding </li>
<li>Through a graph embedding, we are able to visualize a graph in a 2D/3D space and transform problems from a non-Euclidean space to a Euclidean space, where numerous machine learning and data mining tools can be applied </li>
<li>Numerous embedding algorithms have been proposed in data mining, machine learning and signal processing communities. </li>
<li>Naturally, it is necessary for network embedding to preserve the first-order proximity because it implies that two vertexes in real world networks are always similar if they are linked by an observed edge </li>
<li>Experimental results using reviews  from 50 product types show significant improvements over state- of-the-art baseline models. </li>
<li>We conduct extensive  experiments to demonstrate the performance of BASS, concluding that  our approach significantly outperform state-of-the-art approaches. </li>
<li>The advantage is multi-folded.  Firstly,..  Secondly，… </li>
</ol>

      
    </div>

    

    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>坚持原创技术分享，您的支持将鼓励我继续创作！</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>Donate</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/hexo_blog/images/Wechat_pay.jpeg" alt="Jimmy WeChat Pay"/>
        <p>WeChat Pay</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/hexo_blog/images/ali_pay.jpeg" alt="Jimmy Alipay"/>
        <p>Alipay</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/hexo_blog/2018/06/11/template/" rel="next" title="Hello World">
                <i class="fa fa-chevron-left"></i> Hello World
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/hexo_blog/2018/06/11/english_note/101-200/" rel="prev" title="Sentences 101-200">
                Sentences 101-200 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Jimmy</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/hexo_blog/archives/">
                
                    <span class="site-state-item-count">19</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/hexo_blog/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">5</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/hexo_blog/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">6</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://github.com/Jhy1993" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="823322482@qq.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  
                </span>
              
            </div>
          

          
          

          
          

          
            
          
          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright"> &copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jimmy</span>

  

  
</div>




  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> v3.7.1</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a class="theme-link" target="_blank" href="https://theme-next.org">NexT.Pisces</a> v6.4.2</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    
	
    

    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>














  
    
      
  
  <script type="text/javascript" color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/hexo_blog/lib/canvas-nest/canvas-nest.min.js"></script>













  
  
    <script type="text/javascript" src="/hexo_blog/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/hexo_blog/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/hexo_blog/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/hexo_blog/js/src/utils.js?v=6.4.2"></script>

  <script type="text/javascript" src="/hexo_blog/js/src/motion.js?v=6.4.2"></script>



  
  


  <script type="text/javascript" src="/hexo_blog/js/src/affix.js?v=6.4.2"></script>

  <script type="text/javascript" src="/hexo_blog/js/src/schemes/pisces.js?v=6.4.2"></script>



  
  <script type="text/javascript" src="/hexo_blog/js/src/scrollspy.js?v=6.4.2"></script>
<script type="text/javascript" src="/hexo_blog/js/src/post-details.js?v=6.4.2"></script>



  


  <script type="text/javascript" src="/hexo_blog/js/src/bootstrap.js?v=6.4.2"></script>



  



  










  





  

  

  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  
  

  

  

  

  

  

</body>
</html>
