<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[18ICLR_WAE_Wasserstein auto-encoders]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F11%2F07%2Fpaper_note%2F18ICLR_WAE_Wasserstein%20auto-encoders%2F</url>
    <content type="text"><![CDATA[TODO]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Wasserstein</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[18ArXiv_HIB_MODELING UNCERTAINTY WITH HEDGED INSTANCE EMBEDDING]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F11%2F07%2Fpaper_note%2F18ArXiv_HIB_MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING%2F</url>
    <content type="text"><![CDATA[之前的Gaussian Embedding都是将Object映射为一个高斯分布来捕捉不确定性. 但是一个高斯分布足够吗? 本文将Object映射为一组高斯分布,即混合高斯. Gaussian Embedding 又叫 Stochastic Embedding Vector Embedding 又叫 Point Embedding Motivation前面已经有很多工作将各种各样的Object映射为高斯分布来捕获不确定性.但是如果Object的不确定性很高,一个高斯分布也不足以捕获呢? 本文提出了HIB来解决上述问题.所谓的”hedging its bets”就是为了避免损失,多处下注.对应到本文,对于有歧义的输入,将其映射为多个高斯分布, 每个高斯分布代表其中一个意思. 针对图像的歧义需求,设计了$N-digits MNIST$ 数据集,即将mnist中的数字组合成一张新图,这样每张图就有歧义了. figure 1. 这张图就很好的解释了本文的Motivation. 作者总结了本文的3个贡献: 效果提升,尤其是针对一些不确定的输入. 加强了结构正则??怎么理解 也没理解.. Model模型主要思路是通过variational information bottleneck principle来学习hedged instance embedding (HIB). hedged instance embedding (HIB).这里首先介绍两个输入的匹配概率, 实际是输入x到分布Z变换$Z \sim p(z|x)$, 计算Z的匹配概率$$p ( m | x _ { 1 } , x _ { 2 } ) = \int p ( m | z _ { 1 } , z _ { 2 } ) p \left( z _ { 1 } | x _ { 1 } \right) p \left( z _ { 2 } | x _ { 2 } \right) \mathrm { d } z _ { 1 } \mathrm { d } z _ { 2 }$$为了求这个积分分两步: $p(z|x)$这个一般都是用神经网络来映射. 求$p ( m | x _ { 1 } , x _ { 2 } )$ 这里用了MC Sampling, 分别从两个输入$x_1,x_2$ 采样多个样本. $$p ( m | x _ { 1 } , x _ { 2 } ) \approx \frac { 1 } { K ^ { 2 } } \sum _ { k _ { 1 } = 1 } ^ { K } \sum _ { k _ { 2 } = 1 } ^ { K } p ( m | z _ { 1 } ^ { \left( k _ { 1 } \right) } , z _ { 2 } ^ { \left( k _ { 2 } \right) } )$$ 到这里模型其实已经可以用了,但是本文所强调的混合高斯怎么融入到上述模型里呢? 这里其实很简单,就是将$p(z|x)$这个映射变复杂,采样也变成从多个高斯分布中采样.$$p(z|x)=\sum _ { c = 1 } ^ { C } \mathcal { N } ( z ; \mu ( x , c ) , \Sigma ( x , c ) )$$到现在模型其实新颖程度创新程度感觉还不够, 看来有一部分创新在VIB里. variational information bottleneck principle为了训练上述模型, 本文混合了soft contrastive loss 和 the VIB principle.这里利用了17ICLR_VIB_Deep variational information bottleneck的推导结果$$I ( z , y ) - \beta I ( z , x )$$直观理解就是:学到的z和标签y很像,同时 z和输入x尽量相似度小. 这样在保证z有效性的同时,又可以滤除x中无用部分$$ \mathcal { L } _ { \mathrm { VIB } } : = \mathbb { E } _ { z \sim p ( z | x ) } [ \log q ( y | z ) ] - \beta \cdot \mathrm { KL } ( p ( z | x ) | r ( z ) )$$并结合本文Embedding需求,对上述loss里的一些项进行了替换.$$\begin{aligned} \mathcal { L } _ { \mathrm { VIBEmb } } : = &amp; - \mathbb { E } _ { z _ { 1 } \sim p \left( z _ { 1 } | x _ { 1 } \right) , z _ { 2 } \sim p \left( z _ { 2 } | x _ { 2 } \right) } [ \log p ( m = \hat { m } | z _ { 1 } , z _ { 2 } ) ] \ &amp; + \beta \cdot \left[ \mathrm { KL } \left( p \left( z _ { 1 } | x _ { 1 } \right) | r \left( z _ { 1 } \right) \right) + \mathrm { KL } \left( p \left( z _ { 2 } | x _ { 2 } \right) | r \left( z _ { 2 } \right) \right) \right] \end{aligned}$$(5)中$q(y|z)$是根据表示z预测标签y,对应这里的$\log p ( m = \hat { m } | z _ { 1 } , z _ { 2 } ) $,这一项就是soft contrastive loss .]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Gaussian Embedding</tag>
        <tag>Deep Learning</tag>
        <tag>Wasserstein</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[18KDD_DVNE_Deep Variational Network Embedding in Wasserstein Space]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F11%2F07%2Fpaper_note%2F18KDD_DVNE_Deep-Variational-Network-Embedding-in-Wasserstein-Space%2F</url>
    <content type="text"><![CDATA[本文来自network embedding大佬THU 崔鹏老师.在Wasserstein空间将节点表示为一个高斯分布. 感觉这篇实际是18ICLR_Graph2Gauss和18ICLR_WAE_Wasserstein auto-encoders的结合. Motivation本文上来罗列了很多的优点: 节点映射为Wasserstein space的一个gaussian distribution来保持网络结构和不确定性 network embedding的fundamental problem就是保持网络本身的结构和性质(如传递性,X阶结构) Wasserstein distance可以保持transitivity,并且W-2距离对高斯分布有闭市解,线性计算复杂度 KL散度不对称, JS对称但是和KL散度一样无法保持传递性 均值代表节点的位置, 方差代表节点的不确定性 可以保持局部和全局结构 保持一阶相似性和二阶相似性感觉已经成了NE的基本操作了.. 然后分析了一波现有方法的不足 Wasserstein distance相对现有KL的优势,可以保持transitivity 现有方法是把均值向量和方差向量拼接一起作为最终的表示,比较粗糙. 无法反映他们之前的本质关系. 无法保持高阶相似性. graph2gauss可以,但是需要计算最短路径,比较耗时. 但是我觉得相对于Graph2Gauss比较特别的有两点 换了个Wasserstein distance保持transitivity,并且W2距离对于两个高斯分布有闭式解,计算高效. 通过深度变分模型来保持均值与方差之间的关系.(用采样过程,代替简单的拼接,见5.3实验分析) 和之前一样,每个节点的表示$h_i=\mathcal{N}(\mu_i, \sum_i)$ 一个均值向量和一个方差矩阵.这里方差矩阵只考虑对角方差矩阵. Model首先看到模型图就感觉和SDNE很像.SNDE是约束节点对相似,这里是约束三元组的rank关系. 首先看下这里如何对两个高斯分布进行相似性度量,这里选用Wasserstein distance来度量分布的相似性,特别的这里选用$W_2$ 距离.$$\begin{aligned} \operatorname { dist } &amp; = W _ { 2 } \left( \mathcal { N } \left( \mu _ { 1 } , \Sigma _ { 1 } \right) , \mathcal { N } \left( \mu _ { 2 } , \Sigma _ { 2 } \right) \right) \ d i s t ^ { 2 } &amp; = \left| \mu _ { 1 } - \mu _ { 2 } \right| _ { 2 } ^ { 2 } + \operatorname { Tr } \left( \Sigma _ { 1 } + \Sigma _ { 2 } - 2 \left( \Sigma _ { 1 } ^ { 1 / 2 } \Sigma _ { 2 } \Sigma _ { 1 } ^ { 1 / 2 } \right) ^ { 1 / 2 } \right) \end{aligned}$$又因为这里只考虑对角方差矩阵$\Sigma _ { 1 } \Sigma _ { 2 } = \Sigma _ { 2 } \Sigma _ { 1 }$, 上述可以进一步化简$$W _ { 2 } \left( \mathcal { N } \left( \mu _ { 1 } , \Sigma _ { 1 } \right) ; \mathcal { N } \left( m _ { 2 } , \Sigma _ { 2 } \right) \right) ^ { 2 } = \left| \mu _ { 1 } - \mu _ { 2 } \right| _ { 2 } ^ { 2 } + \left| \Sigma _ { 1 } ^ { 1 / 2 } - \Sigma _ { 2 } ^ { 1 / 2 } \right| _ { F } ^ { 2 }$$ Loss分为两部分,类似SDNE的的loss 一阶相似性: 用了和Graph2Gauss一样的基于ranking 的loss, $$\mathcal { L } _ { 1 } = \sum _ { ( i , j , k ) \in \mathrm { D } } \left( E _ { i j } ^ { 2 } + \exp \left( - E _ { i k } \right) \right)$$ 二阶相似性:用WAE来编码每个节点的邻接向量.这里对WAE进行了简化,最后和AE的区别就是encoder后得到的分布中采样的结果才是隐表示Z. $$\mathcal { L } _ { 2 } = \inf _ { Q ( Z | X ) \in Q } \mathbb { E } _ { P _ { X } } \mathbb { E } _ { Q ( Z | X ) } \left[ | X \circ ( X - G ( Z ) ) | _ { 2 } ^ { 2 } \right]$$ 这里也像SDNE一样加大对非零元素的惩罚,只是惩罚系数为0,1 所以SDNE中的B成了这里的X. 优化. 这里用了repara trick.这里变分自编码器学习到的是均值$\mu$和标准差$\sigma$ , 从均值为0,标准差为1的高斯分布中采样,再放缩平移得到Z. $$\mathbf { z } _ { i } = \mu _ { i } + \sigma _ { i } * \epsilon , \epsilon \sim \mathcal { N } ( 0 , \mathbf { I } )$$ 激活函数的选择,有两个需要注意 均值没有用激活函数, 为什么? 这里用elu()+1来保证$\sigma_i$ 是正的. 标准差没有负数! sigmoid来压缩输出到[0,1]之间. Exp数据集:都是无向图, 无属性. 但是graph2gauss需要属性,这里用one-hot代替 为了保证公平,所有学习高斯分布的算法 dim(均值)+dim(方差)=L,本文虽然不是拼接也遵守这个规定. 两类有效性实验: 基于相似性的,包括网络重构和链路预测,这里是基于节点的均值和方差来计算相似性.无向图在计算KL的时候是怎么算的呢? 网络重构的提升很小,链路预测提升还可以. 为什么这两个任务的提升差距这么大? 分类任务,这里对于学习分布的算法,只用了节点的均值来作为输入. 感觉对G2G不公平. 比较奇怪的是,这里用KL距离和W2距离时,DVNE的表现差距很大. DVNE_KL甚至不如很多baseline. 这说明gaussian embedding起到的作用没有W2距离大? 一个不确定性实验: 这个算比较有特色的.这里提出一个intuition: 节点的度越小,越难得到精确的point-vector.换句话说, 节点的度越大,那么连接提供的信息越多, 这个节点也越加稳定. 个人觉得这里的intuition不够准确, graph2gauss的描述更为精确 测试节点的度和方差的关系. 节点的度越大,方差越小. 不确定性中的方差项有助于处理噪声边(为什么有助于?). 注意纵坐标是AUC的下降程度.这里可以就看出噪声边的比例增加,本文算法的AUC下降的最少.但是比较奇怪的是G2G也是gaussian Embedding,为什么它的的下降程度较高? 作者的解释是G2G中均值和方差只是拼接,并没有很好的结合在一起. Conclusion本文相对于Graph2Gauss的改进在于 换了W2距离 把编码均值方差的部分变复杂了,用了个WAE 但是从实验结果看 采样的方式来利用均值方差貌似作用不大,关键还是这个W2距离. 如果进一步做,可以考虑做非对角的方差矩阵.如果方差矩阵非对角,那是什么分布呢?答案是椭圆分布Elliptical Distributions. 且听下回分解.18NIPS_Generalizing Point Embeddings using the Wasserstein Space of Elliptical Distributions]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Network Embedding</tag>
        <tag>Gaussian Embedding</tag>
        <tag>Deep Learning</tag>
        <tag>Wasserstein</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[18NIPS_Distilled Wasserstein Learning for Word Embedding and Topic Modeling]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F11%2F07%2Fpaper_note%2F18NIPS_Distilled-Wasserstein-Learning-for-Word-Embedding-and-Topic-Modeling%2F</url>
    <content type="text"><![CDATA[Motivationword embedding考虑了word之间的关系;而topic model 没有考虑word order和word间的关系.这样会导致一个问题就是word embedding和topic distribution之间mismatch. 为什么要考虑word order呢? 因为在某些场景下, 比如本文的医疗处理记录中, 一些操作的先后顺序会极大的影响对操作/疾病/流程的分析. ss]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Network Embedding</tag>
        <tag>Gaussian Embedding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[18ICLR_Graph2Gauss_DEEP GAUSSIAN EMBEDDING OF GRAPHS_UNSUPERVISED INDUCTIVE LEARNING VIA RANKING]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F10%2F27%2Fpaper_note%2F18ICLR_Graph2Gauss_DEEP%20GAUSSIAN%20EMBEDDING%20OF%20GRAPHS_UNSUPERVISED%20INDUCTIVE%20LEARNING%20VIA%20RANKING%2F</url>
    <content type="text"><![CDATA[目前主要的network embedding方法都是将节点映射为向量,但是向量所能表示的信息还是不够.受word gaussian embedding的启发,用一个高斯分布来表示word,可以捕获word的多重含义.类比到网络中,node可能有不同方面的信息(多个研究领域,属于多个社区),一些算法尝试将node也映射为高斯分布. 节点表示相当于只学习了高斯分布的均值,而高斯表示除了均值外还可有方差,包括了更多的信息. 但是还有一些困惑: 高斯分布所谓的”捕获不确定性”到底对评测任务(节点分类)有什么提升? 单个高斯分布 VS 混合高斯模型? 目前都是假定协方差矩阵是对角的,如果不是对角的呢?(18NIPS已经做了) 下面梳理了几篇gaussian embedding的文章,包括word和node Motivation网络中的节点可能含有丰富的甚至冲突的语义,比如节点属于不同的社区,有着不同的爱好.(这点和word2gaussian中word的多义,歧义现象符合).这种现象被解释为不确定性.那么如何在进行网络表示学习,对节点进行embedding的时候来捕获这种不确定性呢?那就是将节点映射为高斯分布,而不是一个向量. 高斯分布除了均值部分(相当于原来向量表示),还有方差(不确定性). 为了做上面这件事,本文用无监督的rank,同时利用属性和结构来做一个inductive的算法. Model: Deep Gaussian Embedding这里对Gaussian Embedding的定义:每个节点的表示$h_i=\mathcal{N}(\mu_i, \sum_i)$ 一个均值向量和一个方差矩阵.注意这里只考虑对角方差矩阵. encoder这里为了得到节点的$h_i=\mathcal{N}(\mu_i, \sum_i)$,本文用神经网络对节点属性进行变换得到intermediate hidden representation,然后分别引出两个L维向量代表$\mu$和$\sum_i$. ranking本文认为rank可以比一阶二阶更好捕获局部和全局结构.所以基于节点对之间的最短路径长度,对节点对的相似性进行了排序.$$\Delta \left( \mathbf { h } _ { i } , \mathbf { h } _ { k _ { 1 } } \right) &lt; \Delta \left( \mathbf { h } _ { i } , \mathbf { h } _ { k _ { 2 } } \right) &lt; \cdots &lt; \Delta \left( \mathbf { h } _ { i } , \mathbf { h } _ { k _ { K } } \right) \quad \forall k _ { 1 } \in N _ { i 1 } , \forall k _ { 2 } \in N _ { i 2 } , \ldots , \forall k _ { K } \in N _ { i K }$$ 但是基于最短路径的rank足够好吗? 感觉最短路径长的节点相似性也可以很高. Similarity &amp; Dissimilarity这里用了非对称的KL来计算相似性(适用于无向图), 也可以用对称的JS.$$\Delta \left( \mathbf { h } _ { i } , \mathbf { h } _ { j } \right) = D _ { K L } \left( \mathcal { N } _ { j } | \mathcal { N } _ { i } \right) =\\\frac { 1 } { 2 } \left[ \operatorname { tr } \left( \Sigma _ { i } ^ { - 1 } \Sigma _ { j } \right) + \left( \mu _ { i } - \mu _ { j } \right) ^ { T } \Sigma _ { i } ^ { - 1 } \left( \mu _ { i } - \mu _ { j } \right) - L - \log \frac { \operatorname { det } \left( \Sigma _ { j } \right) } { \operatorname { det } \left( \Sigma _ { i } \right) } \right]$$ KL散度有一些固有的缺点 非对称(虽然有些论文说这是特点,适用于有向图). 在两个分布相差较小的时候,KL散度可能无意义. 不满足三角不等式. 所以一些论文尝试换距离,比如Wasserstein 距离.见18KDD_DVNE Loss$$\mathcal { L } = \sum _ { i } \sum _ { k &lt; l } \sum _ { j _ { k } \in N _ { i k } } \sum _ { j _ { l } \in N _ { i l } } \left( E _ { i j _ { k } } ^ { 2 } + \exp ^ { - E _ { i j _ { l } } } \right) = \sum _ { \left( i , j _ { k } , j _ { l } \right) \in \mathcal { D } _ { t } } \left( E _ { i j _ { k } } ^ { 2 } + \exp ^ { - E _ { i j _ { l } } } \right) \\E _ { i j } = D _ { K L } \left( \mathcal { N } _ { j } | \mathcal { N } _ { i } \right)$$ 最小化这个loss: Ijk比较近,能量比较小; Ijl比较远,能量比较大. 为了使得整体最小,需要ijl的能量指数衰减. 可以理解为,对ijk的约束比较大,要很近;对较远的点(负样本)约束比较小. 从而实现了类似margin的作用. Sampling strategy上述loss需要在整个图上算,这里利用采样的方法来近似.$$\text{replace}\sum _ { \left( i , j _ { k } , j _ { l } \right) \in \mathcal { D } _ { t } } \text { with } \mathbb { E } _ { \left( i , j _ { k } , j _ { l } \right) \sim \mathcal { D } _ { t } }$$但是直接采样会导致优化过程中对low-degree的点更新较少.本文提出一种an alternative node-anchored sampling strategy:以每个点为一点,采样他们的高阶邻居,组成一批数据进行训练. 然后这种策略依然有着自己的缺点,他估计的梯度是有偏的. Diss 本文在inductive的时候不需要结构信息,而之前的方法graphsage则需要.究其原因,是本文在学习节点表示的时候仅仅利用了属性信息,而结构信息用来计算loss优化模型. 没有属性时,可以用onehot编码来代替属性. 一般能做到O(E)时间复杂度就不错了,这里只有O(N)可以说是相当高效了 Experiment 本文可以处理有向图,但是一些baseline只支持无向图,所以要把有向图无向化,这会给这些baseline一些优势 有些试验需要基于L维向量进行比较,针对这些实验本文学的是L/2维度的, 然后把均值和向量拼接起来进行试验.这种方法感觉就比较粗暴,没有很好利用他们之间的关系.之后的DVNE就较好的融合了均值方差. 链路预测这里用the negative energy $-E_{ij}$作为边存在的概率, 节点的维度L由均值和方差拼接而成. 节点分类这里按照前面的介绍应该是均值和方差拼接为长度为L的向量,但是在代码的example里面,只用了均值. 采样策略相对于随机采样,效果提升不多,但是极大的降低了梯度的方差 不确定性节点的不确定性应该与它邻居的多样性(种类)有关. 邻居种类越少, 节点越确定. Summary本文提出了Graph2Gauss来捕获节点的不确定性,模型感觉也是简洁有效,实验效果也不错.尤其是是没有属性的G2G_oh效果也很好,证明了gaussian embedding的优势.但是本文在求均值和方差所用的模型太简单(全连接),并且没有利用结构信息,这样可能导致学到的gaussian embedding不够好. 可以试试用GNN搞一波.此外,那个ranking信息感觉也有争议.最短路径某些情况下也许不够好. 最后,在进行试验的时候只是简单的将均值方差拼接,有没有更好了方式可以反映他们之间的关系? 且听下回分解&lt;&lt;18KDD_DVNE_Deep Variational Network Embedding in Wasserstein Space&gt;&gt;.]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Network Embedding</tag>
        <tag>Gaussian Embedding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[83. Remove Duplicates from Sorted List]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F09%2F25%2Fhexo_leetcode%2F083.Remove_duplicates_from_sorted_list%2F</url>
    <content type="text"><![CDATA[#83. Remove Duplicates from Sorted List 题目:https://leetcode.com/problems/remove-duplicates-from-sorted-list/ 难度: Easy 60ms 53%how 注意是sort的, 所以重复数字都是相邻的 重复数字可能有多次如 1-&gt;1-&gt;1 1234567891011121314151617181920# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def deleteDuplicates(self, head): """ :type head: ListNode :rtype: ListNode """ cur = head while cur: # 因为这里重复可能有多次, 所以用while而不是if #检查cur.next有没有, 尾部判断 while cur.next and cur.val == cur.next.val: cur.next = cur.next.next cur = cur.next return head dummy 大法 123456789101112class Solution(object): def deleteDuplicates(self, head): """ :type head: ListNode :rtype: ListNode """ dummy = head while head: while head.next and head.next.val == head.val: head.next = head.next.next # skip duplicated node head = head.next # not duplicate of current node, move to next node return dummy]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[62. unique paths]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F09%2F25%2Fhexo_leetcode%2F062.unique_paths%2F</url>
    <content type="text"><![CDATA[62. unique paths 不同路径难度: 中等 刷题内容 原题连接 https://leetcode.com/problems/unique-paths https://leetcode-cn.com/problems/unique-paths/description 内容描述 12345一个机器人位于一个 m x n 网格的左上角 （起始点在下图中标记为“Start” ）。机器人每次只能向下或者向右移动一步。机器人试图达到网格的右下角（在下图中标记为“Finish”）。问总共有多少条不同的路径？ 123456789101112131415161718例如，上图是一个7 x 3 的网格。有多少可能的路径？说明：m 和 n 的值均不超过 100。&gt; 示例 1:输入: m = 3, n = 2输出: 3解释:从左上角开始，总共有 3 条路径可以到达右下角。1. 向右 -&gt; 向右 -&gt; 向下2. 向右 -&gt; 向下 -&gt; 向右3. 向下 -&gt; 向右 -&gt; 向右&gt; 示例 2:输入: m = 7, n = 3输出: 28 DP 20ms 99%思路 DP两个点,递归公式和初始值 递归公式很好理解 $d[i][j]=d[i-1][j]+d[i][j-1]$, 因为这里只能向右下行走,所以只考虑当前点上方和左侧. 初始值需要考虑一下. 整个棋盘的左边界,左边界上的点没有左侧,只能从[0,0]一路向下,所以全是1 整个棋盘的上边界,上边界上的点没有上侧,只能从[0,0]一路向右,所以全是1 123456789101112131415161718class Solution(object): def uniquePaths(self, m, n): """ :type m: int :type n: int :rtype: int """ d = [[None]* n for _ in range(m)] for i in range(m): for j in range(n): if i== 0 or j == 0: d[i][j] = 1 else: d[i][j] = 0 for i in range(1, m): for j in range(1, n): d[i][j] = d[i-1][j] + d[i][j-1] return d[-1][-1] 组合 20ms 99% 思路 这里可以转化为排列组合问题,总共要走m+n-2步, 其中m-1步向下, n-1步向右 只有向左,向下两种选择,而且不考虑顺序问题,所以是$C_{m+n-2}^{m-1}$ 而且$C_n^m=C_n^{n-m}$ 所以先写个求阶乘,在套上述公式即可 12345678910111213class Solution(object): def uniquePaths(self, m, n): """ :type m: int :type n: int :rtype: int """ def fab(nums): res = 1 for i in range(1, nums+1): res *= i return res return fab(m+n-2) / (fab(n-1) * fab(m-1))]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[21. Merge Two Sorted Lists]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F09%2F25%2Fhexo_leetcode%2F021.Merge_two_sorted_lists%2F</url>
    <content type="text"><![CDATA[21. Merge Two Sorted Lists难度: Easy 刷题内容 原题连接 https://leetcode.com/problems/merge-two-sorted-lists/description/ 内容描述 123456Merge two sorted linked lists and return it as a new list. The new list should be made by splicing together the nodes of the first two lists.Example:Input: 1-&gt;2-&gt;4, 1-&gt;3-&gt;4Output: 1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4 48ms 77%How 新建一个哑节点, ListNode(0),最后返回的时候返回head.next,正好忽略第一个哑节点 判断大小,将链表的值依次放入, l1和l2有一个先取完, 未取完的部分拼接到后面即可 12345678910111213141516171819202122232425262728# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def mergeTwoLists(self, l1, l2): """ :type l1: ListNode :type l2: ListNode :rtype: ListNode """ cur = head = ListNode(0) while l1 and l2: if l1.val &lt; l2.val: cur.next = l1 l1 = l1.next else: cur.next = l2 l2 = l2.next cur = cur.next if l1: cur.next = l1 if l2: cur.next = l2 return head.next]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[160. Intersection of Two Linked Lists]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F09%2F25%2Fhexo_leetcode%2F160.Intersection_of_two_linked_lists%2F</url>
    <content type="text"><![CDATA[160. Intersection of Two Linked Lists题目:https://leetcode.com/problems/intersection-of-two-linked-lists/ 难度: Easy 如果两个linkedlist有intersection的话，可以看到，其实如果一开始我们就走到b2的话，那么我们就可以两个pointer一个一个的对比，到哪一个地址一样，接下来就是intersection部分。 12345A: a1 → a2 ↘ c1 → c2 → c3 ↗ B: b1 → b2 → b3 比较巧妙的数学解法，看下面的解释和代码 240ms 40% 假设两个链表交集前的长度分别为a和b, 交集后的长度为 c 则,len(L1) = a+c len(L2)=b+c 两个链表同时开始遍历, 遍历完自身之后,开始遍历对方 同步遍历,假定遍历完自己后,分别遍历对方了k1和k2步 这是a+c+k1 = b+c+k2,则k1=b, k2=a 123456789101112131415161718192021222324# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def getIntersectionNode(self, headA, headB): """ :type head1, head1: ListNode :rtype: ListNode """ pA = headA pB = headB while pA is not pB: if pA: pA = pA.next else: pA = headB if pB: pB = pB.next else: pB = headA return pA AC代码如下: 1234567891011class Solution(object): def getIntersectionNode(self, headA, headB): """ :type head1, head1: ListNode :rtype: ListNode """ pA, pB = headA, headB while pA is not pB: pA = pA.next if pA else headB pB = pB.next if pB else headA return pA Just count the number of moves by each pointer before they meet. One pointer will traverse entire list1 for N moves and then jump to the head of list1 to move (M-K) steps to intersection, where K represents the length of common part. Now the other pointer must also moved the same number of steps since they are both moved at the same time. The second pointer traverses the entire list2 for M steps and jumped to the head of list1 to move (N-K) steps. So the loop finished with M+N-K times.详见zzg_zzm的评论 This algorithm is sooooo perfect! I was wonder if the running time is O(n+m), but later I figured out that the actually running time is just: m+n for non-intersection case With intersection: Suppose for LL-A, it’s a+b=n, a is the # of nodes before intersection Suppose for LL-B, it’s c+b=m, c is the # of nodes before intersection Thus the actual running time is a+b+c = n+c = m+a. Actually, when b=0, this just stands for the case with no intersection with a+b+c=n+m]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[203. Remove Linked List Elements]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F09%2F25%2Fhexo_leetcode%2F203.Remove_linked_list_elements%2F</url>
    <content type="text"><![CDATA[203. Remove Linked List Elements题目:https://leetcode.com/problems/remove-linked-list-elements/ 难度: Easy 76ms 40%AC代码如下: 注意linked list 的首个node就可能需要删除,所以建立一个哑节点 dummy, dummy.next = head 链表中的删除实际是跳过该节点 1234567891011121314151617181920class Solution(object): def removeElements(self, head, val): """ :type head: ListNode :type val: int :rtype: ListNode """ dummy = ListNode(-1) dummy.next = head cur = dummy while cur.next: #注意这里从cur.next开始判断,因为如果到了cur且cur.val==val,这时已经无法跳过cur了 if cur.next.val == val: cur.next = cur.next.next else: cur = cur.next return dummy.next]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Sentences 1-100]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F06%2F11%2Fenglish_note%2F1-100%2F</url>
    <content type="text"><![CDATA[One of the holy grails of machine learning is to compute meaningful low-dimensional embeddings for high-dimensional complex objects. That ability is crucial to tackle advanced tasks, such as inference on texts using word embeddings [Pennington et al., 2014, Bojanowski et al., 2016], image understanding [Norouzi et al., 2014] or concise representations for nodes in a huge graph [Grover and Leskovec, 2016]. While learning and performing inference on homogeneous networks have motivated a large amount of research, few work exists on heterogeneous networks and there are open and challenging issues for existing methods that were previously developed for homogeneous networks. We demonstrate empirically that Walklets’s latent representations encode various scales of social hierarchy, and improves multi-label classification results over previous methods on a variety of real world graphs Embedding complex objects as vectors in low dimensional spaces is a longstanding problem in machine learning. To learn the community embedding, we hinge upon the insight that We propose in this work a comprehensive framework for probabilistic embeddings, in which point embeddings are seamlessly handled as a particular case. The cornerstone of our approach lies in the use of the 2-Wasserstein distance Classification in heterogeneous networks, representative of real world media, is much more recent with only a few attempts for now. Many of them rely on the idea of mapping an heterogeneous network onto an homogeneous network so that classical relational techniques are then used [8, 10, 14, 2]. Motivated by the ideology of distributed representation learning in recent years, network representation learning (NRL) is proposed to address these issues. NRL aims to learn a real-valued vector for each vertex to reflect its network in-formation Both methods leverage random walks to achieve this and utilize the skip-gram model to learn the repre-sentation of a node that facilitates the prediction of its structural context—local neighborhoods—in a homogeneous network These low-dimensional representations are distributed; meaning each social phenomena is expressed by a subset of the dimensions and each dimension contributes to a subset of the social concepts expressed by the space. This ubiquitous form of representing information has been studied in many disciplines. For instance, in connection with statistical learning, it draws much input from Relational Learning, which aims at capturing the correlation between connected objects, especially in the presence of uncertainty To lift this limitation, several reformulations of elliptical distributions have been proposed to handle degenerate scale matrices Graph embedding is a main stream graph representation framework , which aims to project a graph into a low-dimensional space for further analytic tasks, such as classification, clustering and so on. Graph data is becoming increasingly popular, thanks to the proliferation of various social media (e.g., blogs, Flickr, Twitter) and many other kinds of information networks (e.g., DBLP, knowledge graphs) For example, taking a user-selected meta-path &lt; author − paper − venue − paper − author &gt; (i.e., shared-venues) to find similar authors, the projection will generate a homogeneous network consisting of only authors, but it loses important network structure information (e.g., venues and papers) PathSim oly capture desired semantic similarity between peer objects They do not fully exploit the correlations between the different node labels or characteristics. Objects of different types and links carry different semantic meanings,can identify objects that not only are strongly connected but also share similar type The assumption we make in the paper is that nodes of different types do influence each other, so that labels of the different node types will be inter-dependent; modeling this dependence between different node types is thus important for accurate node classification or labeling and cannot be achieved via classical homogeneous network formulations Without loss of generality, we define the relations between vertices as a set of labels, instead of a single label It is important to mention that the proposed algorithm learns a latent representation of the network nodes so that all nodes, irrespectively of their type, will share a common latent space. Such representation will then be effectively used to infer the categories The main contributions are :1) an novel method for mapping nodes and edges onto a common latent sapce:This mapping exploits…… we proposed a general framework to lean etwork represention in heterogeneous networks The paper is organized as follows: Section 2 presents the notations, and introduces some background concerning Section 3 describes the proposed algorithm and…Section 4 presents experimental results on ?? datasets, and a qualitative analysis of the latent representions learned by the model a regularized term is adopted to avoid the over-fitting problem. That is due to the fact that we seek to preserve the inference ability that can match the vertices containing the similar contexts, rather than match those vertices containing exactly the same context A heterogeneous network will be modeled as an undirected weighted graph with a node type associated to each node. algorithm based on mete-path has teo drawbacks：1）Dependencies between the different the two types of nodes are lost,since the latent represention of nodes are only dependent of the nodes of its neighbors of the same type. However, these homogeneous models cannot capture the information about entity types nor about relations across different typed entities in HINs. Various similarity metrics have been designed for vector spaces in different tasks, such as cosine similarity [24], Jaccard coefficient [14], and the p -norm distance Unlike the reconstruction task, this task predicts the future links instead of reconstructing the existing links. In addition, we add Common Neighbor in this task because it has been proved as an effective method to do link prediction . XXX alse extends the DeepWalk in generating random walks fed to SkipGram by only selecting unvisited neighbors of the predecessor node in the early stage it’s can consider more information in diferent edges2 choosing neighbors of the predecessors node with bias probability w.r.t. different connection weights, meaning the co-occurrence frequency of two nodes in the corpus reflects their correlation The key difference from existing embedding methods for a single net- work is that there are three kind of edges and two type of latent spaces corresponding to two networks The experiment is to evaluate the effectiveness of the latent features learned by the proposed EOE model on com-mon data mining tasks including visualization, link predic- tion, multi-class classification, and multi-label classification. The basic idea of our approach is to preserve the HIN structure information into the learned embeddings, such that vertices which co-occur in many path instances turn to have similar embeddings. Since learning rich vertex embeddings does not require the accurate probability of each path instance, we adopt the NCE for optimization, which has been proved to significantly accelerate the training without cutting down the embedding qualities However, the discriminant power of the node embeddings maybe improved by considering the node label information and the node attribute information. Recently, motivated by the success of the unsupervised distributed representation learning techniques in the natural language processing area, several novel network embedding methods have been proposed to learn distributed dense representations for networks Instead of handcrafed network feature design, these representation learning methods enable the automatic discovery of useful and meaningful (latent) features from the “raw networks.” Yet a large number of social and information networks are heterogeneous in nature, involving diversity of node types and/or relationships between nodes How do we efectively preserve the concept of “word-context” among multiple types of nodes, e.g., authors, papers, venues, organizations, etc Recently, network embedding has been utilized to fill the gap of applying tuple-based data mining models to networked datasets by learning embeddings which preserve the network structure For the implementation of the algorithm to solve the EII model, the dimension of embeddings is set as… Different from existing approaches that rely heavily on feature engineering, we propose to use network embedding approach to address the problem, which can automatically represent nodes into lower dimensional feature vectors Although feature engineering can incorporate prior knowledge of the problem and network structure, usually it is time-consuming, problem specific (thus not transferable), and the extracted features may be too simple for complicated data sets A key idea behind network embedding is learning to map nodes into vector space, such that the proximities among nodes can be preserved From these two examples, it is easy to see that in a heterogeneous network, even compare two nodes of the same type (e.g. paper), going from different paths can lead to different semantic meanings. The networked data is usually high-dimensional and sparse, as there can be many nodes but the links are usually sparse [1]. This brings challenges to represent nodes in the network Most of existing network embedding techniques [17, 26, 25] are based on the idea that, embeddings of nodes can be learned by neighbor prediction, which is to predict the neighborhood given a node, i.e. the linking probability P(j|i) from node i to node j. The former focuses more on the direct information related to the specific task, while the latter can better explore more global and diverse information in the heterogeneous information network. We study the hyper-parameters ω, which is the trade-off term for combing A and B Machine learning applications in net-works (such as network classification [16,37], content rec- ommendation [12], anomaly detection [6], and missing link prediction [23]) must be able to deal with this sparsity in order to survive. These low-dimensional representations are distributed; meaning each social phenomena is expressed by a subset of the dimensions and each dimension contributes to a subset of the social concepts expressed by the space. It is this connection to local structure that motivates us to use a stream of short random walks as our basic tool for extracting information from a network The objective of a good embedding is to preserve the proximity (i.e., similarity) between vertices in the original graph The availability and growth of large networks, such as social net- works, co-author networks, and knowledge base graphs, has given rise to numerous applications that search and analyze information in them The proximity among objects in a HIN is not just a measure of closeness or distance, but it is also based on semantics The heterogeneity of nodes and edges in HINs bring challenges, but also opportunities to support important applications Deep Learning’s recent successes have mostly relied on Convolutional Networks,which exploit fundamental statistical properties of images, sounds and video data:the local stationarity and multi-scale compositional structure, that allows expressing long range interactions in terms of shorter, localized interactions In this paper we consider the general question of how to….. In particular, we deveplop… In this work, we are interested in generalizing convolutional neural networks(CNNs) from low-dimensional regular grids, where image, video and speech arerepresented, to high-dimensional irregular domains, such as social networks, brainconnectomes or words’ embedding, represented by graphs User data on social networks, gene data on biological regulatory networks, log data on telecom-munication networks, or text documents on word embeddings are important examples of data lyingon irregular or non-Euclidean domains which can be structured with graphs, which are universalrepresentations of heterogeneous pairwise relationships To address these issues, Network Representation Learning (NRL) aims at the possibility of encoding node information in a unified continuous space Despite the success of embedding individual nodes for graph analytics, we notice that an important concept of embedding communities (i.e., groups of nodes) is missing in the literature Our major insight to learn the community embedding is hinged upon the mutual reinforcement between node embedding and community embeddin With the success of deep learning in recent years, there is a marked switch from hand-crafted features to those that are learned from raw data in recommendation research Besides, this research sheds new light on the usage of heterogeneous information in the knowledge base, which can be consumed in more application scenarios However, for relational network classification, DeepWalk can be suboptimal as it lacks a mechanism to optimize the objective of the target task The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data Good representations are expressive, meaning that areasonably-sized learned representation can capture a hugenumber of possible input configurations One of the challenges of representation learning that distinguishes it from other machine learning tasks such as classification is the difficulty in establishing a clear objective, or target for training As the core of semantic proximity search, we have to measure the proximity on a heterogeneous graph, whose nodes are various types of objects. With recent development on graph embedding, we see a good chance to avoid feature engineering for semantic proximity search The core problem of semantic proximity search is how to measure the proximity on a heterogeneous graph with various types of objects Embedding nodes is an indirect approach to learn proximity, because it lacks an explicit representation for the network structure between two possibly distant nodes To enable knowledge discovery from such networked data, network representation learning (NRL) aims to learn vector representations for network nodes, such that off-the-shelf machine learning algorithms can be directly applied Existing studies on network representation learning (NRL) have shown that NRL can effectively capture network structure to facilitate subsequent analytic tasks, such as node classification [3], anomaly detection [4], and link prediction Despite its great potential, learning useful network representations faces several challenges. (1) Sparsity (2) Structure preserving (3) Rich-Content How to leverage rich content information and its interplay with network structure for representation learning remains an open problem. By simultaneously integrating homophily, structural context, and node content, HSCA effectively embeds a network into a single latent representation space that captures the interplay between the three information sources. Graph embedding aims to reduce the dimensionality of the original data in the vector-based form, while NRL seeks to learn effective vector representations of network nodes to facilitate downstream network analytic tasks. This approach turns the discrete topology of the relations into a continuous one, enabling the design of efficient algorithms and potentially benefitting many applications. This source of information has been ignored by most of the current recommender systems while it can potentially alleviate the sparsity problem and improve the quality of recommendations With the large volume of new information created every day, determining the validity of information in a knowledge graph and filling in its missing parts are crucial tasks for many researchers and practitioners A number of KGC algorithms have been developed in recent years, and the most successful models all have one thing in common: they use low-dimensional embedding vectors to represent entities and relationships. With these considerations in mind, in the present work we rethink some of the basic decisions made by previous models to create a projection embedding model (ProjE) for KGC. The newest contributions in this line of work focus primarily on the changes in how the embedding planes are computed and/or how the embeddings are combined To recap, the contributions of the present work are as follows: In recent years, a series of approaches have been proposed to embed a knowledge graph into a continuous vector space while preserving the properties of the knowledge graph Generally speaking, precise link prediction would improve the feasibility of knowledge completion, the effectiveness of knowledge reasoning, and the performance of many knowledge-related tasks What is the best way to describe a user in a social network with just a few numbers? Mathematically, this is equivalent to assigning a vector representation to each node in a graph, a process called graph embedding Through a graph embedding, we are able to visualize a graph in a 2D/3D space and transform problems from a non-Euclidean space to a Euclidean space, where numerous machine learning and data mining tools can be applied Numerous embedding algorithms have been proposed in data mining, machine learning and signal processing communities. Naturally, it is necessary for network embedding to preserve the first-order proximity because it implies that two vertexes in real world networks are always similar if they are linked by an observed edge Experimental results using reviews from 50 product types show significant improvements over state- of-the-art baseline models. We conduct extensive experiments to demonstrate the performance of BASS, concluding that our approach significantly outperform state-of-the-art approaches. The advantage is multi-folded. Firstly,.. Secondly，…]]></content>
      <categories>
        <category>English</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Graph Neural Network之GCN的理论]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F06%2F11%2Fpaper_note%2Fgcn%2F</url>
    <content type="text"><![CDATA[图神经网络,尤其是图卷积,在近两年的顶会上疯狂刷脸. 这里整理了部分相关文献,并准备从GCN系列开始梳理. 最早的图卷积网络都是从谱域出发,利用图傅里叶变换,滤波器来设计. Ref: https://tkipf.github.io/graph-convolutional-networks/]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Graph Neural Network</tag>
        <tag>GCN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sentences 101-200]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F06%2F11%2Fenglish_note%2F101-200%2F</url>
    <content type="text"></content>
      <categories>
        <category>English</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F06%2F11%2Ftemplate%2F</url>
    <content type="text"><![CDATA[s]]></content>
      <categories>
        <category>cate</category>
      </categories>
      <tags>
        <tag>Network Embedding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[237. Delete Node in a Linked List]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F05%2F14%2Fhexo_leetcode%2F237.%20Delete%20Node%20in%20a%20Linked%20List%2F</url>
    <content type="text"><![CDATA[237. Delete Node in a Linked Listhttps://leetcode.com/problems/delete-node-in-a-linked-list/description/ 28ms 98% 输入只给了节点,并没有给整个链表, 假设 1234 删除2 将该node后面一个节点的值付给该node, 变成1334 然后将该node的 next跳过一步即可(略过第二个3), 1-3(原来的2)-4 1234567891011121314# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def deleteNode(self, node): """ :type node: ListNode :rtype: void Do not return anything, modify node in-place instead. """ node.val = node.next.val node.next = node.next.next 237. Delete Node in a Linked List题目:https://leetcode.com/problems/delete-node-in-a-linked-list/ 难度:Easy 这道题，第一感觉，像删链表一样来删，把所有的node val前移一个,但是有个问题，为什么tail那个node还是存在？哼(ˉ(∞)ˉ)唧.. 已经被解答： http://stackoverflow.com/questions/38879291/python-delete-a-node-in-linked-list-given-just-access-to-that-node 另外一个O（1）的办法更好，把后一个node的val移到待删这个节点，并且把node.next = node.next.next​题目说了不会删最后一个点，所以node.next.next一定存在，所以直接让node的val等于它next的val，然后让node的next指向它的next的next，举个例子：​ 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;None,要删除第四个节点，就让4变成5，然后让第四个节点指向第五个节点的next，这样原来的第四个节点就不存在了，虽然原来的第五个节点仍然存在且指向None，变成了1-&gt;2-&gt;3-&gt;5-&gt;None-&lt;5​ 123456789O(1)时间class Solution(object): def deleteNode(self, node): """ :type node: ListNode :rtype: void Do not return anything, modify node in-place instead. """ node.val = node.next.val node.next = node.next.next 123456789101112O(n)时间class Solution(object): def deleteNode(self, node): """ :type node: ListNode :rtype: void Do not return anything, modify node in-place instead. """ while node.next: node.val = node.next.val prev, node = node, node.next # clear reference to tail prev.next = None]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[142. Linked List Cycle II]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F05%2F13%2Fhexo_leetcode%2F142.%20Linked%20List%20Cycle%20II%2F</url>
    <content type="text"><![CDATA[142. Linked List Cycle II思路1 98% 70msHow 首先判断有无环,slow和fast法, 其实slow和fast会相遇在环内的某个位置meeting, 不一定在入口entry相遇 假设起点到环长度为L, 环长度为C, 相遇时,快指针走了L + meeting + N*C, N为圈数 慢指针走了 L + meeting, 还差C - meeting回到入口 而因为快指针走2步, 慢指针走1步,所以步数是2倍关系 2(L + meeting) = L + meeting + N C 得到L = N *C - meeting = C - meeting, 因为C是循环的 快慢指针相遇时, 慢指针还差C-meating回到入口,这时新建一个指针从起点开始遍历, 新指针和慢指针相遇的位置就是入口 123456789101112131415161718192021222324252627# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def detectCycle(self, head): """ :type head: ListNode :rtype: ListNode """ if head is None or head.next is None: return None entry =head slow = head fast = head while fast and fast.next: slow = slow.next fast = fast.next.next if slow == fast: while slow != entry: entry = entry.next slow = slow.next # if slow == entry: return entry return None]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[141. Linked List Cycle]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F05%2F13%2Fhexo_leetcode%2F141.%20Linked%20List%20Cycle%2F</url>
    <content type="text"><![CDATA[Linked List Cycle https://leetcode.com/problems/linked-list-cycle/description/ 思路1 73ms 63%经典的快慢指针, 快指针走2步, 慢指针走1步,* 如果有环,慢指针会赶上快指针,两者相遇在环内的某个位置 ​ 12345678910111213141516171819202122# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def hasCycle(self, head): """ :type head: ListNode :rtype: bool """ slow = head fast = head # fast走的更快,所以只要判断fast的合法即可 while fast and fast.next: slow = slow.next fast = fast.next.next # 这里要判断slow==fast,而不能用slow.val=fast.val, 因为没环的也可能slow.val=fast.val if slow == fast: return True return False 思路2 98% 66ms将遍历过的链表反向, 如果有环,将会遍历环之后,从环的入口返回起点 123456789101112131415161718192021# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def hasCycle(self, head): """ :type head: ListNode :rtype: bool """ pre, cur = None, head while cur: # pre.next = cur, cur.next #这里变量位置不能变,不然会报错,还没搞明白 pre, cur.next, cur = cur, pre, cur.next if cur == head: return True return False]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[26. Remove Duplicates from Sorted Array]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F05%2F05%2Fhexo_leetcode%2F026.%20Remove%20Duplicates%20from%20Sorted%20Array%2F</url>
    <content type="text"><![CDATA[26. Remove Duplicates from Sorted Arrayhttps://leetcode.com/problems/remove-duplicates-from-sorted-array/#/description Given a sorted array, remove the duplicates in place such that each element appear only once and return the new length.Do not allocate extra space for another array, you must do this in place with constant memory.For example,Given input array nums = [1,1,2],Your function should return length = 2, with the first two elements of nums being 1 and 2 respectively. It doesn’t matter what you leave beyond the new length. 注意 有序数组 只能O(1)的额外空间 除了返回length之外, 原始数组的前length的元素应该是不重复的数字 例如[1,1,2], 返回length=2和[1,2] 思路1 99% 遍历,遇到不同的数字就flag+=1 并把该数字放到前面 Time O(n)只需遍历一次 Space O(1) 1234567891011121314151617class Solution: def removeDuplicates(self, nums): """ :type nums: List[int] :rtype: int """ if len(nums) == 0: return 0 flag = 1 cur_num = nums[0] for num in nums: if cur_num != num: flag += 1 cur_num = num nums[flag - 1] = cur_num return flag]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python Basic Skill]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F05%2F05%2F0%20python%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[字典加减https://www.jianshu.com/p/58d21289ff5c 字典相加: 合并两个字典, 相同键的值相加 字典相减 cnt1 - cnt2: 如果cnt1包含cnt2 对应的键值相减 cnt2包含cnt1, 减不了,返回空* 浅拷贝和深拷贝那么浅拷贝（shallow copy）与深拷贝（deep copy）有什么区别呢？ 对于不可变对象，比如整数、字符串、元组、还有由这些不可变对象组成的集合对象，浅拷贝和深拷贝没有区别，都是拷贝一个新对象 两者的区别在于拷贝组合对象，比如列表中还有列表，字典中还有字典或者列表的情况时，浅拷贝只拷贝了外面的壳子，里面的元素并没有拷贝，而深拷贝则是把壳子和里面的元素都拷贝了一份新的。 对列表的切片拷贝 z[:] 或者是调用对象的copy方法 list.copy() 都属于浅拷贝 b = a 引用,改变b, a也随之改变 b = a[:] 这里是浅拷贝,但是list的浅拷贝等价于深拷贝, 改变b不影响a 不过最好还是用from copy import deepcopy b = deepcopy(a) 初始化二维列表正确初始化二维列表[m,n], 用None来占位 1d = [[None] * n for _ in range(m)]]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[1. Two Sum I]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F04%2F14%2Fhexo_leetcode%2F001.%20Two%20Sum%20I%2F</url>
    <content type="text"><![CDATA[1. Two Sum I 100%https://leetcode.com/problems/two-sum/#/description Given an array of integers, return indices of the two numbers such that they add up to a specific target.You may assume that each input would have exactly one solution, and you may not use the same element twice.Example:Given nums = [2, 7, 11, 15], target = 9,Because nums[0] + nums[1] = 2 + 7 = 9,return [0, 1].Subscribe to see which companies asked this question. 思路 新建一个字典来保存 target - nums[i] dict{target - nums[i] : i} 继续搜索，当nums[j] in dict 时，则 nums[i] + nums[j] = target Time O(n) 遍历字典一次 Space O(n) 新建字典 12345678910111213141516# time O(n)# space O(n)class Solution(object): def twoSum(self, nums, target): """ :type nums: List[int] :type target: int :rtype: List[int] """ dict = &#123;&#125; for i in range(len(nums)): if nums[i] in dict: return [dict[nums[i]], i] else: dict[target - nums[i]] = i 1234567891011121314更干净的写法class Solution: def twoSum(self, nums, target): """ :type nums: List[int] :type target: int :rtype: List[int] """ d = &#123;&#125; for idx, num in enumerate(nums): if num in d: return [d[num], idx] else: d[target - num] = idx $s_1^1=10$$$s_1=a \s_2=b$$ 12]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
</search>
