<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[18AAAI_DHNE_Structural Deep Embedding for Hyper-Networks]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F12%2F13%2F18AAAI_DHNE%2F</url>
    <content type="text"><![CDATA[本文来自network embedding大佬Peng Cui老师组。针对超图，超边设计了一种深度模型来学习embedding。整体感觉是超图版本的SDNE。 Motivation本文首先研究了如何在embedding中保持超边的不可分割性，同时证明了线性的相似度函数无法保持超边相似性。这样就迫使我们不得不设计一个非线性的深度模型来保持超边的不可分割性。 Model整个模型分2块，1. 类似SDNE的结构保持部分，2. N-tuplewise similarity function。 Structure Preserve（AutoEncoder）这里用了一个SDNE中的AutoEncoder来对节点的邻接向量（邻接矩阵某一行）来进行编码。 那么超图中的邻接矩阵是怎么定义的呢？ 这里的邻接是指可以通过超边连接。例如超边实例e1：（a1，b1，c1）和e2：（a1， b2， c3） 那么a1-e1-b1， a1-e1-c1, a1-e2-b2, a1-e2-c3,可以得到a1的基于超边的邻居有b1,c1,b2,c3. 上述过程用矩阵的形式表达为， 节点-超边实例的超边连接矩阵 $\mathbf{H}$，这样$\mathbf{H}\mathbf{H}^T$ 就得到了 节点-超边实例-节点 的邻接矩阵， 然后做下处理（$-\mathbf{D}_v$）即可得到超图的邻接矩阵 $\mathbf{A}$。 不同类型节点的邻接矩阵一样吗？ 不一样，针对t类型节点其对应的邻接矩阵为$\mathbf{A}^t$。 这里将不同类型的节点映射到不同空间。整体loss \mathcal { L } _ { 2 } = \sum _ { t } \left\| \operatorname { sign } \left( \mathbf { A } _ { i } ^ { t } \right) \odot \left( \mathbf { A } _ { i } ^ { t } - \hat { \mathbf { A } } _ { i } ^ { t } \right) \right\| _ { F } ^ { 2 }N-tuplewise similarity function这个函数主要是为了保持超边（N元组）的相似性，这里的公式都是以3元组（a，b，c）为例,输入是$\left( \mathbf { X } { i } ^ { a } , \mathbf { X } { j } ^ { b } , \mathbf { X } _ { k } ^ { c } \right)$ 。 三元组的联合表示: \mathbf { L } _ { i j k } = \sigma \left( \mathbf { W } _ { a } ^ { ( 2 ) } * \mathbf { X } _ { i } ^ { a } + \mathbf { W } _ { b } ^ { ( 2 ) } * \mathbf { X } _ { j } ^ { b } + \mathbf { W } _ { c } ^ { ( 2 ) } * \mathbf { X } _ { k } ^ { c } + \mathbf { b } ^ { ( 2 ) } \right) 三元组的相似性 \mathbf { S } _ { i j k } \equiv \mathcal { S } \left( \mathbf { X } _ { i } ^ { a } , \mathbf { X } _ { j } ^ { b } , \mathbf { X } _ { k } ^ { c } \right) = \sigma \left( \mathbf { W } ^ { ( 3 ) } * \mathbf { L } _ { i j k } + \mathbf { b } ^ { ( 3 ) } \right) 分类loss, 存在超边label就是1 \mathcal { L } _ { 1 } = - \left( \mathbf { R } _ { i j k } \log \mathbf { S } _ { i j k } + \left( 1 - \mathbf { R } _ { i j k } \right) \log \left( 1 - \mathbf { S } _ { i j k } \right) \right)一句话： 保持N元组的相似度函数，就把N元组作为输入，经过NN映射得到一个相似度。 整个模型图如下: Experiments本文的实验不同于一般的network embedding实验。 数据： 数据集都是只针对于单种hyperedge，那么如果有多种hyperedge本文的模型处理不了，应该怎么做？ 对比算法： DHNE是基于超边（三元组）的算法，数据集也是超图。一些pairwise的算法如Deepwalk和line也都作为baseline。这样怎么去和baseline对比呢？ 这里首先将超图基于图1（c）的clique expansion技术转化为普通图 对于超图重构和超边预测，这里对一条超边里面的节点分别计算pairwise的相似性，并取mean或者minimum来做预测 网络重构（超图重构）这里是重构超图，文中的“We use the learned embeddings to predict the links of origin networks”的link也是超边。 链路预测（超边预测）对于DHNE， 训练好的模型一方面可以得到不同类型节点的embedding，一方面可以得到计算超边的相似性函数 这里做了2种不同的任务 隐藏20%的超边，用80%的边来训练网络。 对超边进行采样（改变稀疏度），并重复任务1 Summary本文的首先做了超图的embedding问题，又有一定的理论证明，最后设计模型验证了效果。感觉两个亮点1.是分析了线性相似度函数的局限性 2.N-tuplewise similarity function的设计。 那么如果一个超图中有多种类型的超边该怎么处理呢？]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Network Embedding</tag>
        <tag>Hypergraph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[word2vec, skip-gram, negative sampling]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F12%2F13%2Fword2vec%2F</url>
    <content type="text"><![CDATA[最新版见 https://jhy1993.github.io/Jhy1993.github.io/2018/12/13/word2vec/ word2vecskip-gram给定一个单词 $w$ 和其窗口内的上下文 $c$, 我们考虑条件概率$p(c|w;\theta)$ \arg \max _ { \theta } \prod _ { w \in T e x t } \left[ \prod _ { c \in C ( w ) } p ( c | w ; \theta ) \right]其中, $C(w)$是单词$w$的上下文. 把$w$和$c$ 这种从文本中抽取的的word-pair构建一个集合$D$ \arg \max _ { \theta } \prod _ { ( w , c ) \in D } p ( c | w ; \theta )skip-gram模型根据中心词来预测上下文的条件概率可以写为 p ( c | w ; \theta ) = \frac { e ^ { v _ { c } \cdot v _ { w } } } { \sum _ { c ^ { \prime } \in C } e ^ { v _ { c ^ { \prime } } \cdot v _ { w } } }其中$v_w$和$v_c$分别是中心词和上下文的向量表示.上式分母要对整个上下文求和,计算量太大,我们尝试来减小整个计算量.比如负采样技术 负采样首先把上式取log并展开 \underset { \theta } { \arg \max } \sum _ { ( w , c ) \in D } \log p ( c | w ) = \sum _ { ( w , c ) \in D } \left( \log e ^ { v _ { c } \cdot v _ { w } } - \log \sum _ { c ^ { \prime } } e ^ { v _ { c ^ { \prime } } \cdot v _ { w } } \right)这里$( w , c ) \in D$ 意味着我们可以从语料中采样出他们, 那么我们定义 $( w , c ) $可以从D采样出的概率$p(D=1|w,c)$ ,与此对应,如果也可以定义$p(D=0|w,c)$ , 即$( w , c ) \notin D$. 我们最大化所有可能出现的$( w , c ) \in D$的概率 { \arg \max _ { \theta } \prod _ { ( w , c ) \in D } p ( D = 1 | w , c ; \theta ) } \\ { = \arg \max _ { \theta } \log \prod _ { ( w , c ) \in D } p ( D = 1 | w , c ; \theta ) } \\ { = \arg\max _ { \theta } \sum _ { ( w , c ) \in D } \log p ( D = 1 | w , c ; \theta ) }$p ( D = 1 | w , c ; \theta ) $ 可以定义为(内积较大,则容易出现在同一个窗口内): p ( D = 1 | w , c ; \theta ) = \frac { 1 } { 1 + e ^ { - v _ { c } \cdot v _ { w } } }这样目标函数就转为 \arg \max _ { \theta } \sum _ { ( w , c ) \in D } \log \frac { 1 } { 1 + e ^ { - v _ { c } \cdot v _ { w } } }这里$D’​$就是随机采样的$(w,c)​$,我们假定他们全是错误的负样本 \arg \max _ { \theta } \prod _ { ( w , c ) \in D } p ( D = 1 | c , w ; \theta ) \prod _ { ( w , c ) \in D ^ { \prime } } p ( D = 0 | c , w ; \theta ) \\ = \arg \max _ { \theta } \prod _ { ( w , c ) \in D } p ( D = 1 | c , w ; \theta ) \prod _ { ( w , c ) \in D ^ { \prime } } ( 1 - p ( D = 1 | c , w ; \theta ) ) \\ { = \arg \max _ { \theta } \sum _ { ( w , c ) \in D } \log p ( D = 1 | c , w ; \theta ) + \sum _ { ( w , c ) \in D ^ { \prime } } \log ( 1 - p ( D = 1 | w , c ; \theta ) ) \\ = \arg \max _ { \theta } \sum _ { ( w , c ) \in D } \log \frac { 1 } { 1 + e ^ { - v _ { c } \cdot v _ { w } } } + \sum _ { ( w , c ) \in D ^ { \prime } } \log \left( 1 - \frac { 1 } { 1 + e ^ { - v _ { c } \cdot v _ { w } } } \right) \\ = \arg \max _ { \theta } \sum _ { ( w , c ) \in D } \log \frac { 1 } { 1 + e ^ { - v _ { c } \cdot v _ { w } } } + \sum _ { ( w , c ) \in D ^ { \prime } } \log \left( \frac { 1 } { 1 + e ^ { v _ { c } \cdot v _ { w } } } \right) } \\如果我们令$\sigma ( x ) = \frac { 1 } { 1 + e ^ { - x } }$ ,则 \begin{aligned} \arg \max _ { \theta } & \sum _ { ( w , c ) \in D } \log \frac { 1 } { 1 + e ^ { - v _ { c } \cdot v _ { w } } } + \sum _ { ( w , c ) \in D ^ { \prime } } \log \left( \frac { 1 } { 1 + e ^ { v _ { c } \cdot v _ { w } } } \right) \\ = & \arg \max _ { \theta } \sum _ { ( w , c ) \in D } \log \sigma \left( v _ { c } \cdot v _ { w } \right) + \sum _ { ( w , c ) \in D ^ { \prime } } \log \sigma \left( - v _ { c } \cdot v _ { w } \right) \end{aligned}]]></content>
      <categories>
        <category>ML &amp; DL</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Embedding</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零单排PRML 第6章 Kernel Methods]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F12%2F10%2FPRML_6_kernel%20methods%2F</url>
    <content type="text"><![CDATA[引言机器学习里面对待训练数据有2种,抛弃或者保留 有的是训练完得到参数后就可以抛弃了，比如神经网络； 有的是还需要原来的训练数据比如KNN，SVM也需要保留一部分数据—支持向量。 许多线性参数模型可以转化为对偶形式,转化后依然是线性模型. 只是线性组合的对象变成了一组非线性基函数. 这里的非线性基函数就是在基于各个数据点计算的核函数. 对偶表示最小均方误差形式 J ( \boldsymbol { w } ) = \frac { 1 } { 2 } \sum _ { n = 1 } ^ { N } \left\{ \boldsymbol { w } ^ { T } \boldsymbol { \phi } \left( \boldsymbol { x } _ { n } \right) - t _ { n } \right\} ^ { 2 } + \frac { \lambda } { 2 } \boldsymbol { w } ^ { T } \boldsymbol { w }对偶形式:核函数形式 J ( \boldsymbol { a } ) = \frac { 1 } { 2 } \boldsymbol { a } ^ { T } \boldsymbol { K } \boldsymbol { K } \boldsymbol { a } - \boldsymbol { a } ^ { T } \boldsymbol { K } \mathbf { t } + \frac { 1 } { 2 } \mathbf { t } ^ { T } \mathbf { t } + \frac { \lambda } { 2 } \boldsymbol { a } ^ { T } \boldsymbol { K } \boldsymbol { a }这里K是著名的Gram矩阵$\boldsymbol { K } = \boldsymbol { \Phi } \boldsymbol { \Phi } ^ { T }$.上式中的$\boldsymbol { a } $又有闭式解 \boldsymbol { a } = \left( \boldsymbol { K } + \lambda \boldsymbol { I } _ { N } \right) ^ { - 1 } \mathbf { t }这样,问题的解$\boldsymbol { a } $就只与核函数$\boldsymbol { K } $有关了. 最小均方转为核函数的组合$k \left( x , x ^ { \prime } \right)$后,我们就只关心$x , x ^ { \prime } $基于核函数的结果,对偶表示可以避免了显式的使用特征向量$\phi(x)$.这样做不关心中间过程$\boldsymbol { \phi} ( x ) ^ { \mathrm { T } } \boldsymbol { \phi } \left( x ^ { \prime } \right) $ ,只关心结果$k \left( x , x ^ { \prime } \right)$. 核函数形式$k \left( x , x ^ { \prime } \right) =\boldsymbol { \phi} ( x ) ^ { \mathrm { T } } \boldsymbol { \phi } \left( x ^ { \prime } \right) $ ,也就是映射后高维特征空间的内积可以通过原来低维的特征得到, 这样做的好处是可以使用隐式的高维特征,所以kernel method得到了广泛应用. 以SVM为例,低维不可分的数据,经过kernel将特征映射到高维后,就可以分开了. 以下式为例, 这里 $k ( \boldsymbol { x } , \boldsymbol { z } ) $ 的形式很简洁,但是如果想找到$\phi(x)$是比较难的.这里的$\phi(x)$只是中间需求,最终我们需要只是$k ( \boldsymbol { x } , \boldsymbol { z } ) $. k ( \boldsymbol { x } , \boldsymbol { z } ) = \left( \boldsymbol { x } ^ { T } \boldsymbol { z } \right) ^ { 2 },形式简洁,好算 \\ \phi ( \mathbf { x } ) = \left( x _ { 1 } ^ { 2 } , \sqrt { 2 } x _ { 1 } x _ { 2 } , x _ { 2 } ^ { 2 } \right) ^ { \mathrm { T } },很难找到,计算量大构建kernel的方法特征映射法选择一个特征映射函数$\boldsymbol { \phi} ( x ) $ ,并通过这个映射来寻找kernel. k \left( x , x ^ { \prime } \right) =\boldsymbol { \phi} ( x ) ^ { \mathrm { T } } \boldsymbol { \phi } \left( x ^ { \prime } \right) = \sum _ { i = 1 } ^ { M } \phi _ { i } ( x ) \phi _ { i } \left( x ^ { \prime } \right)直接构造直接写出kernel的形式, k ( \mathbf { x } , \mathbf { z } ) = \left( \mathbf { x } ^ { \mathrm { T } } \mathbf { z } \right) ^ { 2 }这里kernel的形式是写出来了,但是这个形式合法吗? 合法性检验有2个方法: 找到对应的特征映射函数 Gram矩阵在所有可能的{xn}的选择下都是半正定的. 下面是通过方法1来对kernel进行合法性检查.上式可以认为是二维输入$\mathbf { x } = \left( x { 1 } , x { 2 } \right)$, 经过特征映射法的结果,原因是可以将上式展开了为2个经过特征映射后的$\boldsymbol { \phi} ( x ) $的内积. \begin{aligned} k ( \mathbf { x } , \mathbf { z } ) & = \left( \mathbf { x } ^ { \mathrm { T } } \mathbf { z } \right) ^ { 2 } = \left( x _ { 1 } z _ { 1 } + x _ { 2 } z _ { 2 } \right) ^ { 2 } \\ & = x _ { 1 } ^ { 2 } z _ { 1 } ^ { 2 } + 2 x _ { 1 } z _ { 1 } x _ { 2 } z _ { 2 } + x _ { 2 } ^ { 2 } z _ { 2 } ^ { 2 } \\ & = \left( x _ { 1 } ^ { 2 } , \sqrt { 2 } x _ { 1 } x _ { 2 } , x _ { 2 } ^ { 2 } \right) \left( z _ { 1 } ^ { 2 } , \sqrt { 2 } z _ { 1 } z _ { 2 } , z _ { 2 } ^ { 2 } \right) ^ { \mathrm { T } } \\ & = \phi ( \mathbf { x } ) ^ { \mathrm { T } } \phi ( \mathbf { z } ) \end{aligned}这里的特征映射函数形式为$\phi ( \mathbf { x } ) = \left( x { 1 } ^ { 2 } , \sqrt { 2 } x { 1 } x { 2 } , x { 2 } ^ { 2 } \right) ^ { \mathrm { T } }$ ,可以验证$k ( \mathbf { x } , \mathbf { z } ) = \left( \mathbf { x } ^ { \mathrm { T } } \mathbf { z } \right) ^ { 2 }$ 是一个合法的kernel. 基于现有的合法kernel进行构建假设 $k { 1 } \left( \mathbf { x } , \mathbf { x } ^ { \prime } \right) $和$ k { 2 } \left( \mathbf { x } , \mathbf { x } ^ { \prime } \right)$都是合法kernel,那么通过以下形式组合构建的新的kernel也是合法的. k \left( \mathbf { x } , \mathbf { x } ^ { \prime } \right) = c k _ { 1 } \left( \mathbf { x } , \mathbf { x } ^ { \prime } \right), c>0 \\ k \left( \mathbf { x } , \mathbf { x } ^ { \prime } \right) = k _ { 1 } \left( \mathbf { x } , \mathbf { x } ^ { \prime } \right) + k _ { 2 } \left( \mathbf { x } , \mathbf { x } ^ { \prime } \right) \\ k \left( \mathbf { x } , \mathbf { x } ^ { \prime } \right) = \exp \left( k _ { 1 } \left( \mathbf { x } , \mathbf { x } ^ { \prime } \right) \right)\\ k \left( \mathbf { x } , \mathbf { x } ^ { \prime } \right) = f ( \mathbf { x } ) k _ { 1 } \left( \mathbf { x } , \mathbf { x } ^ { \prime } \right) f \left( \mathbf { x } ^ { \prime } \right), f \ is \ any \ function \\ ...这里以经典的 高斯核 为例, 通过其构造过程来验证其合法性. 首先高斯核函数是一种径向基核 k \left( \mathbf { x } , \mathbf { x } ^ { \prime } \right) = \exp \left( - \left\| \mathbf { x } - \mathbf { x } ^ { \prime } \right\| ^ { 2 } / 2 \sigma ^ { 2 } \right)把exp中的平方项展开得到 \left\| \mathbf { x } - \mathbf { x } ^ { \prime } \right\| ^ { 2 } = \mathbf { x } ^ { \mathrm { T } } \mathbf { x } + \left( \mathbf { x } ^ { \prime } \right) ^ { \mathrm { T } } \mathbf { x } ^ { \prime } - 2 \mathbf { x } ^ { \mathrm { T } } \mathbf { x } ^ { \prime } \\ k \left( \mathbf { x } , \mathbf { x } ^ { \prime } \right) = \exp \left( - \mathbf { x } ^ { \mathrm { T } } \mathbf { x } / 2 \sigma ^ { 2 } \right) \exp \left( \mathbf { x } ^ { \mathrm { T } } \mathbf { x } ^ { \prime } / \sigma ^ { 2 } \right) \exp \left( - \left( \mathbf { x } ^ { \prime } \right) ^ { \mathrm { T } } \mathbf { x } ^ { \prime } / 2 \sigma ^ { 2 } \right)这里线性kernel $k \left( \mathbf { x } , \mathbf { x } ^ { \prime } \right) = \mathbf { x } ^ { \mathrm { T } } \mathbf { x } ^ { \prime }$ 是合法的,然后基于上述的构造性质,可以看出高斯核是合法的.同时,由于$exp$的泰勒展开有无穷多项,所以高斯核将特征向量映射到无穷多维. 通过概率生成模型来构建给定一个生成模型$p(x)$ , 我们可以定义kernel k \left( \mathbf { x } , \mathbf { x } ^ { \prime } \right) = p ( \mathbf { x } ) p \left( \mathbf { x } ^ { \prime } \right)这里的$p(x)$可以认f为是$\phi(x)​$. 径向基函数网络在径向基函数中， 每⼀个基函数只依赖于样本和中⼼$\mu_j$之间的径向距离（通常是欧⼏⾥得距离）， 即 这里的网络体现在哪里? 高斯过程什么是高斯过程?首先高斯分布定义为函数$y(\mathbf{x})$上的一个高斯分布,并且这个函数在一系列点 $\mathbf { x } { 1 } , \dots , \mathbf { x } { N }$ 的响应值 $y \left( \mathbf { x } { 1 } \right) , \ldots , y \left( \mathbf { x } { N } \right)$ 联合概率分布也是高斯的. 高斯过程=高斯+过程 高斯: 对于$y(\mathbf{x})$,为什么它是高斯的呢?我们令 y ( \mathbf { x } ) = \mathbf { w } ^ { \mathrm { T } } \boldsymbol { \phi } ( \mathbf { x } )\\ p ( \mathbf { w } ) = \mathcal { N } ( \mathbf { w } | \mathbf { 0 } , \alpha ^ { - 1 } \mathbf { I } ) 这里的$y$是一些$\mathbf{w}$的混合,而$\mathbf{w}$ 我们给了一个高斯分布的先验.然后利用性质:高斯分布的线性组合依然是高斯分布.所以$y(\mathbf{x})$也是高斯分布. 这里我们只要确定$y(\mathbf{x})$的均值和方差就可以确定这个高斯分布. 过程:时间或者空间上一系列点$\mathbf { x } { 1 } , \dots , \mathbf { x } { N }$ 的响应值$y \left( \mathbf { x } { 1 } \right) , \ldots , y \left( \mathbf { x } { N } \right)$ . 考虑一系列的点,我们可以得到 \mathbf { y } = \mathbf { \Phi } _ { \mathbf { W } }下面就是这个多维高斯分布的均值和协方差. \begin{aligned} \mathbb { E } [ \mathbf { y } ] & = \mathbf { \Phi } \mathbb { E } [ \mathbf { w } ] = \mathbf { 0 } \\ \operatorname { cov } [ \mathbf { y } ] & = \mathbb { E } \left[ \mathbf { y y } ^ { \mathrm { T } } \right] = \mathbf { \Phi } \mathbb { E } \left[ \mathbf { w } \mathbf { w } ^ { \mathrm { T } } \right] \mathbf { \Phi } ^ { \mathrm { T } } = \frac { 1 } { \alpha } \mathbf { \Phi } \mathbf { \Phi } ^ { \mathrm { T } } = \mathbf { K } \end{aligned}这里我们进一步做了简化,将$y(\mathbf{x})$的均值设为0,那么一个高斯过程就仅仅受协方差矩阵的控制.更重要的是,这个协方差矩阵我们用一个kernel来确定,kernel形式为特征映射法的形式. 这也是为什么高斯过程放在kernel method这一章! 综上,我们得到确定一个高斯过程的步骤: 确定核函数的形式 求解核形式的协方差矩阵 确定高斯过程 有了前几章的基础,书中这里举了2个核函数例子(高斯核,指数核).给定了$\mathbf { y } $的分布,我们可以进行采样,得到不同的$\mathbf { y } $. 下图就是基于不同的核采样得到的$\mathbf { y } $. 到这里什么是高斯过程及其如何确定一个高斯过程就清楚了.接下来是一些高斯过程应用(回归,分类). 基于高斯过程的回归考虑观测值$yn$上的一个高斯噪声$\epsilon { n }$, t _ { n } = y _ { n } + \epsilon _ { n }\\ y_n=y(\mathbf{x}_n)这样$\mathbf { t }$可以认为是从以$\mathbf { y}$为均值的高斯分布中采样得到.似然函数如下 p ( \mathbf { t } | \mathbf { y } ) = \mathcal { N } ( \mathbf { t } | \mathbf { y } , \beta ^ { - 1 } \mathbf { I } _ { N } )基于高斯过程的定义, 下式为高斯过程先验 p ( \mathbf { y } ) = \mathcal { N } ( \mathbf { y } | \mathbf { 0 } , \mathbf { K } )可以看出:核函数$K$控制先验. 为了得到$\mathbf { t }$的分布,我们需要对其联合概率分布求积分,即边缘化 p ( \mathbf { t } ) = \int p ( \mathbf { t } | \mathbf { y } ) p ( \mathbf { y } ) \mathrm { d } \mathbf { y } = \mathcal { N } ( \mathbf { t } | \mathbf { 0 } , \mathbf { C } )这里的$p ( \mathbf { t } | \mathbf { y } ) 和 p ( \mathbf { y } ) $都是高斯分布,其中 C \left( \mathbf { x } _ { n } , \mathbf { x } _ { m } \right) = k \left( \mathbf { x } _ { n } , \mathbf { x } _ { m } \right) + \beta ^ { - 1 } \delta _ { n m }\\ \delta _ { n m }=1, if \ m=n这里意思是只有协方差矩阵的对角线叠加了噪声.这是因为每次的噪声都是相互独立,所以非对角线的位置相关性都为0.这里对核函数$K$ 的唯一限制就是协方差矩阵是正定的.假设$\lambdai$ 是核函数$K$ 的特征值,那么协方差矩阵$\mathbf { C }$的特征值就是$\lambda_i+\beta^{-1}$.核函数需要满足半正定,即$\lambda { i } \geq 0$,加上一个正数$\beta$就可以$\lambda_i+\beta^{-1}&gt;0$, 所以$\mathbf { C }$是正定. 回归问题可以认为是基于之前的输入$\mathbf { x } { 1 } , \dots , \mathbf { x } { N }$和输出$\mathbf { t } { N } = \left( t { 1 } , \ldots , t { N } \right) ^ { \mathrm { T } }$ ,对新的输入$\mathbf{x}{N+1}$ 进行预测(求$t_{N+1}$).写成条件概率的形式就是 p \left( t _ { N + 1 } | \mathbf { t } _ { N } , \mathbf { x } _ { 1 } , \dots , \mathbf { x } _ { N },\mathbf{x}_{N+1} \right), (省略输入)\\ =p \left( t _ { N + 1 } | \mathbf { t } _ { N }\right)前面已经得到了$p ( \mathbf { t } )=\mathcal { N } ( \mathbf { t } | \mathbf { 0 } , \mathbf { C } )$ (高斯分布),那么 p \left( \mathbf { t } _ { N + 1 } \right) = \mathcal { N } \left( \mathbf { t } _ { N + 1 } | \mathbf { 0 } , \mathbf { C } _ { N + 1 } \right)这里的$\mathbf { C } _ { N + 1 } $和前面的$\mathbf { C }_N$ 的关系为,为什么要写成这种形式呢? \mathbf { C } _ { N + 1 } = \left( \begin{array} { c c } { \mathbf { C } _ { N } } & { \mathbf { k } } \\ { \mathbf { k } ^ { \mathrm { T } } } & { c } \end{array} \right)写成上述形式,可以利用公式 \begin{aligned} \boldsymbol { \mu } _ { a | b } & = \boldsymbol { \mu } _ { a } + \boldsymbol { \Sigma } _ { a b } \boldsymbol { \Sigma } _ { b b } ^ { - 1 } \left( \mathbf { x } _ { b } - \boldsymbol { \mu } _ { b } \right) \\ \boldsymbol { \Sigma } _ { a | b } & = \boldsymbol { \Sigma } _ { a a } - \boldsymbol { \Sigma } _ { a b } \boldsymbol { \Sigma } _ { b b } ^ { - 1 } \boldsymbol { \Sigma } _ { b a } \end{aligned}这里的$p \left( \mathbf { x } { a } | \mathbf { x } { b } \right)=p \left( t { N + 1 } | \mathbf { t } { N }\right)$ , b对应数据$\mathbf { t } { N } = \left( t { 1 } , \ldots , t { N } \right) ^ { \mathrm { T } }$ , a对应$t{N+1}$.代入得到 p \left( t _ { N + 1 } | \mathbf { t } \right) = \mathcal { N } \left( \mathbf { t } _ { N + 1 } | m \left( \mathbf { x } _ { N + 1 } \right) , \sigma ^ { 2 } \left( \mathbf { x } _ { N + 1 } \right) \right) \\ \begin{aligned} m \left( \mathbf { x } _ { N + 1 } \right) & = \mathbf { k } ^ { \mathrm { T } } \mathbf { C } _ { N } ^ { - 1 } \mathbf { t } \\ \sigma ^ { 2 } \left( \mathbf { x } _ { N + 1 } \right) & = c - \mathbf { k } ^ { \mathrm { T } } \mathbf { C } _ { N } ^ { - 1 } \mathbf { k } \end{aligned}这里有了均值和方差就定了:具有任意核函数$k \left( \mathbf { x } { n } , \mathbf { x } { m } \right)$ 的高斯过程回归的预测分布. 这里高斯过程回归的预测是一堆基函数的表示,也就是在函数空间 对应之前的线性回归的预测是一堆参数的表示,也就是在参数空间 学习超参数第一次听到GP是用于神经网络超参数的搜索,这里终于看到了真身. GP依赖于协方差函数的选择 ,而实际中用一组带有参数的函数来代替协方差函数,这组函数可以描述协方差矩阵的参数.举例如下: k \left( \mathbf { x } _ { n } , \mathbf { x } _ { m } \right) = \theta _ { 0 } \exp \left\{ - \frac { \theta _ { 1 } } { 2 } \left\| \mathbf { x } _ { n } - \mathbf { x } _ { m } \right\| ^ { 2 } \right\} + \theta _ { 2 } + \theta _ { 3 } \mathbf { x } _ { n } ^ { \mathrm { T } } \mathbf { x } _ { m }这里的参数$\theta_0,\theta_1…$控制了协方差矩阵.不同参数的效果如下 这里利用最大似然来估计参数. \ln p ( \mathbf { t } | \boldsymbol { \theta } ) = - \frac { 1 } { 2 } \ln \left| \mathbf { C } _ { N } \right| - \frac { 1 } { 2 } \mathbf { t } ^ { \mathrm { T } } \mathbf { C } _ { N } ^ { - 1 } \mathbf { t } - \frac { N } { 2 } \ln ( 2 \pi )求导,并利用矩阵求导公式 \begin{aligned} \frac { \partial } { \partial \mathbf { x } } \left( A ^ { - 1 } \right) & = - A ^ { - 1 } \frac { \partial A } { \partial \mathbf { x } } A ^ { - 1 } \\ \frac { \partial } { \partial \mathbf { x } } \ln | A | & = \operatorname { Tr } \left( A ^ { - 1 } \frac { \partial A } { \partial \mathbf { x } } \right) \end{aligned}可以得到 \frac { \partial } { \partial \theta _ { i } } \ln p ( \mathbf { t } | \boldsymbol { \theta } ) = - \frac { 1 } { 2 } \operatorname { Tr } \left( \mathbf { C } _ { N } ^ { - 1 } \frac { \partial \mathbf { C } _ { N } } { \partial \theta _ { i } } \right) + \frac { 1 } { 2 } \mathbf { t } ^ { \mathrm { T } } \mathbf { C } _ { N } ^ { - 1 } \frac { \partial \mathbf { C } _ { N } } { \partial \theta _ { i } } \mathbf { C } _ { N } ^ { - 1 } \mathbf { t }自动相关性确定⾼斯过程中的⾃动相关性确定: 通过最⼤似然⽅法进⾏的参数最优化，能 够将不同输⼊的相对重要性从数据中推断出来。 假定一个二维高斯过程$\mathbf { x } = \left( x { 1 } , x { 2 } \right)$,其核函数如下 k \left( \mathbf { x } , \mathbf { x } ^ { \prime } \right) = \theta _ { 0 } \exp \left\{ - \frac { 1 } { 2 } \sum _ { i = 1 } ^ { 2 } \eta _ { i } \left( x _ { i } - x _ { i } ^ { \prime } \right) ^ { 2 } \right\}不同的参数$\eta { i }$的影响如下: 当$\eta { i }$变小的时候,核函数变的对相应的输入$x_i$ 不敏感了. 我们首先看看,根据参数$\etai$来输入$x_i$的影响.下右图: 参数$\eta_2$变小了,那么在其相应的输入$x_2$下,输出变得较为稳定了.直观理解就是上式中$\eta { i } \left( x { i } - x { i } ^ { \prime } \right) ^ { 2 } $变小了,影响自然也下降. 另一个角度,根据输入$x_i$来看看参数$\eta_i$. 高斯过程分类GP分类的预测在整个实数轴上,如果分类需要进行放缩（sigmoid来放缩). 这里通过$\sigma$函数将高斯过程$a ( \mathbf { x } )$ 变成了非高斯的随机过程$y$. 与前面的回归类似,分类问题可以认为是基于之前的输入$\mathbf { x } { 1 } , \dots , \mathbf { x } { N }$和输出$\mathbf { t } { N } = \left( t { 1 } , \ldots , t { N } \right) ^ { \mathrm { T } }$ ,对新的输入$\mathbf{x}{N+1}$ 进行预测(求$t_{N+1}$).写成条件概率的形式就是 p \left( t _ { N + 1 } | \mathbf { t } _ { N }\right)只不过这里的高斯过程换成了$a$,而预测的$t_{N+1}$ 是非高斯过程. p \left( \mathbf { a } _ { N + 1 } \right) = \mathcal { N } \left( \mathbf { a } _ { N + 1 } | \mathbf { 0 } , \mathbf { C } _ { N + 1 } \right)同时,在分类时,我们假定所有label都是正确的,所以协方差矩阵没有像分类一样叠加了一个噪声项$\beta ^ { - 1 } \delta _ { n m }$. 这里的$\nu$看起来像噪声,实际是为了保证协方差正定引入的. 对于2分类,将$t _ { N + 1 }$预测为1的概率 \begin{aligned} p \left( t _ { N + 1 } = 1 | \mathbf { t } _ { N } \right) & = \int p \left( t _ { N + 1 } = 1 | a _ { N + 1 } \right) p \left( a _ { N + 1 } | \mathbf { t } _ { N } \right) \mathrm { d } a _ { N + 1 } \\ p \left( t _ { N + 1 } = 1 | a _ { N + 1 } \right) & = \sigma \left( a _ { N + 1 } \right) \end{aligned}那么问题来了:上面这个积分怎么解决? 目前有3种方法: variational inference expectation propagation Laplace approximation (下一节重点) 拉普拉斯近似关于拉普拉斯近似的详细介绍见 PRML 4.4. 简而言之,laplace approximation就是使用Gaussian去近似一个连续变量的概率密度函数,这个Gaussian是以后验分布的概率的众数为中心的。 注: 本节的$\mathbf { a } { N }$就是前面的 $\mathbf { t } { N }$. 上节说到, $p \left( t { N + 1 } = 1 | \mathbf { t } { N } \right)$ 不好求解. \begin{aligned} p \left( a _ { N + 1 } | \mathbf { t } _ { N } \right) & = \int p \left( a _ { N + 1 } , \mathbf { a } _ { N } | \mathbf { t } _ { N } \right) \mathrm { d } \mathbf { a } _ { N } \\ & = \frac { 1 } { p \left( \mathbf { t } _ { N } \right) } \int p \left( a _ { N + 1 } , \mathbf { a } _ { N } \right) p \left( \mathbf { t } _ { N } | a _ { N + 1 } , \mathbf { a } _ { N } \right) \mathrm { d } \mathbf { a } _ { N } \\ & = \frac { 1 } { p \left( \mathbf { t } _ { N } \right) } \int p \left( a _ { N + 1 } | \mathbf { a } _ { N } \right) p \left( \mathbf { a } _ { N } \right) p \left( \mathbf { t } _ { N } | \mathbf { a } _ { N } \right) \mathrm { d } \mathbf { a } _ { N } \\ & = \int p \left( a _ { N + 1 } | \mathbf { a } _ { N } \right) p \left( \mathbf { a } _ { N } | \mathbf { t } _ { N } \right) \mathrm { d } \mathbf { a } _ { N } \end{aligned}积分中 第一项是高斯过程,$p \left( a { N + 1 } | \mathbf { a } { N } \right) = \mathcal { N } \left( a { N + 1 } | \mathbf { k } ^ { \mathrm { T } } \mathbf { C } { N } ^ { - 1 } \mathbf { a } { N } , c - \mathbf { k } ^ { \mathrm { T } } \mathbf { C } { N } ^ { - 1 } \mathbf { k } \right)$ 第二项是后验分布,如果我们能通过拉普拉斯近似找到一个高斯分布 那么,$p \left( t { N + 1 } = 1 | \mathbf { t } { N } \right)$就变成对2个高斯分布的积分,可以求解. 现在核心问题变成了第二项:后验怎么求? 后验=先验*似然(省略归一化) 先验:前面已经得到, \ln p ( \mathbf { t } | \boldsymbol { \theta } ) = - \frac { 1 } { 2 } \ln \left| \mathbf { C } _ { N } \right| - \frac { 1 } { 2 } \mathbf { t } ^ { \mathrm { T } } \mathbf { C } _ { N } ^ { - 1 } \mathbf { t } - \frac { N } { 2 } \ln ( 2 \pi )​ 把上式中的$t$换成$a$ ,就得到 $p(\boldsymbol { a } _ { N })$ 似然: 由现有的数据及其预测得到, p \left( \mathbf { t } _ { N } | \mathbf { a } _ { N } \right) = \prod _ { n = 1 } ^ { N } \sigma \left( a _ { n } \right) ^ { t _ { n } } \left( 1 - \sigma \left( a _ { n } \right) \right) ^ { 1 - t _ { n } } = \prod _ { n = 1 } ^ { N } e ^ { a _ { n } t _ { n } } \sigma \left( - a _ { n } \right) \\ \sigma \left( a _ { n } \right) ^ { t _ { n } } \left( 1 - \sigma \left( a _ { n } \right) \right) ^ { 1 - t _ { n } }=(\frac{e^{a_n}}{1+e^{a_n}})^{t_n}(\frac{1}{e^{a_n}+1})^{1-t_n}=e ^ { a _ { n } t _ { n } }\frac{1}{1+e^{a_n}} 结合先验和似然,得到后验的表达形式 \begin{aligned} \Psi \left( \boldsymbol { a } _ { N } \right) = & \ln p \left( \boldsymbol { a } _ { N } \right) + \ln p \left( \mathbf { t } _ { N } | \boldsymbol { a } _ { N } \right) \\ = & - \frac { 1 } { 2 } \boldsymbol { a } _ { N } ^ { T } \boldsymbol { C } _ { N } ^ { - 1 } \boldsymbol { a } _ { N } - \frac { N } { 2 } \ln ( 2 \pi ) - \frac { 1 } { 2 } \ln \left| \boldsymbol { C } _ { N } \right| + \mathbf { t } _ { N } ^ { T } \boldsymbol { a } _ { N } \\ & - \sum _ { n = 1 } ^ { N } \ln \left( 1 + e ^ { a _ { n } } \right) \end{aligned}目前我们已经有了后验的表达形式,如何来拉普拉斯近似来解呢? 找到后验分布的众数 后验求导,并代入 [\ln(1+e^x)]'=\frac{e^x}{1+e^x}=\frac{1}{1+e^{-x}}=\sigma(x)​ 得到 \nabla \Psi \left( \boldsymbol { a } _ { N } \right) = \mathbf { t } _ { N } - \boldsymbol { \sigma } _ { N } - \boldsymbol { C } _ { N } ^ { - 1 } \boldsymbol { a } _ { N }​ 其中,$\boldsymbol { \sigma } { N }$是元素为$\sigma(a_n)$的向量. 这里由于$\boldsymbol { \sigma } { N }$和$\boldsymbol { a } { N }$的非线性关系, 所以令一阶导为零没有闭式解.利用二阶导和IRLS算法我们可以找到众数 $\mathbf { a } { N } ^ { \star }$. 计算以众数 $\mathbf { a } { N } ^ { \star }$为中心的高斯分布$q \left( \mathbf { a } { N } \right) $ . q \left( \mathbf { a } _ { N } \right) = \mathcal { N } \left( \mathbf { a } _ { N } | \mathbf { a } _ { N } ^ { \star } , \mathbf { H } ^ { - 1 } \right) 到这里, $p \left( a { N + 1 } | \mathbf { t } { N } \right)$已经可以解了,分别将2个高斯分布代入得到,最终p \left( a _ { N + 1 } | \mathbf { t } _ { N } \right) 是一个高斯分布,均值方差如下 \begin{aligned} \mathbb { E } \left[ a _ { N + 1 } | \mathbf { t } _ { N } \right] & = \mathbf { k } ^ { \mathrm { T } } \left( \mathbf { t } _ { N } - \boldsymbol { \sigma } _ { N } \right) \\ \operatorname { var } \left[ a _ { N + 1 } | \mathbf { t } _ { N } \right] & = c - \mathbf { k } ^ { \mathrm { T } } \left( \mathbf { W } _ { N } ^ { - 1 } + \mathbf { C } _ { N } \right) ^ { - 1 } \mathbf { k } \end{aligned}接下来,我们需要确定 协方差函数的参数$\theta$. 最大似然即可 参考 https://www.zhihu.com/question/46631426 PRML http://blog.videolectures.net/100-most-popular-machine-learning-talks-at-videolectures-net/ MLAPP http://www.52nlp.cn/prml读书会第六章-kernel-methods]]></content>
      <categories>
        <category>PRML</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>PRML</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[18KDD_HEER_Easing Embedding Learning by Comprehensive Transcription of Heterogeneous Information Networks]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F12%2F04%2F18KDD_HEER%2F</url>
    <content type="text"><![CDATA[本文来自Jiawei Han大佬,针对HIN中潜在的不兼容语义设计了一种无监督的HINE算法HEER. 这篇和他们之前发的18SDM ASPEM Embedding Learning by Aspects in Heterogeneous Information Networks有异曲同工之妙. 区别就是:本文是多方面协同学习,而18SDM是独立处理各个方面. 整个模型用一句话描述就是: edge representations that are further coupled with properly-learned heterogeneous metrics. Motivation本文首先用一张图来描述其motivation 简单来说:HIN中不同的语义可能关联度很低甚至冲突,必须要分开处理. 例如这里的musical -Stan-Ang Lee,他们3者之间有连接,如果同样对待的话,他们3个都会很像.但是实际musical和Ang Lee没啥关系,这里是异质边带来的问题了,所以本文针对不同边来处理. 现有的HINE方法 指定元路径,或者用一些监督信号来选择元路径. 只适用于指定类型HINE的方法或者有一些补充信息. 只embed到metric space里面. 个人感觉HINE就是要将不同节点或者边embed到不同空间来体现异质性. 本文通过一些统计指标来验证HIN中的不兼容度,这样本文的motivation就很强.如图所示, paper-author和paper-year相似度很低,不兼容性很大.直观理解就是,每一年发表的paper多种多样,year并不能作为识别paper的有效特征. 而paper-author和paper-term相似度很高,即paper,author, term这三者有紧密关系. 这里稍微解释下: $l(u,v;r)=\boldsymbol { P } { u , : } ^ { r } , \left( \boldsymbol { P } { v } ^ { r } , : \right) ^ { \top }$ 定义了两个节点u和v通过r连接到的节点的相似性. 相似性越高, u和v在关系r下越像. $J \left( u ; r { 1 } , r { 2 } \right) : = \frac { \min { \varphi ( \nu ) = t } \left{ l \left( u , v ; r { 1 } \right) , l \left( u , v ; r { 2 } \right) \right} } { \max { \varphi ( v ) = t } \left{ l \left( u , v ; r { 1 } \right) , l \left( u , v ; r { 2 } \right) \right} }$ 定义了节点u在两种关系(r1,r2)下与同类型节点的相似性的接近程度. 两个不同关系下的相似性衡量越接近(都大或者都小),那这两种关系越兼容. 算出一系列的$J$之后,通过查看$J$值的分布(CDF图)来发现其规律. Model一个好的embedding算法应该可以编码HIN中的语义,并能够重构这个HIN.考虑在HIN中边是类型的,这就对embedding算法提出了更高的要求:同时预测边的存在和边的类型.这里定义了 typed closeness ,同时考虑两端的节点对$u,v$和他们之间的边$r$ s_r(u,v)=\frac { \exp \left( \boldsymbol { \mu } _ { r } ^ { \top } \mathbf { g } _ { u v } \right) } { \sum _ { \tilde { \boldsymbol { v } } \in \mathcal { P } _ { u * } ^ { r } } \exp \left( \boldsymbol { \mu } _ { r } ^ { \top } \mathbf { g } _ { u \tilde { v } } \right) + \sum _ { \tilde { u } \in \mathcal { P } _ { * v } ^ { r } } \exp \left( \boldsymbol { \mu } _ { r } ^ { \top } \mathbf { g } _ { \tilde { u } v } \right) } ,(u,v)\in \mathcal { P } ^ { r }本文实际没有学习边的embedding而是对节点的embedding进行简单操作得到边的embedding. 本文可以同时处理有向边和无向边. \mathbf { g } _ { u v } : = \left\{ \begin{array} { l } { 2 \cdot \mathbf { f } _ { u } ^ { O } \circ \mathbf { f } _ { v } ^ { I } , u \rightarrow v} \\ { \mathbf { f } _ { u } ^ { O } \circ \mathbf { f } _ { v } ^ { O } + \mathbf { f } _ { u } ^ { I } \circ \mathbf { f } _ { v } ^ { I } } ,undirected\end{array} \right.优化目标和line很像,都是优化KL距离 O ^ { r } = K L \left( W _ { u v } ^ { ( r ) } , s _ { t } ( u , v ) \right) \\ \min _ { \left\{ \mathbf { f } _ { u } \right\} _ { u \in \mathcal { V } } , \left\{ \boldsymbol { \mu } _ { r } \right\} _ { r \in \mathcal { R } } } \mathcal { O }这里$s_r(u,v)$分母比较难计算,本文利用负采样来优化.这里对负采样进行了约束,即:负采样的样本要和原始样本保持同类型. \log \sigma \left( \boldsymbol { \mu } _ { r } ^ { \top } \mathbf { g } _ { u v } \right) + \sum _ { i = 1 } ^ { K } \mathbb { E } _ { \tilde { v } _ { i } } \log \sigma \left( - \boldsymbol { \mu } _ { r } ^ { \top } \mathbf { g } _ { u \tilde { v } _ { i } } \right) + \sum _ { i = 1 } ^ { K } \mathbb { E } _ { \tilde { u } _ { i } } \log \sigma \left( - \boldsymbol { \mu } _ { r } ^ { \top } \mathbf { g } \tilde { u } _ { i } v \right)模型整体流程图如下 注意最后面是分类型来计算loss Experiment数据集是常见的HIN,注意每个数据集都是有向边+无向边. 这里只做了边重构实验:敲除部分边,训练模型,基于学习到的embedding预测边. 效果很好,因为本文设计的优化目标就是针对边重构的. 一些常规的HINE中的聚类分类实验都没有做. Summary整体来说 本文的模型并不是很复杂,但是做了一个很好的问题:the comprehensive transcription of HINs in embedding learning. 作者发现了HIN中的语义不兼容问题, 并设计了实验来验证不兼容性.这使得本文的Motivation极强.]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Network Embedding</tag>
        <tag>HIN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见散度与距离(KL散度，JS散度，Wasserstein距离，互信息MI)]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F11%2F15%2F%E5%B8%B8%E8%A7%81%E6%95%A3%E5%BA%A6%E8%B7%9D%E7%A6%BB%2F</url>
    <content type="text"><![CDATA[本文对比了常见散度(KL散度，JS散度)和常见距离(尤其是现在很火的Wasserstein距离) F-散度散度是用来衡量两个概率密度p，q区别的函数，即：两个分布的相似程度. D _ { f } ( p \| q ) = \int q ( x ) f \left( \frac { p ( x ) } { q ( x ) } \right) d x这里的$f$需要满足2个条件: $f$是凸函数且$f(1)=0$. 可以证明: 因为$f$是凸函数,由Jensen不等式可知 $E(f(x)) &gt;= f(E(x))$,所以 D _ { f } ( p \| q ) = \int q ( x ) f \left( \frac { p ( x ) } { q ( x ) } \right) d x \geq f \left( \int q ( x ) \frac { p ( x ) } { q ( x ) } d x \right) = f ( 1 ) = 0KL散度KL散度是F-散度的一个特例,当$f(x)=xlogx$ 的时候: D _ { KL} ( p \| q ) = \int p(x) \log \left( \frac { p ( x ) } { q ( x ) } \right) d x需要注意 KL散度非对称,即 $D { KL} ( p | q ) \neq D { KL} ( q | p )$ ,所以KL散度不是一个真正的距离或者度量. KL散度不满足三角不等式 $D { KL} ( p | q ) &gt; D { KL} ( p | r ) + D _ { KL} ( r | q ) $. KL散度取值范围从$[0, \infty]$ , p=q的时候,两个分布完全一样取到0 p和q的影响: 如果想要$D_{KL}$小, 那么p大的地方,q一定要大 那么p小的地方,q对$D_{KL}$的影响不大 如果想要$D_{KL}$大 那么p大的地方,q对$D_{KL}$影响不大 那么p小的地方,q一定要小 KL散度 不对称导致的问题: 情况1.当 $p\rightarrow 0$ 且 $q\rightarrow 1$时, $ p(x) \log \left( \frac { p ( x ) } { q ( x ) }\ \right) \rightarrow 0$ , 对$D_{KL}(p||q)$的贡献为0 情况2.当$ p\rightarrow 1 $且 $q\rightarrow 0$时, $ p(x) \log \left( \frac { p ( x ) } { q ( x ) }\ \right) \rightarrow +\infty $, 对$D_{KL}(p||q)$的贡献为$+\infty $. 进步一这里可以解释为什么我们通常选择正态分布而不是均匀分布. 均匀分布:只要两个均分分布不是完全相同,必然出现$p\neq0,q=0$的情况,导致$D_{KL}(p||q)\rightarrow +\infty $. 正态分布:所有概率密度都是非负的,所以不会出现上述情况. 换而言之, pq之间的KL散度可能导致惩罚或者梯度差距极大. JS散度JS散度实际就是KL散度的一个扩展,被用来推导GAN. D _ { JS} ( p \| q ) =\frac{1}{2}D _ { KL} ( p \| \frac{p+q}{2} ) +\frac{1}{2}D _ { KL} ( q \| \frac{p+q}{2} )需要注意 JS散度是对称的. JS散度有界,范围是$[0,\log2]$这个上界不同情况,见http://pages.stern.nyu.edu/~dbackus/BCZ/entropy/Jensen%96Shannon-divergence-Wikipedia.pdf KL散度和JS散度度量的时候有一个问题： 如果两个分配P,Q离得很远，完全没有重叠的时候，那么KL散度值是没有意义的，而JS散度值是一个常数。这在学习算法中是比较致命的，这就意味这这一点的梯度为0。梯度消失了。 Renyi $\alpha$ -散度 D _ { \alpha} ( p \| q ) = \frac{1}{\alpha(\alpha-1)}\log \bigg( \int \frac { p ( x ) ^{\alpha}} { q ( x )^{\alpha-1} } d x \bigg)当$\alpha \rightarrow 0或1$时,可以得到KL散度或者reverse KL散度. 只有当$\alpha=0.5$的时候对称. 互信息 Mutual Information互信息, 很难计算. I ( X ; Y ) = \int _ { Y } \int _ { X } p ( x , y ) \log \left( \frac { p ( x , y ) } { p ( x ) p ( y ) } \right) d x d y \\\ I ( X ; Y ) = \sum _ { y \in Y } \sum _ { x \in X } p ( x , y ) \log \left( \frac { p ( x , y ) } { p ( x ) p ( y ) } \right)对比互信息和KL散度的公式,可以得到 I ( X ; Y ) = D_{KL}\bigg(p(x,y)||p(x)p(y)\bigg)此外,令$p(x|y)=\frac{p(x,y)}{p(y)}$, 则 \mathrm { I } ( X ; Y ) \\ = \sum _ { y } p ( y ) \sum _ { x } p ( x | y ) \log _ { 2 } \frac { p ( x | y ) } { p ( x ) } \\ = \sum _ { y } p ( y ) D _ { \mathrm { KL } } ( p ( x | y ) \| p ( x ) ) \\ = \mathbb { E } _ { Y } \left[ D _ { \mathrm { KL } } ( p ( x | y ) \| p ( x ) ) \right]距离一个真正的距离或者度量要满足3个条件: TV距离里还证明了 EBGAN 的 loss 是在 minimize TV 距离，而 TV 距离与 JS 是等价的（事实上 KL &gt; TV = JS &gt; EM ） Wasserstein距离定义为 W _ { p } ( \mu , \nu ) : = \left( \begin{array} { c } { \inf } \\ { \gamma \in \Gamma ( \mu , \nu ) } \end{array} \int _ { M \times M } d ( x , y ) ^ { p } \mathrm { d } \gamma ( x , y ) \right)^{1/p} W _ { p } ( \mu , \nu ) ^ { p } = \inf \mathbf { E } \left[ d ( X , Y ) ^ { p } \right]这里$\Gamma ( \mu , \nu )$ 是$ \mu , \nu $分布组合起来的所有的可能的联合分布的集合, $(x,y)$代表从$\gamma$中进行采样出一对样本. 针对这对样本,从x搬移到y处所需要的距离就是 $d(x,y)$. 加上期望就代表,把所有$\mu $搬移到$\nu$所需要的距离. Wasserstein距离满足距离的三个要求. 可以证明其满足三角不等式 W _ { p } ( \mu , \nu ) \leq \left( \int | x - z | ^ { p } d \gamma \right) ^ { 1 / p } = \left( \int | x - z | ^ { p } d \sigma \right) ^ { 1 / p } = \| x - z \| _ { L ^ { p } ( \sigma ) } \\\ \leq \| x - y \| _ { L ^ { p } ( \sigma ) } + \| y - z \| _ { L ^ { p } ( \sigma ) } = \left( \int | x - z | ^ { p } d \sigma \right) ^ { 1 / p } + \left( \int | x - z | ^ { p } d \sigma \right) ^ { 1 / p } \\\ = \left( \int | x - z | ^ { p } d \gamma ^ { + } \right) ^ { 1 / p } + \left( \int | x - z | ^ { p } d \gamma ^ { - } \right) ^ { 1 / p } = W _ { p } ( \mu , \rho ) + W _ { p } ( \rho , \nu )Wessertein距离相比KL散度和JS散度的优势在于：即使两个分布的支撑集没有重叠或者重叠非常少，仍然能反映两个分布的远近。而JS散度在此情况下是常量，KL散度可能无意义.(6)中的$W _ { p } ( \mu , \nu ) ^ { p } $又叫$W_p$距离, 这样就会有$W_1$距离和$W_2$距离 $W_1$距离(EMD距离)TODO $W_2$距离$W_p​$距离的求解是非常麻烦的,但是在某种情况下求解很方便. 针对两个高斯分布,$W_2$距离有闭式解(线性时间求解): d i s t = W _ { 2 } \left( \mathcal { N } \left( \mu _ { 1 } , \Sigma _ { 1 } \right) , \mathcal { N } \left( \mu _ { 2 } , \Sigma _ { 2 } \right) \right) \\\ d i s t ^ { 2 } = \left\| \mu _ { 1 } - \mu _ { 2 } \right\| _ { 2 } ^ { 2 } + \operatorname { Tr } \left( \Sigma _ { 1 } + \Sigma _ { 2 } - 2 \left( \Sigma _ { 1 } ^ { 1 / 2 } \Sigma _ { 2 } \Sigma _ { 1 } ^ { 1 / 2 } \right) ^ { 1 / 2 } \right)如果这两个高斯分布的方差矩阵为对角矩阵,即$\Sigma { 1 } \Sigma { 2 } = \Sigma { 2 } \Sigma { 1 }$,上式可以进一步简化. W _ { 2 } \left( \mathcal { N } \left( \mu _ { 1 } , \Sigma _ { 1 } \right) ; \boldsymbol { N } \left( m _ { 2 } , \Sigma _ { 2 } \right) \right) ^ { 2 } = \left\| \mu _ { 1 } - \mu _ { 2 } \right\| _ { 2 } ^ { 2 } + \left\| \Sigma _ { 1 } ^ { 1 / 2 } - \Sigma _ { 2 } ^ { 1 / 2 } \right\| _ { F } ^ { 2 }JS散度,KL散度,Wessertein距离对比举例如下: 考虑如下二维空间中的两个分布pq，p在线段AB上均匀分布，q在线段CD上均匀分布，通过控制参数$\theta$可以控制着两个分布的距离远近。 D_{KL}(p||q)=D_{KL}(q||p)= \left\{ \begin{array} { l l } { + \infty } & { \text { if } \theta \neq 0 } \\ { 0 } & { \text { if } \theta = 0 } \end{array} \right. D_{JS}(p||q)= \left\{ \begin{array} { l l } { \log 2 } & { \text { if } \theta \neq 0 } \\ { 0 } & { \text { if } \theta = 0 } \end{array} \right. W(p,q)=|\theta|可以看出JS和KL都出现了突变,而W距离保持平滑 Reference https://blog.csdn.net/uestc_c2_403/article/details/75208644 https://www.zhihu.com/question/39872326/answer/83688277 https://zhuanlan.zhihu.com/p/25071913 http://pages.stern.nyu.edu/~dbackus/BCZ/entropy/Jensen%96Shannon-divergence-Wikipedia.pdf The Wasserstein distances Optimal Transport for Applied Mathematicians http://www.geometry.caltech.edu/pubs/dGBOD12.pdf]]></content>
      <categories>
        <category>ML &amp; DL</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Wasserstein</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sentences 1-100]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F11%2F13%2F1-100%2F</url>
    <content type="text"><![CDATA[One of the holy grails of machine learning is to compute meaningful low-dimensional embeddings for high-dimensional complex objects. That ability is crucial to tackle advanced tasks, such as inference on texts using word embeddings [Pennington et al., 2014, Bojanowski et al., 2016], image understanding [Norouzi et al., 2014] or concise representations for nodes in a huge graph [Grover and Leskovec, 2016]. While learning and performing inference on homogeneous networks have motivated a large amount of research, few work exists on heterogeneous networks and there are open and challenging issues for existing methods that were previously developed for homogeneous networks. We demonstrate empirically that Walklets’s latent representations encode various scales of social hierarchy, and improves multi-label classification results over previous methods on a variety of real world graphs Embedding complex objects as vectors in low dimensional spaces is a longstanding problem in machine learning. To learn the community embedding, we hinge upon the insight that We propose in this work a comprehensive framework for probabilistic embeddings, in which point embeddings are seamlessly handled as a particular case. The cornerstone of our approach lies in the use of the 2-Wasserstein distance Classification in heterogeneous networks, representative of real world media, is much more recent with only a few attempts for now. Many of them rely on the idea of mapping an heterogeneous network onto an homogeneous network so that classical relational techniques are then used [8, 10, 14, 2]. Motivated by the ideology of distributed representation learning in recent years, network representation learning (NRL) is proposed to address these issues. NRL aims to learn a real-valued vector for each vertex to reflect its network in-formation Yet, the deployment of Wasserstein distances in a wide class of applications is somehow limited, especially because of an heavy computational burden. These low-dimensional representations are distributed; meaning each social phenomena is expressed by a subset of the dimensions and each dimension contributes to a subset of the social concepts expressed by the space. This ubiquitous form of representing information has been studied in many disciplines. For instance, in connection with statistical learning, it draws much input from Relational Learning, which aims at capturing the correlation between connected objects, especially in the presence of uncertainty To lift this limitation, several reformulations of elliptical distributions have been proposed to handle degenerate scale matrices Though its capacity to learn meaningful embeddings has been highlighted in (Weston et al., 2012), it has never been used, to the best of our knowledge, for mimicking a speciﬁc distance that exhibits computation challenges. Graph data is becoming increasingly popular, thanks to the proliferation of various social media (e.g., blogs, Flickr, Twitter) and many other kinds of information networks (e.g., DBLP, knowledge graphs) Discrete latent-variable models are critical to the endeavor of unsupervised learning because of the ubiquity of discreteness in the natural world, and hence In recent years there has been a surge of interest in learning compact distributed representations or embeddings for many machine learning tasks, including collaborative filtering (Koren et al., 2009), image retrieval (Weston et al., 2011), relation extraction (Riedel et al., 2013), word semantics and language modeling (Bengio et al., 2006; Mnih &amp; Hinton, 2008; Mikolov et al., 2013), and many others. There is a long line of previous work in mapping data cases to probability distributions, perhaps the most famous being radial basis functions It has been shown that optimizing with respect to the Wasserstein loss has various practical beneﬁts over the KL-divergence loss The assumption we make in the paper is that nodes of different types do influence each other, so that labels of the different node types will be inter-dependent; modeling this dependence between different node types is thus important for accurate node classification or labeling and cannot be achieved via classical homogeneous network formulations Without loss of generality, we define the relations between vertices as a set of labels, instead of a single label It is important to mention that the proposed algorithm learns a latent representation of the network nodes so that all nodes, irrespectively of their type, will share a common latent space. Such representation will then be effectively used to infer the categories Reviews are typically informative, pooling an extensive wealth of knowledge for prospective customers.However, the extensive utility of reviews do not only end at this point As such, there have been immense interest in collaborative filtering systems that Recent advances in deep learning has spurred on various innovative models that exploit reviews for recommendation [3, 26, 41]. The intuition is simple yet powerful, i.e., For richer modeling of user and item reviews, deeper and highly accessible interaction interfaces between user-item pairs should be mandatory. the naive concatenation of reviews into a single document is unnatural, ad-hoc and noisy In this section, we identify and review several key areas which are highly relevant to our work. We investigate the inner workings of our proposed model and provide insight about how MPCN works under the hood. Our experiments are designed to answer the following research questions (RQs): This ascertains the effectiveness of our proposed model and clearly answers RQ1. Unlike the reconstruction task, this task predicts the future links instead of reconstructing the existing links. In addition, we add Common Neighbor in this task because it has been proved as an effective method to do link prediction . XXX alse extends the DeepWalk in generating random walks fed to SkipGram by only selecting unvisited neighbors of the predecessor node in the early stage it’s can consider more information in diferent edges2 choosing neighbors of the predecessors node with bias probability w.r.t. different connection weights, meaning the co-occurrence frequency of two nodes in the corpus reflects their correlation The key difference from existing embedding methods for a single net- work is that there are three kind of edges and two type of latent spaces corresponding to two networks The experiment is to evaluate the effectiveness of the latent features learned by the proposed EOE model on com-mon data mining tasks including visualization, link predic- tion, multi-class classification, and multi-label classification. The basic idea of our approach is to preserve the HIN structure information into the learned embeddings, such that vertices which co-occur in many path instances turn to have similar embeddings. Since learning rich vertex embeddings does not require the accurate probability of each path instance, we adopt the NCE for optimization, which has been proved to significantly accelerate the training without cutting down the embedding qualities However, the discriminant power of the node embeddings maybe improved by considering the node label information and the node attribute information. Recently, motivated by the success of the unsupervised distributed representation learning techniques in the natural language processing area, several novel network embedding methods have been proposed to learn distributed dense representations for networks Instead of handcrafed network feature design, these representation learning methods enable the automatic discovery of useful and meaningful (latent) features from the “raw networks.” Yet a large number of social and information networks are heterogeneous in nature, involving diversity of node types and/or relationships between nodes How do we efectively preserve the concept of “word-context” among multiple types of nodes, e.g., authors, papers, venues, organizations, etc Recently, network embedding has been utilized to fill the gap of applying tuple-based data mining models to networked datasets by learning embeddings which preserve the network structure For the implementation of the algorithm to solve the EII model, the dimension of embeddings is set as… Different from existing approaches that rely heavily on feature engineering, we propose to use network embedding approach to address the problem, which can automatically represent nodes into lower dimensional feature vectors Although feature engineering can incorporate prior knowledge of the problem and network structure, usually it is time-consuming, problem specific (thus not transferable), and the extracted features may be too simple for complicated data sets A key idea behind network embedding is learning to map nodes into vector space, such that the proximities among nodes can be preserved From these two examples, it is easy to see that in a heterogeneous network, even compare two nodes of the same type (e.g. paper), going from different paths can lead to different semantic meanings. The networked data is usually high-dimensional and sparse, as there can be many nodes but the links are usually sparse [1]. This brings challenges to represent nodes in the network Most of existing network embedding techniques [17, 26, 25] are based on the idea that, embeddings of nodes can be learned by neighbor prediction, which is to predict the neighborhood given a node, i.e. the linking probability P(j|i) from node i to node j. The former focuses more on the direct information related to the specific task, while the latter can better explore more global and diverse information in the heterogeneous information network. We study the hyper-parameters ω, which is the trade-off term for combing A and B Machine learning applications in net-works (such as network classification [16,37], content rec- ommendation [12], anomaly detection [6], and missing link prediction [23]) must be able to deal with this sparsity in order to survive. These low-dimensional representations are distributed; meaning each social phenomena is expressed by a subset of the dimensions and each dimension contributes to a subset of the social concepts expressed by the space. It is this connection to local structure that motivates us to use a stream of short random walks as our basic tool for extracting information from a network The objective of a good embedding is to preserve the proximity (i.e., similarity) between vertices in the original graph The availability and growth of large networks, such as social net- works, co-author networks, and knowledge base graphs, has given rise to numerous applications that search and analyze information in them The proximity among objects in a HIN is not just a measure of closeness or distance, but it is also based on semantics The heterogeneity of nodes and edges in HINs bring challenges, but also opportunities to support important applications Deep Learning’s recent successes have mostly relied on Convolutional Networks,which exploit fundamental statistical properties of images, sounds and video data:the local stationarity and multi-scale compositional structure, that allows expressing long range interactions in terms of shorter, localized interactions In this paper we consider the general question of how to….. In particular, we deveplop… In this work, we are interested in generalizing convolutional neural networks(CNNs) from low-dimensional regular grids, where image, video and speech arerepresented, to high-dimensional irregular domains, such as social networks, brainconnectomes or words’ embedding, represented by graphs User data on social networks, gene data on biological regulatory networks, log data on telecom-munication networks, or text documents on word embeddings are important examples of data lyingon irregular or non-Euclidean domains which can be structured with graphs, which are universalrepresentations of heterogeneous pairwise relationships To address these issues, Network Representation Learning (NRL) aims at the possibility of encoding node information in a unified continuous space Despite the success of embedding individual nodes for graph analytics, we notice that an important concept of embedding communities (i.e., groups of nodes) is missing in the literature Our major insight to learn the community embedding is hinged upon the mutual reinforcement between node embedding and community embeddin With the success of deep learning in recent years, there is a marked switch from hand-crafted features to those that are learned from raw data in recommendation research Besides, this research sheds new light on the usage of heterogeneous information in the knowledge base, which can be consumed in more application scenarios However, for relational network classification, DeepWalk can be suboptimal as it lacks a mechanism to optimize the objective of the target task The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data Good representations are expressive, meaning that areasonably-sized learned representation can capture a hugenumber of possible input configurations One of the challenges of representation learning that distinguishes it from other machine learning tasks such as classification is the difficulty in establishing a clear objective, or target for training As the core of semantic proximity search, we have to measure the proximity on a heterogeneous graph, whose nodes are various types of objects. With recent development on graph embedding, we see a good chance to avoid feature engineering for semantic proximity search The core problem of semantic proximity search is how to measure the proximity on a heterogeneous graph with various types of objects Embedding nodes is an indirect approach to learn proximity, because it lacks an explicit representation for the network structure between two possibly distant nodes To enable knowledge discovery from such networked data, network representation learning (NRL) aims to learn vector representations for network nodes, such that off-the-shelf machine learning algorithms can be directly applied Existing studies on network representation learning (NRL) have shown that NRL can effectively capture network structure to facilitate subsequent analytic tasks, such as node classification [3], anomaly detection [4], and link prediction Despite its great potential, learning useful network representations faces several challenges. (1) Sparsity (2) Structure preserving (3) Rich-Content How to leverage rich content information and its interplay with network structure for representation learning remains an open problem. By simultaneously integrating homophily, structural context, and node content, HSCA effectively embeds a network into a single latent representation space that captures the interplay between the three information sources. Graph embedding aims to reduce the dimensionality of the original data in the vector-based form, while NRL seeks to learn effective vector representations of network nodes to facilitate downstream network analytic tasks. This approach turns the discrete topology of the relations into a continuous one, enabling the design of efficient algorithms and potentially benefitting many applications. This source of information has been ignored by most of the current recommender systems while it can potentially alleviate the sparsity problem and improve the quality of recommendations With the large volume of new information created every day, determining the validity of information in a knowledge graph and filling in its missing parts are crucial tasks for many researchers and practitioners A number of KGC algorithms have been developed in recent years, and the most successful models all have one thing in common: they use low-dimensional embedding vectors to represent entities and relationships. With these considerations in mind, in the present work we rethink some of the basic decisions made by previous models to create a projection embedding model (ProjE) for KGC. The newest contributions in this line of work focus primarily on the changes in how the embedding planes are computed and/or how the embeddings are combined To recap, the contributions of the present work are as follows: In recent years, a series of approaches have been proposed to embed a knowledge graph into a continuous vector space while preserving the properties of the knowledge graph Generally speaking, precise link prediction would improve the feasibility of knowledge completion, the effectiveness of knowledge reasoning, and the performance of many knowledge-related tasks What is the best way to describe a user in a social network with just a few numbers? Mathematically, this is equivalent to assigning a vector representation to each node in a graph, a process called graph embedding Through a graph embedding, we are able to visualize a graph in a 2D/3D space and transform problems from a non-Euclidean space to a Euclidean space, where numerous machine learning and data mining tools can be applied Numerous embedding algorithms have been proposed in data mining, machine learning and signal processing communities. Naturally, it is necessary for network embedding to preserve the first-order proximity because it implies that two vertexes in real world networks are always similar if they are linked by an observed edge Experimental results using reviews from 50 product types show significant improvements over state- of-the-art baseline models. We conduct extensive experiments to demonstrate the performance of BASS, concluding that our approach significantly outperform state-of-the-art approaches. The advantage is multi-folded. Firstly,.. Secondly，…]]></content>
      <categories>
        <category>English</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[18ArXiv_HIB_MODELING UNCERTAINTY WITH HEDGED INSTANCE EMBEDDING]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F11%2F07%2F18ArXiv_HIB%2F</url>
    <content type="text"><![CDATA[之前的Gaussian Embedding都是将Object映射为一个高斯分布来捕捉不确定性. 但是一个高斯分布足够吗? 本文将Object映射为一组高斯分布,即混合高斯. Stochastic Embedding是将Object映射为一个分布,包括Gaussian Embedding. Point Embedding是将Object映射为一个点, 包括Vector Embedding. Motivation前面已经有很多工作将各种各样的Object映射为高斯分布来捕获不确定性.但是如果Object的不确定性很高,一个高斯分布也不足以捕获呢? 本文提出了HIB来解决上述问题.所谓的”hedging its bets”就是为了避免损失,多处下注.对应到本文,对于有歧义的输入,将其映射为多个高斯分布, 每个高斯分布代表其中一个意思. 针对图像的歧义需求,设计了$N-digits MNIST$ 数据集,即将mnist中的数字组合成一张新图,这样每张图就有歧义了. 这张图就很好的解释了本文的Motivation. 作者总结了本文的3个贡献: 效果提升,尤其是针对一些不确定的输入. 加强了结构正则??怎么理解 也没理解.. Model模型主要思路是通过variational information bottleneck principle来学习hedged instance embedding (HIB). hedged instance embedding (HIB).这里首先介绍两个输入的匹配概率, 实际是输入x到分布Z变换$Z \sim p(z|x)$, 计算Z的匹配概率(如果这里直接用W2距离计算一下呢?) p ( m | x _ { 1 } , x _ { 2 } ) = \int p ( m | z _ { 1 } , z _ { 2 } ) p \left( z _ { 1 } | x _ { 1 } \right) p \left( z _ { 2 } | x _ { 2 } \right) \mathrm { d } z _ { 1 } \mathrm { d } z _ { 2 }为了求这个积分分两步: $p(z|x)$这个一般都是用神经网络来映射. 求$p ( m | x { 1 } , x { 2 } )$ 这里用了MC Sampling, 分别从两个输入$x1,x_2$ 映射后的$z { 1 } ^ { \left( k { 1 } \right) } \sim p \left( z { 1 } | x { 1 } \right)$ 和$z { 2 } ^ { \left( k { 2} \right) } \sim p \left( z { 2} | x _ { 2 } \right)$中采样多个样本. p ( m | x _ { 1 } , x _ { 2 } ) \approx \frac { 1 } { K ^ { 2 } } \sum _ { k _ { 1 } = 1 } ^ { K } \sum _ { k _ { 2 } = 1 } ^ { K } p ( m | z _ { 1 } ^ { \left( k _ { 1 } \right) } , z _ { 2 } ^ { \left( k _ { 2 } \right) } )到这里模型其实已经可以用了,但是本文所强调的混合高斯怎么融入到上述模型里呢?(猜测是用多个NN来得到均值方差,再组合) 这里其实很简单,就是将$p(z|x)$这个映射变复杂,采样也变成从多个高斯分布中采样. p(z|x)=\sum _ { c = 1 } ^ { C } \mathcal { N } ( z ; \mu ( x , c ) , \Sigma ( x , c ) )到现在模型其实新颖程度创新程度感觉还不够, 看来有一部分创新在VIB里. variational information bottleneck principle为了训练上述模型, 本文混合了soft contrastive loss 和 the VIB principle.这里利用了17ICLR_VIB_Deep variational information bottleneck的推导结果 I ( z , y ) - \beta I ( z , x )直观理解就是:学到的z和标签y很像,同时 z和输入x尽量相似度小. 这样在保证z有效性的同时,又可以滤除x中无用部分. 互信息介绍介绍详见 &lt;常见散度与距离(KL散度,JS散度,Wasserstein距离, NMI)&gt; 将公式(4)进行放缩,可以得到(5),详细推导见 最终推导得到 - \mathcal { L } _ { \mathrm { VIB } } = \mathbb { E } _ { z \sim p ( z | x ) } [ \log q ( y | z ) ] - \beta \cdot \mathrm { KL } ( p ( z | x ) \| r ( z ) )这里的第二项是正则项, 约束生成z的分布$p(z|x)$为一个单位高斯分布,这里$r ( z ) = \mathcal { N } ( z ; 0 , I )$ . 并结合本文Embedding需求,对上述loss里的一些项进行了替换,详细推导见附录. \begin{aligned} \mathcal { L } _ { \mathrm { VIBEmb } } = & - \mathbb { E } _ { z _ { 1 } \sim p \left( z _ { 1 } | x _ { 1 } \right) , z _ { 2 } \sim p \left( z _ { 2 } | x _ { 2 } \right) } [ \log p ( m = \hat { m } | z _ { 1 } , z _ { 2 } ) ] \\ & + \beta \cdot \left[ \mathrm { KL } \left( p \left( z _ { 1 } | x _ { 1 } \right) \| r \left( z _ { 1 } \right) \right) + \mathrm { KL } \left( p \left( z _ { 2 } | x _ { 2 } \right) \| r \left( z _ { 2 } \right) \right) \right] \end{aligned}(5)中$q(y|z)$是根据表示z预测标签y,对应这里的$\log p ( m = \hat { m } | z { 1 } , z { 2 } ) $,这一项就是soft contrastive loss . ExperimentTODO Summary之前的论文都用高斯分布来捕获Object的不确定性取得了一定的效果.但是单个高斯分布就够好了吗?本文通过例子分析了单个高斯的不足,提出用多个高斯分布来描述Object的歧义. 想法很好,但是感觉力度不够. 本文亮点的Mixture of Gaussians (MoG) embedding就只一句话描述,不够清楚; 强调了VIB也只是利用现有的工作,换个loss形式. 个人感觉最近一些Gaussian Embedding的论文很多都是:换个Gaussian Distribution的形式+换个loss/distance.]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Wasserstein</tag>
        <tag>Gaussian Embedding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[18ICLR_DWE_Learning Wasserstein Embeddings]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F11%2F07%2F18ICLR_DWE%2F</url>
    <content type="text"><![CDATA[s 18iclr]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Wasserstein</tag>
        <tag>Embedding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[18ICLR_WAE_Wasserstein auto-encoders]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F11%2F07%2F18ICLR_WAE%2F</url>
    <content type="text"><![CDATA[TODO]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Wasserstein</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[18KDD_DVNE_Deep Variational Network Embedding in Wasserstein Space]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F11%2F07%2F18KDD_DVNE%2F</url>
    <content type="text"><![CDATA[本文来自network embedding大佬THU 崔鹏老师.在Wasserstein空间将节点表示为一个高斯分布. 感觉这篇实际是18ICLR_Graph2Gauss和18ICLR_WAE_Wasserstein auto-encoders的结合. Motivation本文上来罗列了很多的优点: 节点映射为Wasserstein space的一个gaussian distribution来保持网络结构和不确定性 network embedding的fundamental problem就是保持网络本身的结构和性质(如传递性,X阶结构) Wasserstein distance可以保持transitivity,并且W-2距离对高斯分布有闭市解,线性计算复杂度 KL散度不对称, JS对称但是和KL散度一样无法保持传递性 均值代表节点的位置, 方差代表节点的不确定性 可以保持局部和全局结构 保持一阶相似性和二阶相似性感觉已经成了NE的基本操作了.. 然后分析了一波现有方法的不足 Wasserstein distance相对现有KL的优势,可以保持transitivity 现有方法是把均值向量和方差向量拼接一起作为最终的表示,比较粗糙. 无法反映他们之前的本质关系. 无法保持高阶相似性. graph2gauss可以,但是需要计算最短路径,比较耗时. 但是我觉得相对于Graph2Gauss比较特别的有两点 换了个Wasserstein distance保持transitivity,并且W2距离对于两个高斯分布有闭式解,计算高效. 通过深度变分模型来保持均值与方差之间的关系.(用采样过程,代替简单的拼接,见5.3实验分析) 和之前一样,每个节点的表示$h_i=\mathcal{N}(\mu_i, \sum_i)$ 一个均值向量和一个方差矩阵.这里方差矩阵只考虑对角方差矩阵. Model首先看到模型图就感觉和SDNE很像.SNDE是约束节点对相似,这里是约束三元组的rank关系. 首先看下这里如何对两个高斯分布进行相似性度量,这里选用Wasserstein distance来度量分布的相似性,特别的这里选用$W_2$ 距离. \begin{aligned} \operatorname { dist } & = W _ { 2 } \left( \mathcal { N } \left( \mu _ { 1 } , \Sigma _ { 1 } \right) , \mathcal { N } \left( \mu _ { 2 } , \Sigma _ { 2 } \right) \right) \\ d i s t ^ { 2 } & = \left\| \mu _ { 1 } - \mu _ { 2 } \right\| _ { 2 } ^ { 2 } + \operatorname { Tr } \left( \Sigma _ { 1 } + \Sigma _ { 2 } - 2 \left( \Sigma _ { 1 } ^ { 1 / 2 } \Sigma _ { 2 } \Sigma _ { 1 } ^ { 1 / 2 } \right) ^ { 1 / 2 } \right) \end{aligned}又因为这里只考虑对角方差矩阵$\Sigma { 1 } \Sigma { 2 } = \Sigma { 2 } \Sigma { 1 }$, 上述可以进一步化简 W _ { 2 } \left( \mathcal { N } \left( \mu _ { 1 } , \Sigma _ { 1 } \right) ; \mathcal { N } \left( m _ { 2 } , \Sigma _ { 2 } \right) \right) ^ { 2 } = \left\| \mu _ { 1 } - \mu _ { 2 } \right\| _ { 2 } ^ { 2 } + \left\| \Sigma _ { 1 } ^ { 1 / 2 } - \Sigma _ { 2 } ^ { 1 / 2 } \right\| _ { F } ^ { 2 }Loss分为两部分,类似SDNE的的loss 一阶相似性: 用了和Graph2Gauss一样的基于ranking 的loss, \mathcal { L } _ { 1 } = \sum _ { ( i , j , k ) \in \mathrm { D } } \left( E _ { i j } ^ { 2 } + \exp \left( - E _ { i k } \right) \right) 二阶相似性:用WAE来编码每个节点的邻接向量.这里对WAE进行了简化,最后和AE的区别就是encoder后得到的分布中采样的结果才是隐表示Z. \mathcal { L } _ { 2 } = \inf _ { Q ( Z | X ) \in Q } \mathbb { E } _ { P _ { X } } \mathbb { E } _ { Q ( Z | X ) } \left[ \| X \circ ( X - G ( Z ) ) \| _ { 2 } ^ { 2 } \right] 这里也像SDNE一样加大对非零元素的惩罚,只是惩罚系数为0,1 所以SDNE中的B成了这里的X. 优化. 这里用了repara trick.这里变分自编码器学习到的是均值$\mu$和标准差$\sigma$ , 从均值为0,标准差为1的高斯分布中采样,再放缩平移得到Z. \mathbf { z } _ { i } = \mu _ { i } + \sigma _ { i } * \epsilon , \epsilon \sim \mathcal { N } ( 0 , \mathbf { I } ) 激活函数的选择,有两个需要注意 均值没有用激活函数, 为什么? 这里用elu()+1来保证$\sigma_i$ 是正的. 标准差没有负数! sigmoid来压缩输出到[0,1]之间. Exp数据集:都是无向图, 无属性. 但是graph2gauss需要属性,这里用one-hot代替 为了保证公平,所有学习高斯分布的算法 dim(均值)+dim(方差)=L,本文虽然不是拼接也遵守这个规定. 两类有效性实验: 基于相似性的,包括网络重构和链路预测,这里是基于节点的均值和方差来计算相似性.无向图在计算KL的时候是怎么算的呢? 网络重构的提升很小,链路预测提升还可以. 为什么这两个任务的提升差距这么大? 分类任务,这里对于学习分布的算法,只用了节点的均值来作为输入. 感觉对G2G不公平. 比较奇怪的是,这里用KL距离和W2距离时,DVNE的表现差距很大. DVNE_KL甚至不如很多baseline. 这说明gaussian embedding起到的作用没有W2距离大? 一个不确定性实验: 这个算比较有特色的.这里提出一个intuition: 节点的度越小,越难得到精确的point-vector.换句话说, 节点的度越大,那么连接提供的信息越多, 这个节点也越加稳定. 个人觉得这里的intuition不够准确, graph2gauss的描述更为精确 测试节点的度和方差的关系. 节点的度越大,方差越小. 不确定性中的方差项有助于处理噪声边(为什么有助于?). 注意纵坐标是AUC的下降程度.这里可以就看出噪声边的比例增加,本文算法的AUC下降的最少.但是比较奇怪的是G2G也是gaussian Embedding,为什么它的的下降程度较高? 作者的解释是G2G中均值和方差只是拼接,并没有很好的结合在一起. Conclusion本文相对于Graph2Gauss的改进在于 换了W2距离 把编码均值方差的部分变复杂了,用了个WAE 但是从实验结果看 采样的方式来利用均值方差貌似作用不大,关键还是这个W2距离. 如果进一步做,可以考虑做非对角的方差矩阵.如果方差矩阵非对角,那是什么分布呢?答案是椭圆分布Elliptical Distributions. 且听下回分解.18NIPS_Generalizing Point Embeddings using the Wasserstein Space of Elliptical Distributions]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Network Embedding</tag>
        <tag>Wasserstein</tag>
        <tag>Gaussian Embedding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[18NIPS_Distilled Wasserstein Learning for Word Embedding and Topic Modeling]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F11%2F07%2F18NIPS_Distilled-Wasserstein-Learning%2F</url>
    <content type="text"><![CDATA[Motivationword embedding考虑了word之间的关系;而topic model 没有考虑word order和word间的关系.这样会导致一个问题就是word embedding和topic distribution之间mismatch. 为什么要考虑word order呢? 因为在某些场景下, 比如本文的医疗处理记录中, 一些操作的先后顺序会极大的影响对操作/疾病/流程的分析.]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Network Embedding</tag>
        <tag>Gaussian Embedding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[18ICLR_Graph2Gauss_DEEP GAUSSIAN EMBEDDING OF GRAPHS_UNSUPERVISED INDUCTIVE LEARNING VIA RANKING]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F10%2F27%2F18ICLR_Graph2Gauss%2F</url>
    <content type="text"><![CDATA[目前主要的network embedding方法都是将节点映射为向量,但是向量所能表示的信息还是不够.受word gaussian embedding的启发,用一个高斯分布来表示word,可以捕获word的多重含义.类比到网络中,node可能有不同方面的信息(多个研究领域,属于多个社区),一些算法尝试将node也映射为高斯分布. 节点表示相当于只学习了高斯分布的均值,而高斯表示除了均值外还可有方差,包括了更多的信息. 但是还有一些困惑: 高斯分布所谓的”捕获不确定性”到底对评测任务(节点分类)有什么提升? 单个高斯分布 VS 混合高斯模型? 目前都是假定协方差矩阵是对角的,如果不是对角的呢?(18NIPS已经做了) 下面梳理了几篇gaussian embedding的文章,包括word和node Motivation网络中的节点可能含有丰富的甚至冲突的语义,比如节点属于不同的社区,有着不同的爱好.(这点和word2gaussian中word的多义,歧义现象符合).这种现象被解释为不确定性.那么如何在进行网络表示学习,对节点进行embedding的时候来捕获这种不确定性呢?那就是将节点映射为高斯分布,而不是一个向量. 高斯分布除了均值部分(相当于原来向量表示),还有方差(不确定性). 为了做上面这件事,本文用无监督的rank,同时利用属性和结构来做一个inductive的算法. Model: Deep Gaussian Embedding这里对Gaussian Embedding的定义:每个节点的表示$h_i=\mathcal{N}(\mu_i, \sum_i)$ 一个均值向量和一个方差矩阵.注意这里只考虑对角方差矩阵. encoder这里为了得到节点的$h_i=\mathcal{N}(\mu_i, \sum_i)$,本文用神经网络对节点属性进行变换得到intermediate hidden representation,然后分别引出两个L维向量代表$\mu$和$\sum_i$. ranking本文认为rank可以比一阶二阶更好捕获局部和全局结构.所以基于节点对之间的最短路径长度,对节点对的相似性进行了排序. \Delta \left( \mathbf { h } _ { i } , \mathbf { h } _ { k _ { 1 } } \right) < \Delta \left( \mathbf { h } _ { i } , \mathbf { h } _ { k _ { 2 } } \right) < \cdots < \Delta \left( \mathbf { h } _ { i } , \mathbf { h } _ { k _ { K } } \right) \quad \forall k _ { 1 } \in N _ { i 1 } , \forall k _ { 2 } \in N _ { i 2 } , \ldots , \forall k _ { K } \in N _ { i K }但是基于最短路径的rank足够好吗? 感觉最短路径长的节点相似性也可以很高. Similarity &amp; Dissimilarity这里用了非对称的KL来计算相似性(适用于无向图), 也可以用对称的JS. \Delta \left( \mathbf { h } _ { i } , \mathbf { h } _ { j } \right) = D _ { K L } \left( \mathcal { N } _ { j } \| \mathcal { N } _ { i } \right) =\\\ \frac { 1 } { 2 } \left[ \operatorname { tr } \left( \Sigma _ { i } ^ { - 1 } \Sigma _ { j } \right) + \left( \mu _ { i } - \mu _ { j } \right) ^ { T } \Sigma _ { i } ^ { - 1 } \left( \mu _ { i } - \mu _ { j } \right) - L - \log \frac { \operatorname { det } \left( \Sigma _ { j } \right) } { \operatorname { det } \left( \Sigma _ { i } \right) } \right]KL散度有一些固有的缺点 非对称(虽然有些论文说这是特点,适用于有向图). 在两个分布相差较小的时候,KL散度可能无意义. 不满足三角不等式. 所以一些论文尝试换距离,比如Wasserstein 距离.见18KDD_DVNE Loss \mathcal { L } = \sum _ { i } \sum _ { k < l } \sum _ { j _ { k } \in N _ { i k } } \sum _ { j _ { l } \in N _ { i l } } \left( E _ { i j _ { k } } ^ { 2 } + \exp ^ { - E _ { i j _ { l } } } \right) = \sum _ { \left( i , j _ { k } , j _ { l } \right) \in \mathcal { D } _ { t } } \left( E _ { i j _ { k } } ^ { 2 } + \exp ^ { - E _ { i j _ { l } } } \right) \\\ E _ { i j } = D _ { K L } \left( \mathcal { N } _ { j } \| \mathcal { N } _ { i } \right)最小化这个loss: Ijk比较近,能量比较小; Ijl比较远,能量比较大. 为了使得整体最小,需要ijl的能量指数衰减. 可以理解为,对ijk的约束比较大,要很近;对较远的点(负样本)约束比较小. 从而实现了类似margin的作用. Sampling strategy上述loss需要在整个图上算,这里利用采样的方法来近似. \text{replace}\sum _ { \left( i , j _ { k } , j _ { l } \right) \in \mathcal { D } _ { t } } \text { with } \mathbb { E } _ { \left( i , j _ { k } , j _ { l } \right) \sim \mathcal { D } _ { t } }但是直接采样会导致优化过程中对low-degree的点更新较少.本文提出一种an alternative node-anchored sampling strategy:以每个点为一点,采样他们的高阶邻居,组成一批数据进行训练. 然后这种策略依然有着自己的缺点,他估计的梯度是有偏的. Diss 本文在inductive的时候不需要结构信息,而之前的方法graphsage则需要.究其原因,是本文在学习节点表示的时候仅仅利用了属性信息,而结构信息用来计算loss优化模型. 没有属性时,可以用onehot编码来代替属性. 一般能做到O(E)时间复杂度就不错了,这里只有O(N)可以说是相当高效了 Experiment 本文可以处理有向图,但是一些baseline只支持无向图,所以要把有向图无向化,这会给这些baseline一些优势 有些试验需要基于L维向量进行比较,针对这些实验本文学的是L/2维度的, 然后把均值和向量拼接起来进行试验.这种方法感觉就比较粗暴,没有很好利用他们之间的关系.之后的DVNE就较好的融合了均值方差. 链路预测这里用the negative energy $-E_{ij}$作为边存在的概率, 节点的维度L由均值和方差拼接而成. 节点分类这里按照前面的介绍应该是均值和方差拼接为长度为L的向量,但是在代码的example里面,只用了均值. 采样策略相对于随机采样,效果提升不多,但是极大的降低了梯度的方差 不确定性节点的不确定性应该与它邻居的多样性(种类)有关. 邻居种类越少, 节点越确定. Summary本文提出了Graph2Gauss来捕获节点的不确定性,模型感觉也是简洁有效,实验效果也不错.尤其是是没有属性的G2G_oh效果也很好,证明了gaussian embedding的优势.但是本文在求均值和方差所用的模型太简单(全连接),并且没有利用结构信息,这样可能导致学到的gaussian embedding不够好. 可以试试用GNN搞一波.此外,那个ranking信息感觉也有争议.最短路径某些情况下也许不够好. 最后,在进行试验的时候只是简单的将均值方差拼接,有没有更好了方式可以反映他们之间的关系? 且听下回分解&lt;&gt;.]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Network Embedding</tag>
        <tag>Gaussian Embedding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[383. Ransom Note]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F09%2F25%2Fhexo_leetcode%2F383._ransom_note%2F</url>
    <content type="text"><![CDATA[383. Ransom Note题目：https://leetcode.com/problems/ransom-note/ 难度 : Easy https://leetcode.com/problems/ransom-note/description/ 思路1 41% 因为要能构成,又限制每个字母只能用一次 一个magazine能构成的rabsomNote的条件是:ransomNote中每个字母都在magazine中出现且次数不少于ransomNote中的次数 可以构建两个字典,并比较 1234567891011121314151617181920212223242526class Solution: def canConstruct(self, ransomNote, magazine): &quot;&quot;&quot; :type ransomNote: str :type magazine: str :rtype: bool &quot;&quot;&quot; d1 = dict() d2 = dict() for i in list(ransomNote): if i in d1: d1[i] += 1 else: d1[i] = 1 for i in list(magazine): if i in d2: d2[i] += 1 else: d2[i] = 1 for key in list(d1.keys()): if key not in d2: return False if key in d2: if d1[key] &gt; d2[key]: return False return True 思路2 77%基本和思路1一样 只不过用Counter来代替字典统计 12345678910111213141516class Solution: def canConstruct(self, ransomNote, magazine): """ :type ransomNote: str :type magazine: str :rtype: bool """ import collections cnt1 = collections.Counter(ransomNote) cnt2 = collections.Counter(magazine) for key in list(cnt1.keys()): if key not in cnt2: return False if key in cnt2 and cnt1[key] &gt; cnt2[key]: return False return True 思路3 77得到 cnt1和cnt2之后可以直接字典相减 https://www.jianshu.com/p/58d21289ff5c 字典相加: 合并两个字典, 相同键的值相加 字典相减 cnt1 - cnt2: 如果cnt1包含cnt2 对应的键值相减 cnt2包含cnt1, 减不了,返回空* 12345678910111213class Solution: def canConstruct(self, ransomNote, magazine): """ :type ransomNote: str :type magazine: str :rtype: bool """ import collections cnt1 = collections.Counter(ransomNote) cnt2 = collections.Counter(magazine) #这里如果cnt2-cnt1,返回字典 #要用 not cnt1-cnt2, 期望返回空字典=False return not cnt1 - cnt2 略微想了一下，用了一个dictionary来存magazine里面的单字出现的个数，然后来对应check是否可以用来组成ransomNote 123456789101112131415161718192021class Solution(object): def canConstruct(self, ransomNote, magazine): """ :type ransomNote: str :type magazine: str :rtype: bool """ maps = &#123;&#125; for i in magazine: if i in maps: maps[i] += 1 else: maps[i] = 1 for i in ransomNote: if i not in maps: return False else: maps[i] -= 1 if maps[i] &lt; 0: return False return True 解法2： 12345678910111213class Solution(object): def canConstruct(self, ransomNote, magazine): """ :type ransomNote: str :type magazine: str :rtype: bool """ magCounter = collections.Counter(magazine) ranCounter = collections.Counter(ransomNote) for k in ranCounter: if ranCounter.get(k) &gt; magCounter.get(k): return False return True]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[160. Intersection of Two Linked Lists]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F09%2F25%2Fhexo_leetcode%2F160.Intersection_of_two_linked_lists%2F</url>
    <content type="text"><![CDATA[160. Intersection of Two Linked Lists题目:https://leetcode.com/problems/intersection-of-two-linked-lists/ 难度: Easy 如果两个linkedlist有intersection的话，可以看到，其实如果一开始我们就走到b2的话，那么我们就可以两个pointer一个一个的对比，到哪一个地址一样，接下来就是intersection部分。 12345A: a1 → a2 ↘ c1 → c2 → c3 ↗ B: b1 → b2 → b3 比较巧妙的数学解法，看下面的解释和代码 240ms 40% 假设两个链表交集前的长度分别为a和b, 交集后的长度为 c 则,len(L1) = a+c len(L2)=b+c 两个链表同时开始遍历, 遍历完自身之后,开始遍历对方 同步遍历,假定遍历完自己后,分别遍历对方了k1和k2步 这是a+c+k1 = b+c+k2,则k1=b, k2=a 123456789101112131415161718192021222324# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def getIntersectionNode(self, headA, headB): """ :type head1, head1: ListNode :rtype: ListNode """ pA = headA pB = headB while pA is not pB: if pA: pA = pA.next else: pA = headB if pB: pB = pB.next else: pB = headA return pA AC代码如下: 1234567891011class Solution(object): def getIntersectionNode(self, headA, headB): """ :type head1, head1: ListNode :rtype: ListNode """ pA, pB = headA, headB while pA is not pB: pA = pA.next if pA else headB pB = pB.next if pB else headA return pA Just count the number of moves by each pointer before they meet. One pointer will traverse entire list1 for N moves and then jump to the head of list1 to move (M-K) steps to intersection, where K represents the length of common part. Now the other pointer must also moved the same number of steps since they are both moved at the same time. The second pointer traverses the entire list2 for M steps and jumped to the head of list1 to move (N-K) steps. So the loop finished with M+N-K times.详见zzg_zzm的评论 This algorithm is sooooo perfect! I was wonder if the running time is O(n+m), but later I figured out that the actually running time is just: m+n for non-intersection case With intersection: Suppose for LL-A, it’s a+b=n, a is the # of nodes before intersection Suppose for LL-B, it’s c+b=m, c is the # of nodes before intersection Thus the actual running time is a+b+c = n+c = m+a. Actually, when b=0, this just stands for the case with no intersection with a+b+c=n+m]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[2. Add Two Numbers]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F09%2F25%2Fhexo_leetcode%2F002._add_two_numbers%2F</url>
    <content type="text"><![CDATA[2. Add Two Numbers难度: 中等 刷题内容 原题连接 https://leetcode.com/problems/add-two-numbers/description/ 内容描述 123456789You are given two non-empty linked lists representing two non-negative integers. The digits are stored in reverse order and each of their nodes contain a single digit. Add the two numbers and return it as a linked list.You may assume the two numbers do not contain any leading zero, except the number 0 itself.Example:Input: (2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)Output: 7 -&gt; 0 -&gt; 8Explanation: 342 + 465 = 807. 解题方案180ms 18%思路 数字相加问题,注意进位,尤其是最后一位. 详见注释 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution:# def addTwoNumbers(self, l1, l2):# """# :type l1: ListNode# :type l2: ListNode# :rtype: ListNode# """# if not l1:# return l2# if not l2:# return l1# if l1.val + l2.val &lt; 10:# l3 = ListNode(l1.val + l2.val)# l3.next = self.addTwoNumbers(l1.next, l2.next)# else:# l3 = ListNode(l1.val + l2.val - 10)# tmp = ListNode(1)# tmp.next = None# l3.next = self.addTwoNumbers(tmp, self.addTwoNumbers(l1.next, l2.next))# return l3 def addTwoNumbers(self, l1, l2): """ :type l1: ListNode :type l2: ListNode :rtype: ListNode """ if not l1: return l2 if not l2: return l1---title: 62. unique pathscategories: - LeetCode--- # 建立哑节点, return res.next res = ListNode(-1) cur = res carry = 0 while l1 or l2 or carry: # 短的链表补0 v1 = l1.val if l1 != None else 0 v2 = l2.val if l2 != None else 0 tmp = v1 + v2 + carry if tmp &lt; 10: carry = 0 else: tmp -= 10 carry = 1 cur.next = ListNode(tmp) if l1: l1 = l1.next if l2: l2 = l2.next cur = cur.next return res.next ref 思路 1 全部变成数字做加法再换回去呗，这多暴力，爽！ 123456789101112131415161718192021222324252627282930class Solution(object): def addTwoNumbers(self, l1, l2): """ :type l1: ListNode :type l2: ListNode :rtype: ListNode """ if not l1: return l2 if not l2: return l1 val1, val2 = [l1.val], [l2.val] while l1.next: val1.append(l1.next.val) l1 = l1.next while l2.next: val2.append(l2.next.val) l2 = l2.next num1 = ''.join([str(i) for i in val1[::-1]]) num2 = ''.join([str(i) for i in val2[::-1]]) tmp = str(int(num1) + int(num2))[::-1] res = ListNode(int(tmp[0])) run_res = res for i in range(1, len(tmp)): run_res.next = ListNode(int(tmp[i])) run_res = run_res.next return res 思路 2 可以使用递归，每次算一位的相加 12345678910111213141516171819202122class Solution(object): def addTwoNumbers(self, l1, l2): """ :type l1: ListNode :type l2: ListNode :rtype: ListNode """ if not l1: return l2 if not l2: return l1 if l1.val + l2.val &lt; 10: l3 = ListNode(l1.val+l2.val) l3.next = self.addTwoNumbers(l1.next, l2.next) else: l3 = ListNode(l1.val+l2.val-10) tmp = ListNode(1) tmp.next = None l3.next = self.addTwoNumbers(l1.next, self.addTwoNumbers(l2.next, tmp)) return l3]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[77. Combinations]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F09%2F25%2Fhexo_leetcode%2F077.Combinations%2F</url>
    <content type="text"><![CDATA[77. Combinations题目：https://leetcode.com/problems/combinations/ 难度 : Medium 652ms 66%思路 典型的dfs, 用递归来解决,这里的k就是递归深度. 12345678910111213141516171819202122class Solution(object): def combine(self, n, k): """ :type n: int :type k: int :rtype: List[List[int]] """ res = [] # 注意题目要求 数字从1~n self.dfs(n, k, 1, [], res) print(res) def dfs(self, n, k, start, tmp, res): if k == 0: res.append(tmp) print(tmp) return # 注意题目要求 数字从1~n, range(1, n+1) for i in range(start, n+1): # tmp.append(i) self.dfs(n, k-1, i+1, tmp + [i], res) ref思路二： 标准的recursion 但是会超时 1234567891011121314151617class Solution(object): def combine(self, n, k): &quot;&quot;&quot; :type n: int :type k: int :rtype: List[List[int]] &quot;&quot;&quot; ans = [] self.dfs(n, k, 1, [], ans) return ans def dfs(self, n, k ,start, lst, ans): if k == 0 : ans.append(lst) return for i in range(start, n+1): self.dfs(n, k - 1, i + 1,lst +[i], ans) 理解方式 12345678910 ---title: 62. unique pathscategories: - LeetCode--- 1 2 3 12 13 14 23 24 34 可以参照这里 http://www.geeksforgeeks.org/print-all-possible-combinations-of-r-elements-in-a-given-array-of-size-n/ 解法三： 采用递归的方式，在n个数中选k个，如果n大于k，那么可以分类讨论，如果选了n，那么就是在1到(n-1)中选(k-1)个，否则就是在1到(n-1)中选k个。递归终止的条件是k为1，这时候1到n都符合要求。 注意一开始这里的else part花了我一点时间来理解，因为n必定大于k，所以这样递归当 n == k的时候选法就是code原作者的写法，也就是直接[range(1,k+1)] 参考这里： https://shenjie1993.gitbooks.io/leetcode-python/content/077%20Combinations.html 12345678910111213141516class Solution(object): def combine(self, n, k): &quot;&quot;&quot; :type n: int :type k: int :rtype: List[List[int]] &quot;&quot;&quot; if k == 1: return [[i + 1] for i in range(n)] result = [] if n &gt; k: result = [r + [n] for r in self.combine(n - 1, k - 1)] + self.combine(n - 1, k) else: #n == k # result = [r + [n] for r in self.combine(n - 1, k - 1)] result = [range(1,k+1)] return result]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[3. Longest Substring Without Repeating Characters]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F09%2F25%2Fhexo_leetcode%2F003._longest_substring_without_repeating_characters%2F</url>
    <content type="text"><![CDATA[3. Longest Substring Without Repeating Characters难度: Medium 刷题内容 原题连接 https://leetcode.com/problems/longest-substring-without-repeating-characters/description/ 内容描述 12345678910111213141516171819Given a string, find the length of the longest substring without repeating characters.Example 1:Input: &quot;abcabcbb&quot;Output: 3 Explanation: The answer is &quot;abc&quot;, with the length of 3. Example 2:Input: &quot;bbbbb&quot;Output: 1Explanation: The answer is &quot;b&quot;, with the length of 1.Example 3:Input: &quot;pwwkew&quot;Output: 3Explanation: The answer is &quot;wke&quot;, with the length of 3. Note that the answer must be a substring, &quot;pwke&quot; is a subsequence and not a substring. 100ms 66%**- 时间复杂度: O(N)**- 空间复杂度: O(N)** 思路 粗一看是dp，细一看是greedy 我们先从第一个字符开始，碰到某个字符均用dict保存下或则更新其idx. 假设从位置i开始检查, 每检查一位,就更新一次重复子串大小 j-i+1 到j位置发现了第一次发现重复字符’X’,那么ij之间没有重复字符,下次检查就可以从j开始了. 注意这里要把字符x的idx更新为j 更新每个字符的最靠后idx 详细见代码注释 1234567891011121314151617class Solution: def lengthOfLongestSubstring(self, s): """ :type s: str :rtype: int """ max_len = 0 start = 0 n = len(s) d = &#123;&#125; for i in range(n): #d.get(s[i], -1)+1 得到当前字符出现的最靠后那次的idx start = max(start, d.get(s[i], -1) + 1) # i - start + 1 dist(当前位置, 当前字符出现的最靠后那次的idx) max_len = max(max_len, i - start + 1) d[s[i]] = i return max_len]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[203. Remove Linked List Elements]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F09%2F25%2Fhexo_leetcode%2F203.Remove_linked_list_elements%2F</url>
    <content type="text"><![CDATA[203. Remove Linked List Elements题目:https://leetcode.com/problems/remove-linked-list-elements/ 难度: Easy 76ms 40%AC代码如下: 注意linked list 的首个node就可能需要删除,所以建立一个哑节点 dummy, dummy.next = head 链表中的删除实际是跳过该节点 1234567891011121314151617181920class Solution(object): def removeElements(self, head, val): """ :type head: ListNode :type val: int :rtype: ListNode """ dummy = ListNode(-1) dummy.next = head cur = dummy while cur.next: #注意这里从cur.next开始判断,因为如果到了cur且cur.val==val,这时已经无法跳过cur了 if cur.next.val == val: cur.next = cur.next.next else: cur = cur.next return dummy.next]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[49. Group Anagrams]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F09%2F25%2Fhexo_leetcode%2F049.Group_anagrams%2F</url>
    <content type="text"><![CDATA[49. Group Anagrams难度: Medium 刷题内容 原题连接 https://leetcode.com/problems/group-anagrams/description/ 内容描述 123456789101112131415Given an array of strings, group anagrams together.Example:Input: [&quot;eat&quot;, &quot;tea&quot;, &quot;tan&quot;, &quot;ate&quot;, &quot;nat&quot;, &quot;bat&quot;],Output:[ [&quot;ate&quot;,&quot;eat&quot;,&quot;tea&quot;], [&quot;nat&quot;,&quot;tan&quot;], [&quot;bat&quot;]]Note:All inputs will be in lowercase.The order of your output does not matter. 解题方案124ms 90%top 是hash table,提示用字典 字典的key是关键, 因为即使打乱顺序也就是那几个字母 开始尝试用set, 不能作为key, 转而用tuple time O(NKlogK) N是strs长度, K是最大str长度, KlogK是sort space O(NK) 新建的res 123456789101112131415class Solution: def groupAnagrams(self, strs): """ :type strs: List[str] :rtype: List[List[str]] """ # 新建一个字典, key是字母的tuple, val是list of str from collections import defaultdict res = defaultdict(list) for st in strs: res[tuple(sorted(st))].append(st) # 注意python3的转换 return list(res.values()) title: 62. unique pathscategories:​ - LeetCode 思路 1 每一个字符串都先排个序看看是不是一样，这样更好判断 1234567891011121314class Solution(object): def groupAnagrams(self, strs): """ :type strs: List[str] :rtype: List[List[str]] """ mapx = &#123;&#125; for i in strs: tmp = ''.join(sorted(list(i))) if tmp in mapx: mapx[tmp].append(i) else: mapx[tmp] = [i] return mapx.values()]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[70. Climbing Stairs]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F09%2F25%2Fhexo_leetcode%2F070.Climbing_Stairs%2F</url>
    <content type="text"><![CDATA[70. Climbing Stairs难度: Easy 刷题内容 原题连接 https://leetcode.com/problems/climbing-stairs/description/ 内容描述 12345678910111213141516171819202122You are climbing a stair case. It takes n steps to reach to the top.Each time you can either climb 1 or 2 steps. In how many distinct ways can you climb to the top?Note: Given n will be a positive integer.Example 1:Input: 2Output: 2Explanation: There are two ways to climb to the top.1. 1 step + 1 step2. 2 stepsExample 2:Input: 3Output: 3Explanation: There are three ways to climb to the top.1. 1 step + 1 step + 1 step2. 1 step + 2 steps3. 2 steps + 1 step 解题方案20ms 97%思路 典型DP,因为每次可以走1或2步,所以dp[i]=dp[i-1]+dp[i-2] 初始值可以看出 第一阶:1, 第二阶:2, 注意我们的dp[0]代表第一阶 1234567891011121314class Solution(object): def climbStairs(self, n): """ :type n: int :rtype: int """ if n == 1: return 1 dp = [None for _ in range(n)] dp[0] = 1 dp[1] = 2 for i in range(2, n): dp[i] = dp[i-1] + dp[i-2] return dp[-1]]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[62. unique paths]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F09%2F25%2Fhexo_leetcode%2F062.Unique_paths%2F</url>
    <content type="text"><![CDATA[62. unique paths 不同路径难度: 中等 刷题内容 原题连接 https://leetcode.com/problems/unique-paths https://leetcode-cn.com/problems/unique-paths/description 内容描述 12345一个机器人位于一个 m x n 网格的左上角 （起始点在下图中标记为“Start” ）。.机器人每次只能向下或者向右移动一步。机器人试图达到网格的右下角（在下图中标记为“Finish”）。问总共有多少条不同的路径？ 123456789101112131415161718例如，上图是一个7 x 3 的网格。有多少可能的路径？说明：m 和 n 的值均不超过 100。&gt; 示例 1:输入: m = 3, n = 2输出: 3解释:从左上角开始，总共有 3 条路径可以到达右下角。1. 向右 -&gt; 向右 -&gt; 向下2. 向右 -&gt; 向下 -&gt; 向右3. 向下 -&gt; 向右 -&gt; 向右&gt; 示例 2:输入: m = 7, n = 3输出: 28 DP 20ms 99%思路 DP两个点,递归公式和初始值 递归公式很好理解 $d[i][j]=d[i-1][j]+d[i][j-1]$, 因为这里只能向右下行走,所以只考虑当前点上方和左侧. 初始值需要考虑一下. 整个棋盘的左边界,左边界上的点没有左侧,只能从[0,0]一路向下,所以全是1 整个棋盘的上边界,上边界上的点没有上侧,只能从[0,0]一路向右,所以全是1 123456789101112131415161718class Solution(object): def uniquePaths(self, m, n): """ :type m: int :type n: int :rtype: int """ d = [[None]* n for _ in range(m)] for i in range(m): for j in range(n): if i== 0 or j == 0: d[i][j] = 1 else: d[i][j] = 0 for i in range(1, m): for j in range(1, n): d[i][j] = d[i-1][j] + d[i][j-1] return d[-1][-1] 组合 20ms 99% 思路 这里可以转化为排列组合问题,总共要走m+n-2步, 其中m-1步向下, n-1步向右 只有向左,向下两种选择,而且不考虑顺序问题,所以是$C_{m+n-2}^{m-1}$ 而且$C_n^m=C_n^{n-m}$ 所以先写个求阶乘,在套上述公式即可 12345678910111213class Solution(object): def uniquePaths(self, m, n): """ :type m: int :type n: int :rtype: int """ def fab(nums): res = 1 for i in range(1, nums+1): res *= i return res return fab(m+n-2) / (fab(n-1) * fab(m-1))]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[83. Remove Duplicates from Sorted List]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F09%2F25%2Fhexo_leetcode%2F083.Remove_duplicates_from_sorted_list%2F</url>
    <content type="text"><![CDATA[83. Remove Duplicates from Sorted List题目:https://leetcode.com/problems/remove-duplicates-from-sorted-list/ 难度: Easy 60ms 53%how 注意是sort的, 所以重复数字都是相邻的 重复数字可能有多次如 1-&gt;1-&gt;1 1234567891011121314151617181920# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def deleteDuplicates(self, head): """ :type head: ListNode :rtype: ListNode """ cur = head while cur: # 因为这里重复可能有多次, 所以用while而不是if #检查cur.next有没有, 尾部判断 while cur.next and cur.val == cur.next.val: cur.next = cur.next.next cur = cur.next return head dummy 大法 123456789101112class Solution(object): def deleteDuplicates(self, head): """ :type head: ListNode :rtype: ListNode """ dummy = head while head: while head.next and head.next.val == head.val: head.next = head.next.next # skip duplicated node head = head.next # not duplicate of current node, move to next node return dummy]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[75. Sort Colors]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F09%2F25%2Fhexo_leetcode%2F075.Sort_colors%2F</url>
    <content type="text"><![CDATA[75. Sort Colors难度: 中等 刷题内容 原题连接 https://leetcode.com/problems/sort-colors/description/ 内容描述 123456789101112131415Given an array with n objects colored red, white or blue, sort them in-place so that objects of the same color are adjacent, with the colors in the order red, white and blue.Here, we will use the integers 0, 1, and 2 to represent the color red, white, and blue respectively.Note: You are not suppose to use the library&apos;s sort function for this problem.Example:Input: [2,0,2,1,1,0]Output: [0,0,1,1,2,2]Follow up:A rather straight forward solution is a two-pass algorithm using counting sort.First, iterate the array counting number of 0&apos;s, 1&apos;s, and 2&apos;s, then overwrite array with total number of 0&apos;s, then 1&apos;s and followed by 2&apos;s.Could you come up with a one-pass algorithm using only constant space? 解题方案24ms 63%思路 其实就是排序, 将0放到前面,2放到后面 要求遍历一遍且inplace,这里需要两个计数,left标记0的个数, n-right标记2的个数 排序完,前left个都是0, 后right个都是2. 12345678910111213141516171819class Solution(object): def sortColors(self, nums): """ :type nums: List[int] :rtype: void Do not return anything, modify nums in-place instead. """ left = 0 right = len(nums) - 1 i = 0 while i &lt;= right: if nums[i] == 1: i += 1 elif nums[i] == 0: nums[left], nums[i] = nums[i], nums[left] left += 1 i += 1 elif nums[i] == 2: nums[right], nums[i] = nums[i], nums[right] right -= 1]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[112. Path Sum]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F09%2F25%2Fhexo_leetcode%2F112._path_sum%2F</url>
    <content type="text"><![CDATA[112. Path Sum题目:https://leetcode.com/problems/path-sum/ 难度: Easy 56ms 81%思路 递归查找, 分别从左/右开始递归, 每递归一层, 都需要sum-root.val 1234567891011121314151617181920# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def hasPathSum(self, root, sum): """ :type root: TreeNode :type sum: int :rtype: bool """ if root == None: return False if root.left == None and root.right == None: return root.val == sum return self.hasPathSum(root.left, sum - root.val) or self.hasPathSum(root.right, sum - root.val)]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[21. Merge Two Sorted Lists]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F09%2F25%2Fhexo_leetcode%2F021.Merge_two_sorted_lists%2F</url>
    <content type="text"><![CDATA[21. Merge Two Sorted Lists难度: Easy 刷题内容 原题连接 https://leetcode.com/problems/merge-two-sorted-lists/description/ 内容描述 123456Merge two sorted linked lists and return it as a new list. The new list should be made by splicing together the nodes of the first two lists.Example:Input: 1-&gt;2-&gt;4, 1-&gt;3-&gt;4Output: 1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4 48ms 77%How 新建一个哑节点, ListNode(0),最后返回的时候返回head.next,正好忽略第一个哑节点 判断大小,将链表的值依次放入, l1和l2有一个先取完, 未取完的部分拼接到后面即可 12345678910111213141516171819202122232425262728# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def mergeTwoLists(self, l1, l2): """ :type l1: ListNode :type l2: ListNode :rtype: ListNode """ cur = head = ListNode(0) while l1 and l2: if l1.val &lt; l2.val: cur.next = l1 l1 = l1.next else: cur.next = l2 l2 = l2.next cur = cur.next if l1: cur.next = l1 if l2: cur.next = l2 return head.next]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[18KDD_Multi-Pointer Co-Attention Networks for Recommendation]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F06%2F11%2F18KDD_MPCN%2F</url>
    <content type="text"><![CDATA[本文是一篇融合review信息做评分预测的推荐论文.针对user-item pair,在现有的co-attention机制上加入了对review的选择(引入pointer mechanism)来选择最有价值的user’s review 和item’s review. Motivation每个user 或者 item都会有很多reviews(一个review list), 同时这些review的价值不同.而现有的融合review的推荐算法都是把review-list拼接起来代表user/item. 作者针对上述做法的一系列的优化: 每一个review应该分开建模,并且有不同的重要性(权重) 针对user-item pair,应该动态选择比较match这个pair的review. 应该分层处理, review-level和word-level 虽然本文模型部分写的很多很复杂,但是实际就是提出了用pointer mechanism来选择reviews ModelInput encoding 对文本的处理首先就是将文本映射为向量,这里就是简单的embedding_lookup 这里作者又尝试了对输入进行过滤. 输入$xi$经过过滤后得到$\overline { x } { i }$ . \overline { x } _ { i } = \sigma \left( \mathbf { W } _ { g } x _ { i } \right) + \mathbf { b } _ { g } \odot \tanh \left( \mathbf { W } _ { u } x _ { i } + b _ { u } \right) 这里的过滤操作感觉有点奇怪. 尚未理解. Review-level Co-Attention首先是Co-Attention操作 s _ { i j } = F \left( a _ { i } \right) ^ { \top } \mathbf { M } F \left( b _ { j } \right) \\\ a ^ { \prime } = { G } \left( \max _ { c o l } ( s ) \right) ^ { \top } a \quad \text { and } \quad b ^ { \prime } = \left( G \left( \max _ { r o w } ( s ) \right) \right) ^ { \top } b这里用了max pooling. 相对于传统co-attention, 本文的变化在于$G$. 之前co-attention中的$G$就是个普通的softmax(得到一个soft的分布),类似[0.1, 0.3,…,0.3]的连续向量 但是本文是选择,直接选择部分输入, 类似[1,0,…,0]的离散向量. 离散体现了选择,但是这里不可导的,无法直接放到nn中.作者选择了Gumbel-Max trick来解决 Review Pointers上节把review-level部分模型的形式结构都清楚了,唯一还不明确的是G. 首先明确一点: Gumbel-Softmax trick，它用近似的方式使采样离散变量的过程变得可导. 因为在神经网络中我们希望使用BP算法,但是review-level co-attention里面的离散向量不可导. 更加详细解释可以参考如下: 重参数技巧 Word-level Co-Attention和上面的review-level很像,区别是: 这里的G就是普通的softmax 这里用了mean pooling Multi-Pointer LearningPointer机制通过选择滤除了噪音,但是也有一定的问题. 上述的pointer只能指向一对reviews(user和item各一个).也许有一些信息遗漏了. 本文用了a multi-pointer composition mechanism来解决上述问题.实际就是多做几次pointer操作,选出几对不同的reviews. 那么这几对reviews该怎么用呢?这里用了3种简单的方式:拼接,叠加,NN. 最终得到$a_f$和$b_f$. Prediction Layer这里将$a_f$和$b_f$送入FM中进行评分预测. F ( x ) = w _ { 0 } + \sum _ { i = 1 } ^ { n } w _ { i } x _ { i } + \sum _ { i = 1 } ^ { n } \sum _ { j = i + 1 } ^ { n } \left\langle v _ { i } , v _ { j } \right\rangle x _ { i } x _ { j }loss为评分预测常见的MSE. Experiment作者提出了4个RQ,并针对这些问题设计了实验.实验效果很好的回答了这些RQ Summary本文的核心贡献就是将reviews分别处理,然后利用了pointer机制来选择一些有用的reviews . model部分看起来提出了超多算法也很复杂,但是实际作者自己的东西很少. co-attention和gumbel都是现有的算法. 比较启发的是,强化学习也可以做选择,但是学习效率低等问题促使作者选择了pointer来达到同样的效果.那么其他一些选择操作,比如选择节点的有效邻居等操作是不是也可以用pointer? 另外,本文Introduction和Model部分写的很厉害.]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Recommendation</tag>
        <tag>Attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sentences 101-200]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F06%2F11%2F101-200%2F</url>
    <content type="text"></content>
      <categories>
        <category>English</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Graph Neural Network之GCN的理论]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F06%2F11%2Fgcn%2F</url>
    <content type="text"><![CDATA[图神经网络,尤其是图卷积,在近两年的顶会上疯狂刷脸. 这里整理了部分相关文献,并准备从GCN系列开始梳理. 最早的图卷积网络都是从谱域出发,利用图傅里叶变换,滤波器来设计. Ref: https://tkipf.github.io/graph-convolutional-networks/]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Graph Neural Network</tag>
        <tag>GCN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Reparametrization tricks 重参数技巧]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F06%2F11%2F%E9%87%8D%E5%8F%82%E6%95%B0%E5%8C%96%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[最新版见 https://jhy1993.github.io/Jhy1993.github.io/2018/06/11/重参数化技巧/ 这里讨论各种各样的Reparametrization tricks重参数技巧 对连续分布的重参数 VAE 对离散分布的不可导问题的重参数 VAE中对高斯分布的重参数这里是对连续分布的重参数. VAE中隐变量z一般取高斯分布,即$z=\mathcal{N}(\mu, \sigma^2)$,然后从这个分布中采样.但是这个采样操作是不可导的,进而导致整个模型无法BP. 解决方法就是Reparametrization tricks重参数技巧. 我们首先从从均值为0,标准差为1的高斯分布中采样,再放缩平移得到Z. \mathbf { z } _ { i } = \mu _ { i } + \sigma _ { i } * \epsilon , \epsilon \sim \mathcal { N } ( 0 , \mathbf { I } )这样从$\epsilon$到$\mathbf{z}$只涉及了线性操作(平移缩放),采样操作在NN计算图之外,而$\epsilon$对于NN来说只是一个常数. 离散分布的采样Gumbel-softmaxGumbel-Softmax TrickVAE的例子是一个连续分布(正态分布)的重参数，离散分布的情况也一样，首先需要可以采样，使得离散的概率分布有意义而不是只取概率最大的值，其次需要可以计算梯度。那么怎么做到的，具体操作如下： 对于n维概率向量$\pi$,对$\pi$对应的离散随机变量$x_{\pi}$添加Gumbel噪声，再取样 x_{\pi}=\arg\max(\log(\pi_i)+G_i)其中$G_i$是是独立同分布的标准Gumbel分布的随机变量，标准Gumbel分布的CDF为$F(x)=e^{-e^{-x}},F^{-1}(x)=-\log(-\log(x))$.这就是Gumbel-Max trick。可以看到由于这中间有一个argmax操作. 上述的 argmax操作是不可导的. 所以尝试用softmax来代替, 即Gumbel-Softmax Trick. 这里我们假设argmax返回的是一个one-hot向量,那么我们需要找到argmax的一个显式且光滑的逼近. 这里的$Gi$可以利用$F{-1}(x)$从均匀分布中采样得到,即$G_i=-\log(-\log(U_i)),U_i\sim U(0,1)$. 综上总体思路: 基于Gumbel Distribution采样来避免不可导问题 在1中引入了argmax又导致了不可导(Gumbel max) 又引入softmax函数来对argmax进行光滑近似,使得可导(Gumbel softmax) 具体步骤如下: 对于网络输出的一个n维向量v, 生成n个服从均匀分布U(0,1)的独立样本$\epsilon_1,…,\epsilon_n$ 通过$G_i=-\log(-\log(\epsilon_i))$计算得到$G_i$ 对应相加得到新的值向量$v’=[v_1+G_1,v_2+G_2,…,v_n+G_n]$ 通过softmax函数 \sigma_{\tau}(v'_i)=\frac{e^{v'_i/\tau}}{\sum\limits_{j=1}^ne^{v'_j/\tau}} 这里$\sigma_{\tau}(v’_i)$ 就可以实现对argmax的显式且光滑的逼近 {\lim_{\tau \to 0}}\sigma_{\tau}(v'_i)=argmax温度参数$\tau$的影响: $\tau$越小(趋近于0), 越接近categorical分布;$\tau$越大(趋近于无穷), 越接近均匀分布 证明常规的softmax形式为 \pi_k=\frac{e^{x_k}}{\sum^K_{k'=1}e^{x'_k}}其中,$\pi_k$是softmax之后得到一个概率密度函数. 那么有没有某个分布能够等价于上述的分布呢? 如果对每个$x_k$添加独立标准的gumbel噪声(位置为0,尺度为1),并选择值最大的维度输出,每次的输出结果有一个概率密度函数.这样一个概率密度同样为$\pi_k$ . 化简 \begin{array}{l} P(z_k\ge z_{k'};\forall k'\not = k|\{x_{k'}\}_{k'=1}^K)\\ \qquad \qquad =\int \prod_{k'\not= k}e^{-e^{-(z_k-x_{k'})}}\cdot e^{-(z_k-x_k)-e^{-(z_k-x_k)}}\,dz_k \\ \qquad \qquad = \int e^{-\sum_{k'\not=k}e^{-(z_k-x_{k'})}-(z_k-x_k)-e^{-(z_k-x_k)}}\,dz_k\\ \qquad \qquad = \int e^{-\sum_{k'=1}^Ke^{-(z_k-x_{k'})}-(z_k-x_k)}\,dz_k\\ \qquad \qquad = \int e^{-(\sum_{k'=1}^Ke^{x_{k'}})e^{-z_k}-z_k+x_k}\,dz_k\\ \qquad \qquad = \int e^{-e^{-z_k+\ln(\sum_{k'=1}^Ke^{x_{k'}})}-z_k+x_k}\,dz_k \\ \qquad \qquad = \int e^{-e^{-(z_k-\ln(\sum_{k'=1}^Ke^{x_{k'}}))}-(z_k-\ln(\sum_{k'=1}^Ke^{x_{k'}}))-\ln(\sum_{k'=1}^Ke^{x_{k'}})+x_k}\,dz_k \\ \qquad \qquad = e^{-\ln(\sum_{k'=1}^Ke^{x_{k'}})+x_k}\int e^{-e^{-(z_k-\ln(\sum_{k'=1}^Ke^{x_{k'}}))}-(z_k-\ln(\sum_{k'=1}^Ke^{x_{k'}}))}\,dz_k\\ \qquad \qquad = \frac{e^{x_k}}{\sum_{k'=1}^Ke^{x_{k'}}}\int e^{-e^{-(z_k-\ln(\sum_{k'=1}^Ke^{x_{k'}}))}-(z_k-\ln(\sum_{k'=1}^Ke^{x_{k'}}))}\,dz_k \\ \qquad \qquad = \frac{e^{x_k}}{\sum_{k'=1}^Ke^{x_{k'}}}\int e^{-(z_k-\ln(\sum_{k'=1}^Ke^{x_{k'}}))-e^{-(z_k-\ln(\sum_{k'=1}^Ke^{x_{k'}}))}}\,dz_k \end{array}积分里面是$\mu=\ln(\sum{k’=1}^Ke^{x{k’}})$ 的gumbel分布,整个积分为1,则 P(z_k\ge z_{k'};\forall k'\not = k|\{x_{k'}\}_{k'=1}^K)=\frac{e^{x_k}}{\sum_{k'=1}^Ke^{x_{k'}}}结果与softmax的分布一致. 为什么需要gumbel-softmax乍看起来，gumbel-softmax 的用处令人费解。比如上面的代码示例，直接使用 softmax，也可以达到类似的参数训练效果。但两者有着根本的区别。原理上，常规的 softmax 直接建模了一个概率分布（多项分布），基于交叉熵的训练准则使分布尽可能靠近目标分布；而 gumbel-softmax 则是对多项分布采样的一个近似。使用上，常规的有监督学习任务（分类器训练）中，直接学习输出的概率分布是自然的选择；而对于涉及采样的学习任务（VAE 隐变量采样、强化学习中对actions 集合进行采样以确定下一步的操作），gumbel-softmax 提供了一种再参数化的方法，使得模型可以以端到端的方式进行训练。 Ref CATEGORICAL REPARAMETERIZATIONWITH GUMBEL-SOFTMAX https://zhuanlan.zhihu.com/p/35218887 https://casmls.github.io/general/2017/02/01/GumbelSoftmax.html http://lips.cs.princeton.edu/the-gumbel-max-trick-for-discrete-distributions/ https://blog.csdn.net/jackytintin/article/details/53641885 大量tf代码实例]]></content>
      <categories>
        <category>ML &amp; DL</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F06%2F11%2Ftemplate%2F</url>
    <content type="text"><![CDATA[s]]></content>
      <categories>
        <category>ML &amp; DL</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Network Embedding</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零单排PRML 第4章 Linear Models for Classification]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F06%2F11%2FPRML_4_Linear%20Models%20for%20Classification%2F</url>
    <content type="text"><![CDATA[拉普拉斯近似]]></content>
      <categories>
        <category>PRML</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>PRML</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[各种GAN]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F06%2F11%2F%E5%90%84%E7%A7%8DGAN%2F</url>
    <content type="text"><![CDATA[GANSimple GANBasic Idea: MLE $\rightarrow$ minimize KL given a data distribution $P_{data}(x)$ $pixel \rightarrow vector$ means the distribution of image a distribution $P_G(x;\theta)$ parametered by $\theta$ \theta^*=argmax Generator G is a function, input $z$ ,output $x$ 过去用GMM来拟合,但是效果很差,GAN用NN来做分布变换效果很好 Given a prior distribution $P_{prior}(z)$, a probability distribution $P_G(x)$ is defined by function $G$ Unit gaussian distribution$\xrightarrow{NN}$ generated distribution $\approx$ real distribution Discriminator $D$ is a function, input $x$ , output $scalar$ (分类概率) Evaluate the “difference” between$PG(x)$ and $P{data}(x)$ Function $V(G,D)$ $G^*=\arg\underset{m}{\min}\underset{D}{\max}V(G,D)$ Given $G$, the optimal $D^*$ maximizing \begin{aligned} V&=E_{x\sim P_{data}}[logD(x)]+E_{x\sim P_G}[log(1-D(x))] \\\ &=\int_x P_{data}(x)logD(x)dx+\int_xP_G(x)log(1-D(x))dx \\\ &=\int_x[P_{data}logD(x)+P_G(x)log(1-D(x))]dx \\\ & assume \quad{}D(x) can have any value here \\\ &P_{data}和P_G(x)均已知 ,所以简化为\\\ &f(D)=alog(D)+blog(1-D) \end{aligned} Given $x=Data$, find $D^*$ maximizing \begin{aligned} & P_{data}logD(x)+P_G(x)log(1-D(x)) \\\ &\frac{df(D)}{dD}=a*\frac{1}{D}+b*\frac{1}{1-D}*(-1)=0\\\ &解得D^*=\frac{a}{a+b}\\\ & D^*=\frac{P_{data}(x)}{P_{data}(x)+P_G(x)} \end{aligned} Given $G$, $\underset{D}{max}V(G,D)=V(G,D^*)$ \begin{aligned} &V(G,D^*)=E_{x\sim P_{data}}[logD(x)]+E_{x\sim P_G}[log(1-D(x))] \\\ &=E_{x\sim P_{data}}[log{\frac{P_{data}(x)}{P_{data}(x)+P_G(x)}}]+E_{x\sim P_G}[log{\frac{P_{G}(x)}{P_{data}(x)+P_G(x)}}] \\\ &=-2log2+\int_x P_{data}(x)log{\frac{P_{data}(x)}{(P_{data}(x)+P_G(x))/2}}dx+\int_xP_G(x)log{\frac{P_{G}(x)}{(P_{data}(x)+P_G(x))/2}}dx \\\ &=-2log2+KL\Big(P_{data}(x)||\frac{P_{data}(x)+P_G(x)}{2}\Big) +KL\Big(P_{G}(x)||\frac{P_{data}(x)+P_G(x)}{2}\Big) \\\ &=-2log2+ 2JSD\Big(P_{data}(x)||P_G(x)\Big) \\\ &\propto JSD\Big(P_{data}(x)||P_G(x)\Big) \\\ &KL散度不对称,但是这里的KL()+KL()整体对称,又叫JSD\\ \end{aligned} Kullback–Leibler divergence \quad D(P||Q)\\\ D_KL(P||Q)=\sum_iP(i)log\frac{P(i)}{Q(i)} \\\ Jensen–Shannon \quad divergence \quad and \quad \\\ 完全一样0]]></content>
      <categories>
        <category>ML &amp; DL</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[237. Delete Node in a Linked List]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F05%2F14%2Fhexo_leetcode%2F237.%20Delete%20Node%20in%20a%20Linked%20List%2F</url>
    <content type="text"><![CDATA[237. Delete Node in a Linked List https://leetcode.com/problems/delete-node-in-a-linked-list/description/ 28ms 98% 输入只给了节点,并没有给整个链表, 假设 1234 删除2 将该node后面一个节点的值付给该node, 变成1334 然后将该node的 next跳过一步即可(略过第二个3), 1-3(原来的2)-4 1234567891011121314# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def deleteNode(self, node): """ :type node: ListNode :rtype: void Do not return anything, modify node in-place instead. """ node.val = node.next.val node.next = node.next.next 237. Delete Node in a Linked List题目:https://leetcode.com/problems/delete-node-in-a-linked-list/ 难度:Easy 这道题，第一感觉，像删链表一样来删，把所有的node val前移一个,但是有个问题，为什么tail那个node还是存在？哼(ˉ(∞)ˉ)唧.. 已经被解答： http://stackoverflow.com/questions/38879291/python-delete-a-node-in-linked-list-given-just-access-to-that-node 另外一个O（1）的办法更好，把后一个node的val移到待删这个节点，并且把node.next = node.next.next​题目说了不会删最后一个点，所以node.next.next一定存在，所以直接让node的val等于它next的val，然后让node的next指向它的next的next，举个例子：​ 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;None,要删除第四个节点，就让4变成5，然后让第四个节点指向第五个节点的next，这样原来的第四个节点就不存在了，虽然原来的第五个节点仍然存在且指向None，变成了1-&gt;2-&gt;3-&gt;5-&gt;None-&lt;5​ 123456789O(1)时间class Solution(object): def deleteNode(self, node): """ :type node: ListNode :rtype: void Do not return anything, modify node in-place instead. """ node.val = node.next.val node.next = node.next.next 123456789101112O(n)时间class Solution(object): def deleteNode(self, node): """ :type node: ListNode :rtype: void Do not return anything, modify node in-place instead. """ while node.next: node.val = node.next.val prev, node = node, node.next # clear reference to tail prev.next = None]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[142. Linked List Cycle II]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F05%2F13%2Fhexo_leetcode%2F142.%20Linked%20List%20Cycle%20II%2F</url>
    <content type="text"><![CDATA[142. Linked List Cycle II思路1 98% 70msHow 首先判断有无环,slow和fast法, 其实slow和fast会相遇在环内的某个位置meeting, 不一定在入口entry相遇 假设起点到环长度为L, 环长度为C, 相遇时,快指针走了L + meeting + N*C, N为圈数 慢指针走了 L + meeting, 还差C - meeting回到入口 而因为快指针走2步, 慢指针走1步,所以步数是2倍关系 2(L + meeting) = L + meeting + N C 得到L = N *C - meeting = C - meeting, 因为C是循环的 快慢指针相遇时, 慢指针还差C-meating回到入口,这时新建一个指针从起点开始遍历, 新指针和慢指针相遇的位置就是入口 123456789101112131415161718192021222324252627# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def detectCycle(self, head): """ :type head: ListNode :rtype: ListNode """ if head is None or head.next is None: return None entry =head slow = head fast = head while fast and fast.next: slow = slow.next fast = fast.next.next if slow == fast: while slow != entry: entry = entry.next slow = slow.next # if slow == entry: return entry return None]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[141. Linked List Cycle]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F05%2F13%2Fhexo_leetcode%2F141.%20Linked%20List%20Cycle%2F</url>
    <content type="text"><![CDATA[Linked List Cycle https://leetcode.com/problems/linked-list-cycle/description/ 思路1 73ms 63%经典的快慢指针, 快指针走2步, 慢指针走1步,* 如果有环,慢指针会赶上快指针,两者相遇在环内的某个位置 ​ 12345678910111213141516171819202122# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def hasCycle(self, head): """ :type head: ListNode :rtype: bool """ slow = head fast = head # fast走的更快,所以只要判断fast的合法即可 while fast and fast.next: slow = slow.next fast = fast.next.next # 这里要判断slow==fast,而不能用slow.val=fast.val, 因为没环的也可能slow.val=fast.val if slow == fast: return True return False 思路2 98% 66ms将遍历过的链表反向, 如果有环,将会遍历环之后,从环的入口返回起点 123456789101112131415161718192021# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def hasCycle(self, head): """ :type head: ListNode :rtype: bool """ pre, cur = None, head while cur: # pre.next = cur, cur.next #这里变量位置不能变,不然会报错,还没搞明白 pre, cur.next, cur = cur, pre, cur.next if cur == head: return True return False]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[78. Subsets]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F05%2F11%2Fhexo_leetcode%2F78.%20Subsets%2F</url>
    <content type="text"><![CDATA[78. Subsets title: 62. unique pathscategories: - LeetCode Medium Given a set of distinct integers, nums, return all possible subsets (the power set). Note: The solution set must not contain duplicate subsets. Example: 123456789101112Input: nums = [1,2,3]Output:[ [3], [1], [2], [1,2,3], [1,3], [2,3], [1,2], []] https://leetcode.com/problems/subsets/description/ 44ms 54%思路 和77的区别是,这里没有递归深度限制 还有一点这里tmp的元素是nums[i] 12345678910111213141516class Solution: def subsets(self, nums): """ :type nums: List[int] :rtype: List[List[int]] """ res = [] self.dfs2(nums, 0, [], res) print(res) return res def dfs2(self, nums, start, tmp, res): res.append(tmp) for i in range(start, len(nums)): self.dfs2(nums, i+1, tmp + [nums[i]], res) 100%123456789101112131415class Solution: def subsets(self, nums): """ :type nums: List[int] :rtype: List[List[int]] """ res = [[]] for num in nums: for tmp in res[:]: # 注意这里res 和tmp都是list 会变化 # list[:] x = tmp[:] x.append(num) res.append(x) return res]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[26. Remove Duplicates from Sorted Array]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F05%2F05%2Fhexo_leetcode%2F026.%20Remove%20Duplicates%20from%20Sorted%20Array%2F</url>
    <content type="text"><![CDATA[26. Remove Duplicates from Sorted Arrayhttps://leetcode.com/problems/remove-duplicates-from-sorted-array/#/description Given a sorted array, remove the duplicates in place such that each element appear only once and return the new length.Do not allocate extra space for another array, you must do this in place with constant memory.For example,Given input array nums = [1,1,2],Your function should return length = 2, with the first two elements of nums being 1 and 2 respectively. It doesn’t matter what you leave beyond the new length. 注意 有序数组 只能O(1)的额外空间 除了返回length之外, 原始数组的前length的元素应该是不重复的数字 例如[1,1,2], 返回length=2和[1,2] 思路1 99% 遍历,遇到不同的数字就flag+=1 并把该数字放到前面 Time O(n)只需遍历一次 Space O(1) 1234567891011121314151617class Solution: def removeDuplicates(self, nums): """ :type nums: List[int] :rtype: int """ if len(nums) == 0: return 0 flag = 1 cur_num = nums[0] for num in nums: if cur_num != num: flag += 1 cur_num = num nums[flag - 1] = cur_num return flag]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python Basic Skill]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F05%2F05%2Fpython%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[字典加减https://www.jianshu.com/p/58d21289ff5c 字典相加: 合并两个字典, 相同键的值相加 字典相减 cnt1 - cnt2: 如果cnt1包含cnt2 对应的键值相减 cnt2包含cnt1, 减不了,返回空* 浅拷贝和深拷贝那么浅拷贝（shallow copy）与深拷贝（deep copy）有什么区别呢？ 对于不可变对象，比如整数、字符串、元组、还有由这些不可变对象组成的集合对象，浅拷贝和深拷贝没有区别，都是拷贝一个新对象 两者的区别在于拷贝组合对象，比如列表中还有列表，字典中还有字典或者列表的情况时，浅拷贝只拷贝了外面的壳子，里面的元素并没有拷贝，而深拷贝则是把壳子和里面的元素都拷贝了一份新的。 对列表的切片拷贝 z[:] 或者是调用对象的copy方法 list.copy() 都属于浅拷贝 b = a 引用,改变b, a也随之改变 b = a[:] 这里是浅拷贝,但是list的浅拷贝等价于深拷贝, 改变b不影响a 不过最好还是用from copy import deepcopy b = deepcopy(a) 初始化二维列表正确初始化二维列表[m,n], 用None来占位 1d = [[None] * n for _ in range(m)]]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1. Two Sum I]]></title>
    <url>%2FJhy1993.github.io%2F2018%2F04%2F14%2Fhexo_leetcode%2F001.%20Two%20Sum%20I%2F</url>
    <content type="text"><![CDATA[1. Two Sum I 100%https://leetcode.com/problems/two-sum/#/description Given an array of integers, return indices of the two numbers such that they add up to a specific target.You may assume that each input would have exactly one solution, and you may not use the same element twice.Example:Given nums = [2, 7, 11, 15], target = 9,Because nums[0] + nums[1] = 2 + 7 = 9,return [0, 1].Subscribe to see which companies asked this question. 思路 新建一个字典来保存 target - nums[i] dict{target - nums[i] : i} 继续搜索，当nums[j] in dict 时，则 nums[i] + nums[j] = target Time O(n) 遍历字典一次 Space O(n) 新建字典 12345678910111213141516# time O(n)# space O(n)class Solution(object): def twoSum(self, nums, target): """ :type nums: List[int] :type target: int :rtype: List[int] """ dict = &#123;&#125; for i in range(len(nums)): if nums[i] in dict: return [dict[nums[i]], i] else: dict[target - nums[i]] = i 1234567891011121314更干净的写法class Solution: def twoSum(self, nums, target): """ :type nums: List[int] :type target: int :rtype: List[int] """ d = &#123;&#125; for idx, num in enumerate(nums): if num in d: return [d[num], idx] else: d[target - num] = idx $s_1^1=10$ s_1=a \\ s_2=b12]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
</search>
